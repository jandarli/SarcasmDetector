{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import build_train \n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import parameters\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import tokenize\n",
    "import re\n",
    "import math\n",
    "max_len = 20\n",
    "batch_size = 10\n",
    "hidden_size = 25\n",
    "embedding_size = 128\n",
    "num_distinct_heights = 3\n",
    "num_filters = 100\n",
    "channels = 1\n",
    "num_classes = 2\n",
    "Heights = [1,3,5]\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sent_corpus(file):\n",
    "    line_split = []\n",
    "    message_list = defaultdict(list)\n",
    "    with open(file,\"r\") as fid:\n",
    "            c = 0\n",
    "            for line in fid:\n",
    "                c+=1\n",
    "                #if c > 100:\n",
    "                #    break\n",
    "    #             print(\"len\", len(line))\n",
    "                if len(line) > 1:\n",
    "                    line_split = line.split(\" \")\n",
    "                    message_line = \" \".join(line_split[2:])\n",
    "                    \n",
    "                    #message_list.append(\" \".join(line.split()[2:]))\n",
    "                    message_list[line_split[0]].append((message_line,line_split[1]))\n",
    "            #max_len = len(max(message_list, key=len))\n",
    "            print(len(message_list))\n",
    "            print(\" total \",c,\" lines read fron train_labels\")\n",
    "            return  message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genTrainExamples(message_list, max_len, wrd2idx):\n",
    "    #feature_list = defaultdict(list)\n",
    "    #found max_len greater that embedding size don't understand a thing about it. Keeping it 10 temporarily.\n",
    "    feature_list = []\n",
    "    c = 0\n",
    "    sent_list = []\n",
    "    max_len = 20\n",
    "    tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    regex = re.compile(r'[\\.\\]\\%\\[\\'\",\\?\\*!\\}\\{<>\\^-]')\n",
    "    for key, message_line_list in message_list.items():\n",
    "        #c += 1\n",
    "        #if c > 70:\n",
    "        #    break\n",
    "        for sent,label in message_line_list:\n",
    "            content = tokenizer.tokenize(sent)\n",
    "            content = [word for word in content if not regex.match(word)]\n",
    "            for w in content:                 \n",
    "                if w in wrd2idx: #don't know if we have to take words comming only in wrd2idx(topwords)\n",
    "                    sent_list.append(wrd2idx[w])\n",
    "            while len(sent_list) < max_len:\n",
    "                sent_list.append(0)\n",
    "            while len(sent_list) > max_len:\n",
    "                sent_list.pop()\n",
    "            assert len(sent_list) == max_len\n",
    "            feature_list.append((key,sent_list,label))\n",
    "            sent_list = []  \n",
    "    #print(len(feature_list))\n",
    "    return feature_list\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try dense layer, try to figure out why reshape has to use 3*3, try to figure out drop out and also try to understand what happens if we give same heights to all filters, try to figure out the strides, also for varying heights try different height combinations. \n",
    "\n",
    "Used this remember to put this in refference if the structure eremains the same for cnn in future.\n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(object):\n",
    "     \n",
    "    \n",
    "    def __init__(self, graph, E, U):\n",
    "\n",
    "        \n",
    "        self.build_graph(graph, E, U)\n",
    "        #self.find_preds(self, embed, user_embed, hidden_layer_weights, hidden_layer_bias, total_h_grams)\n",
    "\n",
    "    def build_graph(self, graph, E, U):\n",
    "        \n",
    "        with graph.as_default():\n",
    "            total_h_grams = num_distinct_heights * num_filters\n",
    "            # for CNN #####\n",
    "              \n",
    "            self.embeddings = tf.Variable(E, dtype=tf.float32)\n",
    "            self.drop_out = tf.placeholder(tf.float32)\n",
    "            ###### train variables and place holders ####\n",
    "            self.train_inputs = tf.placeholder(tf.int32, shape=[batch_size,max_len])\n",
    "            #print(\"train_inputs\",self.train_inputs )\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "            \n",
    "            train_labels = tf.one_hot(self.train_labels, num_classes) #converting it into categorical class of sarcasm/not sarcasm\n",
    "            \n",
    "            \n",
    "            embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs)\n",
    "            \n",
    "            embed = tf.expand_dims(embed, -1)  # making it 4d for conv2d function  \n",
    "            #print(\"embed after lookup\",embed)\n",
    "            \n",
    "            ################################################# Tune ##############################################\n",
    "            self.tune_inputs = tf.placeholder(tf.int32, shape=[batch_size,max_len])\n",
    "            #print(\"self.tune_inputs\",self.tune_inputs)\n",
    "            self.tune_labels = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "            tune_labels = tf.one_hot(self.tune_labels, num_classes)\n",
    "            \n",
    "            #print(\"tune_labels\",tune_labels )\n",
    "            tune_embed = tf.nn.embedding_lookup(self.embeddings, self.tune_inputs)\n",
    "            #print(\"embed after lookup\",tune_embed)\n",
    "            tune_embed = tf.expand_dims(tune_embed, -1)\n",
    "            \n",
    "            ############################################## Test #################################################\n",
    "            self.test_inputs = tf.placeholder(tf.int32, shape=[None,max_len])\n",
    "            test_embed = tf.nn.embedding_lookup(self.embeddings, self.test_inputs)\n",
    "            print(\"embed after lookup\",test_embed)\n",
    "            test_embed = tf.expand_dims(test_embed, -1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # for RNN ###\n",
    "            self.user_embeddings = tf.Variable(U, dtype=tf.float32)\n",
    "            \n",
    "            ################################# train placeholders and variables ##########################################            \n",
    "            self.user_id = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "            #print(\"user_id\",self.user_id)\n",
    "            user_embed = tf.nn.embedding_lookup(self.user_embeddings, self.user_id)\n",
    "            #print(\"user_embed after lookup\",user_embed)\n",
    "            \n",
    "            ######################### tune placeholders and variables ###################################################\n",
    "            self.tune_user_id = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "            #print(\"tune_user_id\",self.tune_user_id)\n",
    "            tune_user_embed = tf.nn.embedding_lookup(self.user_embeddings, self.tune_user_id)\n",
    "            #print(\"user_embed after lookup\",tune_user_embed)\n",
    "            \n",
    "            ################################# test placeholders and variables ###########################################\n",
    "            self.test_user_id = tf.placeholder(tf.int32, shape=[None,])\n",
    "            print(\"test_user_id\",self.test_user_id )\n",
    "            test_user_embed = tf.nn.embedding_lookup(self.user_embeddings, self.test_user_id)\n",
    "            print(\"user_embed after lookup\",test_user_embed)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            hidden_layer_weights = tf.Variable(tf.truncated_normal([hidden_size, total_h_grams + embedding_size],\n",
    "                                                                      stddev=1.0 / math.sqrt(hidden_size)))\n",
    "            #print(\"hidden layer weights\",hidden_layer_weights)\n",
    "            hidden_layer_bias = tf.Variable(tf.zeros([hidden_size,1]))\n",
    "            \n",
    "            #print(\"hidden_layer_bias\",hidden_layer_bias )\n",
    "            \n",
    "            output_layer_weights = tf.Variable(tf.truncated_normal([num_classes ,hidden_size ],\n",
    "            \n",
    "                                                                   stddev=1.0 / math.sqrt(hidden_size)))\n",
    "            #print(\"output_layer_weights\",output_layer_weights)  \n",
    "            output_layer_bias = tf.Variable(tf.zeros([num_classes,1]))\n",
    "            #print(\"output_layer_bias\",output_layer_bias)         \n",
    "            \n",
    "            \n",
    "            # CALLING NEURAL NET CNN for train #####\n",
    "            self.prediction, filter_weights= self.find_preds(embed,user_embed,hidden_layer_weights,hidden_layer_bias\n",
    "                                                    ,total_h_grams,output_layer_weights,output_layer_bias)\n",
    "            #print(\"prediction\",self.prediction)\n",
    "            \n",
    "            self.loss = self.find_loss(self.prediction, train_labels,filter_weights, hidden_layer_weights, hidden_layer_bias, output_layer_weights, output_layer_bias)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.transpose(self.prediction),\n",
    "                                                                                         labels=train_labels))\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.prediction,\n",
    "                                                                                         labels=self.train_labels))\n",
    "            \n",
    "            cue_cnn_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.prediction),\n",
    "                                                                                          labels=self.train_labels)\n",
    "            \n",
    "            \"\"\"\n",
    "                     \n",
    "            ## finding loss ########\n",
    "            optimizer = tf.train.AdadeltaOptimizer(1.0,0.95,1e-08) #with decay rate of 0.95\n",
    "            \n",
    "            #with constant learning Rate\n",
    "            #optimizer = tf.train.AdamOptimizer(1e-6) \n",
    "            \n",
    "            #optimizer = tf.train.GradientDescentOptimizer(1e-6)\n",
    "            \n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "            clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "            self.app = optimizer.apply_gradients(clipped_grads)\n",
    "            \n",
    "            self.train_prediction = tf.nn.softmax(tf.transpose(self.prediction))\n",
    "            \n",
    "            \n",
    "            ############################# CALLING NEURAL NET CNN for tune ##############################################\n",
    "            self.tune_preds, tune_filter_weights= self.find_preds(tune_embed,tune_user_embed,hidden_layer_weights,hidden_layer_bias,\n",
    "                                                                  total_h_grams,output_layer_weights,output_layer_bias)\n",
    "            \n",
    "            #print(\"tune prediction\",self.tune_preds)\n",
    "            self.tune_loss = self.find_loss(self.tune_preds, tune_labels,filter_weights, hidden_layer_weights, hidden_layer_bias, output_layer_weights, output_layer_bias)\n",
    "            ############################################ finding accuracy for tune set #################################\n",
    "            self.tune_prediction = tf.nn.softmax(tf.transpose(self.tune_preds))\n",
    "            \n",
    "            \n",
    "            ############################# CALLING NEURAL NET CNN for test ##############################################\n",
    "            self.test_preds, test_filter_weights= self.find_preds(test_embed,test_user_embed,hidden_layer_weights,hidden_layer_bias,\n",
    "                                                                  total_h_grams,output_layer_weights,output_layer_bias)\n",
    "            \n",
    "            ########################################### finding accuracy for test set ##################################\n",
    "            self.test_prediction = tf.nn.softmax(tf.transpose(self.test_preds))\n",
    "        \n",
    "            \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "  \n",
    "    def find_preds(self, embed, user_embed, hidden_layer_weights, hidden_layer_bias, total_h_grams, output_layer_weights,output_layer_bias):\n",
    "        pooled_outputs = []       \n",
    "        for f_size in Heights:                \n",
    "            filter_weights = tf.Variable(tf.truncated_normal([f_size, embedding_size, channels, num_filters], stddev=0.1))\n",
    "            bias_conv = tf.Variable(tf.zeros(num_filters))\n",
    "            conv = tf.nn.conv2d(embed,filter_weights,strides=[1, 1, 1, 1],padding=\"VALID\")\n",
    "            additive_bias = tf.nn.bias_add(conv, bias_conv)\n",
    "            Relu_layer = tf.nn.relu(additive_bias)\n",
    "            pooled = tf.nn.max_pool(Relu_layer,ksize=[1, max_len - f_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID')\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "        #print(pooled_outputs)\n",
    "\n",
    "        concatenate_map_filters = tf.concat(pooled_outputs,num_distinct_heights)\n",
    "        pooled_outputs = []\n",
    "        #print(\"Concatenated map filters\", concatenate_map_filters)\n",
    "        Cs_matrix_before_drop_out = tf.reshape(concatenate_map_filters,[total_h_grams, -1])\n",
    "        #print(\"Cs_matrx_befor_drop_out\",Cs_matrix_before_drop_out)\n",
    "        Cs_matrix = tf.nn.dropout(Cs_matrix_before_drop_out, self.drop_out)\n",
    "        # RNN ###\n",
    "        reshaped_user_embed = tf.reshape(user_embed,[embedding_size,-1])\n",
    "        #print(\"reshaped_user_embed\", reshaped_user_embed)\n",
    "        rnn_input = tf.concat([reshaped_user_embed,Cs_matrix],0)\n",
    "        #print(\"rnn_input\",rnn_input)\n",
    "        H_Cs_U = tf.matmul(hidden_layer_weights,rnn_input)\n",
    "        #print(\"H_Cs_U\",H_Cs_U)\n",
    "        hidden_layer_output = tf.add(H_Cs_U,hidden_layer_bias)\n",
    "        #print(\"hidden_layer_output\",hidden_layer_output)\n",
    "        activation_layer_output = tf.nn.relu(hidden_layer_output)\n",
    "        #print_train_labels = tf.Print(train_labels, [train_labels], )\n",
    "        #print(\"activation_layer_output\",activation_layer_output)\n",
    "        preds = tf.add(tf.matmul(output_layer_weights,activation_layer_output),output_layer_bias)\n",
    "        return preds, filter_weights \n",
    "    \n",
    "    \n",
    "    def find_loss(self,logits, labels,filter_weights, hidden_layer_weights, hidden_layer_bias, output_layer_weights, output_layer_bias):\n",
    "                \n",
    "        cue_cnn_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.transpose(logits),\n",
    "                                                                                labels=labels)\n",
    "\n",
    "        ### L2 Regularizer #########\n",
    "        filter_weight_regularizer = tf.nn.l2_loss(filter_weights)\n",
    "\n",
    "        h_weight_regularizer = tf.nn.l2_loss(hidden_layer_weights)\n",
    "        h_bias_regularizer = tf.nn.l2_loss(hidden_layer_bias)\n",
    "\n",
    "        output_layer_weights_regularizer = tf.nn.l2_loss(output_layer_weights)\n",
    "        output_layer_bias_regularizer = tf.nn.l2_loss(output_layer_bias)\n",
    "\n",
    "        E_regularizer = tf.nn.l2_loss(self.embeddings)\n",
    "        U_regularizer = tf.nn.l2_loss(self.user_embeddings)\n",
    "\n",
    "        loss = tf.reduce_mean(cue_cnn_loss + (1e-4) *  filter_weight_regularizer \n",
    "                                                + (1e-4) *  h_weight_regularizer\n",
    "                                                + (1e-4) *  h_bias_regularizer\n",
    "                                                + (1e-4) *  output_layer_weights_regularizer\n",
    "                                                + (1e-4) *  output_layer_bias_regularizer\n",
    "                                                + (1e-4) *  E_regularizer\n",
    "                                                + (1e-4) *  U_regularizer\n",
    "\n",
    "                                                )\n",
    "        return loss\n",
    "    \n",
    "    def train(self, sess, trainFeats, max_len ):\n",
    "        self.init.run()\n",
    "        print(\" Training Initailized\")\n",
    "        c = 0\n",
    "        max_num_steps = 1001\n",
    "        user_idx = {}  \n",
    "        user  = []\n",
    "        sent  = []\n",
    "        label = []\n",
    "        for tuple_i in trainFeats:\n",
    "            try:\n",
    "                u_id = user_idx[tuple_i[0]]\n",
    "            except KeyError:\n",
    "                user_idx[tuple_i[0]] = len(user_idx)\n",
    "                u_id = user_idx[tuple_i[0]]\n",
    "            if u_id > 84: ################################### ADDED because of the error in training of user2vec\n",
    "                break\n",
    "            user.append(u_id)\n",
    "            \n",
    "            sent_i , label_i = tuple_i[1], tuple_i[2]\n",
    "            sent.append(sent_i)\n",
    "            label.append(int(float(label_i)))\n",
    "        average_loss = 0\n",
    "        #print(\"label\",len(label))\n",
    "        #print(\"sent\", sent)\n",
    "        #print(\"user\",user)\n",
    "        #print(label)\n",
    "        \n",
    "        split = int( len(user) * 0.9) # 90-10 split of train and  validation set.\n",
    "        sent_train = sent[:split]\n",
    "        sent_tune  = sent[split:]\n",
    "        user_train = user[:split]\n",
    "        user_tune  = user[split:]\n",
    "        label_train = label[:split]\n",
    "        label_tune  = label[split:]\n",
    "        #print(len(user_tune))\n",
    "        #print(len(label_tune))\n",
    "        #print(len(sent_tune))\n",
    "        #print(\"trainFeats\",len(trainFeats))\n",
    "        for step in range(max_num_steps):\n",
    "            predictions = []\n",
    "            average_loss = 0\n",
    "            average_loss_v = 0\n",
    "            predictions_v = []\n",
    "            for start in range(0, len(user_train), batch_size):\n",
    "                end = (start + batch_size) % len(user_train)\n",
    "                if end < start:\n",
    "                    start -= end\n",
    "                    old_end = end\n",
    "                    end = len(user_train)\n",
    "                batch_user, batch_inputs, batch_labels = user_train[start:end], sent_train[start:end], label_train[start:end]\n",
    "                feed_dict = {self.user_id.name:batch_user, self.train_inputs.name: batch_inputs,\n",
    "                                                       self.train_labels.name: batch_labels, self.drop_out: 0.5 }\n",
    "                                                                                          \n",
    "                _, loss_val, preds_train = sess.run([self.app, self.loss, self.train_prediction], feed_dict=feed_dict)\n",
    "                if start % batch_size != 0:\n",
    "                    \n",
    "                    preds_train = list(preds_train)\n",
    "                    for i in range(0, old_end):\n",
    "                        preds_train.pop(0)\n",
    "                    preds_train = np.array(preds_train)\n",
    "                    \n",
    "                average_loss += loss_val\n",
    "                predictions.extend(list(np.argmax(preds_train, 1)))\n",
    "            \n",
    "            if step % 5 == 0:\n",
    "               \n",
    "                print(\"Average loss at step \", step, \": \", average_loss)\n",
    "               \n",
    "                #print(\"Classsification Report:\\n\",classification_report(label_train, predictions, target_names=[\"Sarcasm\", \"Not Sarcasm\"]))\n",
    "\n",
    "\n",
    "                print(\"Train Accuracy:\", 100.0 * accuracy_score(label_train, predictions), \"%\\n\")\n",
    "                \n",
    "            if step % 10 == 0:\n",
    "                for start_v in range(0, len(user_tune), batch_size):\n",
    "                    \n",
    "                    end_v = (start_v + batch_size) % len(user_tune)\n",
    "                    if end_v < start_v:\n",
    "                        start_v -= end_v\n",
    "                        old_end_v = end_v\n",
    "                        end_v = len(user_tune)\n",
    "                    batch_user_v, batch_inputs_v, batch_labels_v = user_tune[start_v:end_v], sent_tune[start_v:end_v], label_tune[start_v:end_v]\n",
    "\n",
    "                    feed_dict = {self.tune_user_id.name:batch_user_v, self.tune_inputs.name: batch_inputs_v, \n",
    "                                                                      self.tune_labels.name: batch_labels_v, \n",
    "                                                                      self.drop_out: 1.0 }\n",
    "\n",
    "                    tune_loss ,preds_tune = sess.run([self.tune_loss,self.tune_prediction], feed_dict=feed_dict)\n",
    "                    \n",
    "                    if start_v % batch_size != 0:\n",
    "                 \n",
    "                        preds_tune = list(preds_tune)                      \n",
    "\n",
    "                        for i in range(0, old_end_v):\n",
    "                            preds_tune.pop(0)\n",
    "                        preds_tune = np.array(preds_tune)\n",
    "                    average_loss_v += tune_loss\n",
    "                    predictions_v.extend(list(np.argmax(preds_tune, 1)))\n",
    "                print(\"Validation Loss:\", average_loss_v)\n",
    "                #print(\"Validation Report:\\n\",classification_report(label_tune, predictions_v, target_names=[\"Sarcasm\", \"Not Sarcasm\"]))\n",
    "                print(\"Validation Accuracy: \", 100.0 * accuracy_score(label_tune, predictions_v), \"%\\n\" )\n",
    "        print(\"Train Finished.\")     \n",
    "        \n",
    "    def test(self, sess, testFeats, max_len ):\n",
    "        self.init.run()\n",
    "        print(\" Testing Initailized\")\n",
    "        c = 0\n",
    "        max_num_steps = 1001\n",
    "        user_idx = {}  \n",
    "        user  = []\n",
    "        sent  = []\n",
    "        label = []\n",
    "        for tuple_i in testFeats:\n",
    "            try:\n",
    "                u_id = user_idx[tuple_i[0]]\n",
    "            except KeyError:\n",
    "                user_idx[tuple_i[0]] = len(user_idx)\n",
    "                u_id = user_idx[tuple_i[0]]\n",
    "            if u_id > 84: ################################### ADDED because of the error in training of user2vec\n",
    "                break\n",
    "            user.append(u_id)\n",
    "            \n",
    "            sent_i , label_i = tuple_i[1], tuple_i[2]\n",
    "            sent.append(sent_i)\n",
    "            label.append(int(float(label_i)))\n",
    "        average_loss = 0\n",
    "        #print(\"label\",len(label))\n",
    "        #print(\"sent\", len(sent))\n",
    "        #print(\"user\",len(user))\n",
    "        #print(label)\n",
    "        \n",
    "        \n",
    "        #print(\"testFeats\",len(testFeats))\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for start in range(0, len(user), batch_size):\n",
    "            end = (start + batch_size) % len(user)\n",
    "            if end < start:\n",
    "                start -= end\n",
    "                old_end = end\n",
    "                end = len(user)\n",
    "            batch_user, batch_inputs, batch_labels = user[start:end], sent[start:end], label[start:end]\n",
    "            feed_dict = {self.test_user_id.name:batch_user, self.test_inputs.name: batch_inputs,\n",
    "                                                       self.drop_out: 0.5 }\n",
    "\n",
    "            preds_test = sess.run([self.test_prediction], feed_dict=feed_dict)\n",
    "            #print(\"preds_test\",preds_test)\n",
    "            #print(\"preds_test\",type(preds_test))\n",
    "            preds_test = preds_test[0]\n",
    "            if start % batch_size != 0:\n",
    "\n",
    "                preds_test = list(preds_test)\n",
    "                for i in range(0, old_end):\n",
    "                    preds_test.pop(0)\n",
    "                preds_test = np.array(preds_test)\n",
    "\n",
    "\n",
    "            predictions.extend(list(np.argmax(preds_test, 1)))\n",
    "            \n",
    "        #print(len(predictions))  \n",
    "               \n",
    "                #print(\"Average loss at step \", step, \": \", average_loss)\n",
    "               \n",
    "                #print(\"Classsification Report:\\n\",classification_report(label_train, predictions, target_names=[\"Sarcasm\", \"Not Sarcasm\"]))\n",
    "\n",
    "\n",
    "        print(\"Test Accuracy:\", 100.0 * accuracy_score(label, predictions), \"%\\n\")\n",
    "        print(\"Test Report:\\n\",classification_report(label, predictions, target_names=[\"Sarcasm\", \"Not Sarcasm\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_cnn():\n",
    "    E,unigram_prob,wrd2idx,word_counter,n_users = pickle.load(open('train_embeddings.pkl', 'rb'))\n",
    "    U = pickle.load(open(\"user_embeddings.pkl\",\"rb\"))\n",
    "    #print(U.shape)\n",
    "    message_list = create_sent_corpus(\"cleaned_data.txt\")\n",
    "    print(\"Generating Traning Examples\")\n",
    "    trainFeats= genTrainExamples(message_list, max_len, wrd2idx)\n",
    "    #print(trainFeats)\n",
    "    print(\"Done.\")\n",
    "    test_message_list = create_sent_corpus(\"cleaned_data_test.txt\")\n",
    "    print(\"Generating Testing Examples\")\n",
    "    testFeats= genTrainExamples(test_message_list, max_len, wrd2idx)\n",
    "    #print(trainFeats)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    # Build the graph model\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    model = CNNModel(graph, E, U)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        model.train(sess, trainFeats, max_len)\n",
    "        model.test(sess, testFeats,max_len)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12502\n",
      " total  60694  lines read fron train_labels\n",
      "Generating Traning Examples\n",
      "Done.\n",
      "3996\n",
      " total  10946  lines read fron train_labels\n",
      "Generating Testing Examples\n",
      "Done.\n",
      "embed after lookup Tensor(\"embedding_lookup_2:0\", shape=(?, 20, 128), dtype=float32)\n",
      "test_user_id Tensor(\"Placeholder_8:0\", shape=(?,), dtype=int32)\n",
      "user_embed after lookup Tensor(\"embedding_lookup_5:0\", shape=(?, 128), dtype=float32)\n",
      " Training Initailized\n",
      "Average loss at step  0 :  109.711598873\n",
      "Train Accuracy: 49.9189627229 %\n",
      "\n",
      "Validation Loss: 12.2626973391\n",
      "Validation Accuracy:  53.6231884058 %\n",
      "\n",
      "Average loss at step  5 :  104.023840785\n",
      "Train Accuracy: 59.319286872 %\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-226-99ce17f48c9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minit_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-221-2a974a7a3baa>\u001b[0m in \u001b[0;36minit_cnn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainFeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestFeats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-225-7816cdaf0eed>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sess, trainFeats, max_len)\u001b[0m\n\u001b[0;32m    263\u001b[0m                                                        self.train_labels.name: batch_labels, self.drop_out: 0.5 }\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_prediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
