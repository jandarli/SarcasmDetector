{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "import parameters\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#User2Vec = namedtuple('User2Vec', ['user_id', 'sent_ids', 'neg_ids', 'optimizer', 'loss', 'normalized_U'])\n",
    "User2Vec = namedtuple('User2Vec', ['user_id', 'sent_ids', 'neg_ids', 'app', 'loss', 'normalized_U'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_loss(user_embeds, word_embeds, neg_sample_ids):\n",
    "    pos_score = tf.matmul(user_embeds, word_embeds, transpose_b = True)\n",
    "    print('pos_score: ', pos_score)\n",
    "    \n",
    "    user_embeds_t = tf.transpose(user_embeds)\n",
    "    neg_sample_ids_t = tf.transpose(neg_sample_ids)\n",
    "    \n",
    "    neg_score = tf.matmul(neg_sample_ids, user_embeds_t)\n",
    "    #neg_score = tf.tensordot(neg_sample_ids_t, user_embeds_t, [0,0])\n",
    "    print('neg_score: ', neg_score)\n",
    "\n",
    "    loss = tf.maximum(0.0, 1 - tf.add(pos_score,neg_score))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(sess, graph, embed_matrix_rows, n_users, embed_matrix):\n",
    "    lam = 1e-8\n",
    "    with graph.as_default():\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "        with tf.device('/cpu:0'):\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            \n",
    "            # u_j\n",
    "            user_id = tf.placeholder(tf.int32, shape=[1])\n",
    "            print('user_ids: ', user_id)\n",
    "            U = tf.Variable(tf.random_uniform([n_users, parameters.embedding_size], -1.0, 1.0))\n",
    "            print('U: ', U)\n",
    "            user_embed = tf.nn.embedding_lookup(U, user_id)\n",
    "            #user_embed = tf.slice(U, [0, user_id], [U.get_shape()[0], 1])\n",
    "#             user_embed = tf.transpose(user_embed)\n",
    "            print('user_embed: ', user_embed)\n",
    "\n",
    "            # e_i\n",
    "            E = tf.Variable(embed_matrix, dtype=tf.float32)\n",
    "            print('E: ', E)\n",
    "            sent_ids = tf.placeholder(tf.int32, shape=None)\n",
    "            print('sent_ids: ', sent_ids)\n",
    "            word_embeds = tf.nn.embedding_lookup(E, sent_ids)\n",
    "            print('word_embeds :', word_embeds)\n",
    "            \n",
    "            # e_l\n",
    "            neg_ids = tf.placeholder(tf.int32, shape=None)\n",
    "            neg_sample_ids = tf.nn.embedding_lookup(E, neg_ids)\n",
    "\n",
    "            hinge_loss_1 =  hinge_loss(user_embed, word_embeds, neg_sample_ids)\n",
    "            #U_regularizer = tf.nn.l2_loss(U)\n",
    "            #E_regularizer = tf.nn.l2_loss(E)\n",
    "\n",
    "            loss = tf.reduce_mean(hinge_loss_1 )#+ (lam/2) *  U_regularizer + (lam/2) *  E_regularizer)\n",
    "\n",
    "            \n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(1e-6).minimize(loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "        #optimizer = tf.train.GradientDescentOptimizer(1e-6)\n",
    "        #grads = optimizer.compute_gradients(loss)\n",
    "        #clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "        #app = optimizer.apply_gradients(clipped_grads)\n",
    "        \n",
    "        #AdamOptimizer\n",
    "        \n",
    "        #optimizer = tf.train.AdamOptimizer(1e-6)\n",
    "        #grads = optimizer.compute_gradients(loss)\n",
    "        #clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "        #app = optimizer.apply_gradients(clipped_grads)\n",
    "        \n",
    "        #MomentumOptimizer\n",
    "        optimizer = tf.train.MomentumOptimizer(1e-8,0.9)\n",
    "        grads = optimizer.compute_gradients(loss)\n",
    "        clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "        app = optimizer.apply_gradients(clipped_grads)\n",
    "        \n",
    "        \n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(U), 1, keep_dims=True))\n",
    "        normalized_U = U / norm\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "\n",
    "    #model = User2Vec(user_id, sent_ids, neg_ids, optimizer, loss, normalized_U)\n",
    "    model = User2Vec(user_id, sent_ids, neg_ids, app, loss, normalized_U)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, model, n_users):\n",
    "    \n",
    "    user_ids = np.arange(n_users)\n",
    "    max_num_steps = 10000\n",
    "  \n",
    "    user_idx = {}\n",
    "    for prev_user, train, test, neg_samples in user_train_data:\n",
    "        \n",
    "        try:\n",
    "            user_id = user_idx[prev_user]\n",
    "        except KeyError:\n",
    "            user_idx[prev_user] = len(user_idx)\n",
    "        print('user: ', user_idx[prev_user])\n",
    "        average_loss_step = max(parameters.checkpoint_step/10, 100)\n",
    "    \n",
    "        average_loss = 0\n",
    "        for step in range(max_num_steps):\n",
    "#             print('step: ', step)\n",
    "            for id in np.random.permutation(len(train)):\n",
    "                \n",
    "#                 print('train[id]', len(train[id]))\n",
    "                \n",
    "#                 print('train[id]', len(neg_samples[id]))\n",
    "#                 print('neg samples: ', neg_samples)\n",
    "#                 print('train: ', train)\n",
    "                \n",
    "                for x in train[id]:\n",
    "                    assert not np.any(np.isnan(x))\n",
    "                #print('train: ', train[id])\n",
    "                if train[id] == []:\n",
    "                    continue\n",
    "                feed_dict = {model.user_id.name: [user_idx[prev_user]], model.sent_ids.name: train[id], model.neg_ids.name: neg_samples[id]}\n",
    "\n",
    "\n",
    "            #_, loss_val = sess.run([model.optimizer, model.loss], feed_dict=feed_dict)\n",
    "            _, loss_val = sess.run([model.app, model.loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "            \n",
    "            if step % 1000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 1000\n",
    "                print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                average_loss = 0\n",
    "                \n",
    "    final_embeddings = model.normalized_U.eval()\n",
    "    print(\"Train Finished\")\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed matrix shape 0:  1034\n",
      "embed matrix shape:  (1034, 128)\n",
      "embed matrix shape 1:  128\n",
      "embed matrix len:  1034\n",
      "user_ids:  Tensor(\"Placeholder:0\", shape=(1,), dtype=int32, device=/device:CPU:0)\n",
      "U:  <tf.Variable 'Variable_1:0' shape=(86, 128) dtype=float32_ref>\n",
      "user_embed:  Tensor(\"embedding_lookup:0\", shape=(1, 128), dtype=float32, device=/device:CPU:0)\n",
      "E:  <tf.Variable 'Variable_2:0' shape=(1034, 128) dtype=float32_ref>\n",
      "sent_ids:  Tensor(\"Placeholder_1:0\", dtype=int32, device=/device:CPU:0)\n",
      "word_embeds : Tensor(\"embedding_lookup_1:0\", dtype=float32, device=/device:CPU:0)\n",
      "pos_score:  Tensor(\"MatMul:0\", shape=(1, ?), dtype=float32, device=/device:CPU:0)\n",
      "neg_score:  Tensor(\"MatMul_1:0\", shape=(?, 1), dtype=float32, device=/device:CPU:0)\n",
      "Initialized\n",
      "user:  0\n",
      "Average loss at step  0 :  0.327806830406\n",
      "Average loss at step  1000 :  0.326070125282\n",
      "Average loss at step  2000 :  0.322534230232\n",
      "Average loss at step  3000 :  0.318998102307\n",
      "Average loss at step  4000 :  0.315461895943\n",
      "Average loss at step  5000 :  0.311925642967\n",
      "Average loss at step  6000 :  0.308389380157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-08a65e7cd423>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0muser_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-257-b107bad66f1b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(sess, model, n_users)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m#_, loss_val = sess.run([model.optimizer, model.loss], feed_dict=feed_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0maverage_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # load pickled word embeddings\n",
    "    # because we want the number of users which we pickled here\n",
    "    embed_matrix, unigram_prob, wrd2idx, word_counter, n_users = pickle.load(open(parameters.output_pkl, 'rb'))\n",
    "    print('embed matrix shape 0: ', embed_matrix.shape[0])\n",
    "    print('embed matrix shape: ', embed_matrix.shape)\n",
    "    print('embed matrix shape 1: ', embed_matrix.shape[1])\n",
    "    print('embed matrix len: ', len(embed_matrix))\n",
    "    \n",
    "    user_train_data = pickle.load(open(parameters.output, 'rb'))\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "\n",
    "        model = build_model(sess, graph, embed_matrix.shape[0], n_users, embed_matrix)\n",
    "        \n",
    "        user_embeddings = train(sess, model, n_users)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
