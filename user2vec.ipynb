{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "import parameters\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "User2Vec = namedtuple('User2Vec', ['user_id', 'sent_ids', 'neg_ids', 'optimizer', 'loss', 'normalized_U'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(user_embeds, word_embeds, neg_sample_ids):\n",
    "    pos_score = tf.matmul(user_embeds, word_embeds, transpose_b = True)\n",
    "    print('pos_score: ', pos_score)\n",
    "    \n",
    "    user_embeds_t = tf.transpose(user_embeds)\n",
    "    neg_sample_ids_t = tf.transpose(neg_sample_ids)\n",
    "    \n",
    "    neg_score = tf.tensordot(neg_sample_ids_t, user_embeds_t, [0,0])\n",
    "    print('neg_score: ', neg_score)\n",
    "\n",
    "    loss = tf.maximum(0.0, 1 - pos_score + neg_score)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(sess, graph, embed_matrix_rows, n_users, embed_matrix):\n",
    "\n",
    "    with graph.as_default():\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "        with tf.device('/cpu:0'):\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            \n",
    "            # u_j\n",
    "            user_id = tf.placeholder(tf.int32, shape=[1])\n",
    "            print('user_ids: ', user_id)\n",
    "            U = tf.Variable(tf.random_uniform([n_users, parameters.embedding_size], -1.0, 1.0))\n",
    "            print('U: ', U)\n",
    "            user_embed = tf.nn.embedding_lookup(U, user_id)\n",
    "            #user_embed = tf.slice(U, [0, user_id], [U.get_shape()[0], 1])\n",
    "#             user_embed = tf.transpose(user_embed)\n",
    "            print('user_embed: ', user_embed)\n",
    "\n",
    "            # e_i\n",
    "            E = tf.Variable(embed_matrix, dtype=tf.float32)\n",
    "            print('E: ', E)\n",
    "            sent_ids = tf.placeholder(tf.int32, shape=None)\n",
    "            print('sent_ids: ', sent_ids)\n",
    "            word_embeds = tf.nn.embedding_lookup(E, sent_ids)\n",
    "            print('word_embeds :', word_embeds)\n",
    "            \n",
    "            # e_l\n",
    "            neg_ids = tf.placeholder(tf.int32, shape=None)\n",
    "            neg_sample_ids = tf.nn.embedding_lookup(E, neg_ids)\n",
    "\n",
    "            loss = tf.reduce_mean(hinge_loss(user_embed, word_embeds, neg_sample_ids))\n",
    "            \n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(.05).minimize(loss, global_step=global_step)\n",
    "\n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(U), 1, keep_dims=True))\n",
    "        normalized_U = U / norm\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "\n",
    "    model = User2Vec(user_id, sent_ids, neg_ids, optimizer, loss, normalized_U)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, model, n_users):\n",
    "    \n",
    "    user_ids = np.arange(n_users)\n",
    "    max_num_steps = 10\n",
    "    \n",
    "    user_idx = {}\n",
    "    for prev_user, train, test, neg_samples in user_train_data:\n",
    "        \n",
    "        try:\n",
    "            user_id = user_idx[prev_user]\n",
    "        except KeyError:\n",
    "            user_idx[prev_user] = len(user_idx)\n",
    "    \n",
    "        average_loss_step = max(parameters.checkpoint_step/10, 100)\n",
    "    \n",
    "        average_loss = 0\n",
    "        for step in range(max_num_steps):\n",
    "#             print('step: ', step)\n",
    "            for id in np.random.permutation(len(train)):\n",
    "#                 print('train: ', train)\n",
    "#                 print('train[id]', len(train[id]))\n",
    "                \n",
    "#                 print('train[id]', len(neg_samples[id]))\n",
    "#                 print('neg samples: ', neg_samples)\n",
    "#                 print('train: ', train)\n",
    "#                 print('id: ', id)\n",
    "\n",
    "                feed_dict = {model.user_id.name: [user_idx[prev_user]], model.sent_ids.name: train[id], model.neg_ids.name: neg_samples[id]}\n",
    "\n",
    "\n",
    "            _, loss_val = sess.run([model.optimizer, model.loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "            \n",
    "            if step % average_loss_step == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= average_loss_step\n",
    "                print('Average loss at step: ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                \n",
    "    final_embeddings = model.normalized_U.eval()\n",
    "    \n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed matrix shape 0:  1086\n",
      "embed matrix shape:  (1086, 128)\n",
      "embed matrix shape 1:  128\n",
      "embed matrix len:  1086\n",
      "user_ids:  Tensor(\"Placeholder:0\", shape=(1,), dtype=int32, device=/device:CPU:0)\n",
      "U:  <tf.Variable 'Variable_1:0' shape=(86, 128) dtype=float32_ref>\n",
      "user_embed:  Tensor(\"embedding_lookup:0\", shape=(1, 128), dtype=float32, device=/device:CPU:0)\n",
      "E:  <tf.Variable 'Variable_2:0' shape=(1086, 128) dtype=float32_ref>\n",
      "sent_ids:  Tensor(\"Placeholder_1:0\", dtype=int32, device=/device:CPU:0)\n",
      "word_embeds : Tensor(\"embedding_lookup_1:0\", dtype=float32, device=/device:CPU:0)\n",
      "pos_score:  Tensor(\"MatMul:0\", shape=(1, ?), dtype=float32, device=/device:CPU:0)\n",
      "neg_score:  Tensor(\"Tensordot:0\", dtype=float32, device=/device:CPU:0)\n",
      "Initialized\n",
      "Average loss at step:  0 :  1.0\n",
      "Average loss at step:  0 :  0.9999998807907104\n",
      "Average loss at step:  0 :  1.0\n",
      "Average loss at step:  0 :  1.0\n",
      "Average loss at step:  0 :  1.0\n",
      "Average loss at step:  0 :  0.9963253736495972\n",
      "Average loss at step:  0 :  1.0019843578338623\n",
      "Average loss at step:  0 :  1.0297876596450806\n",
      "Average loss at step:  0 :  0.9979217052459717\n",
      "Average loss at step:  0 :  0.0\n",
      "Average loss at step:  0 :  0.9733633995056152\n",
      "Average loss at step:  0 :  1.0145084857940674\n",
      "Average loss at step:  0 :  0.9731452465057373\n",
      "Average loss at step:  0 :  0.9949368834495544\n",
      "Average loss at step:  0 :  0.0\n",
      "Average loss at step:  0 :  1.0474047660827637\n",
      "Average loss at step:  0 :  0.9888511896133423\n",
      "Average loss at step:  0 :  1.0547807216644287\n",
      "Average loss at step:  0 :  1.014148235321045\n",
      "Average loss at step:  0 :  1.031508445739746\n",
      "Average loss at step:  0 :  0.9920930862426758\n",
      "Average loss at step:  0 :  1.0306847095489502\n",
      "Average loss at step:  0 :  1.0515234470367432\n",
      "Average loss at step:  0 :  1.0093034505844116\n",
      "Average loss at step:  0 :  0.966194748878479\n",
      "Average loss at step:  0 :  0.9601155519485474\n",
      "Average loss at step:  0 :  1.0218311548233032\n",
      "Average loss at step:  0 :  1.0075926780700684\n",
      "Average loss at step:  0 :  0.9726026654243469\n",
      "Average loss at step:  0 :  0.9797505140304565\n",
      "Average loss at step:  0 :  1.0313682556152344\n",
      "Average loss at step:  0 :  0.991209864616394\n",
      "Average loss at step:  0 :  1.125886082649231\n",
      "Average loss at step:  0 :  1.074902057647705\n",
      "Average loss at step:  0 :  0.9219657778739929\n",
      "Average loss at step:  0 :  1.0139503479003906\n",
      "Average loss at step:  0 :  1.016014575958252\n",
      "Average loss at step:  0 :  0.9847503900527954\n",
      "Average loss at step:  0 :  0.9271904230117798\n",
      "Average loss at step:  0 :  0.9680674076080322\n",
      "Average loss at step:  0 :  1.0964715480804443\n",
      "Average loss at step:  0 :  1.066238284111023\n",
      "Average loss at step:  0 :  0.0\n",
      "Average loss at step:  0 :  1.0806167125701904\n",
      "Average loss at step:  0 :  0.9427093267440796\n",
      "Average loss at step:  0 :  1.0716769695281982\n",
      "Average loss at step:  0 :  0.9884287118911743\n",
      "Average loss at step:  0 :  1.026829481124878\n",
      "Average loss at step:  0 :  0.9963071942329407\n",
      "Average loss at step:  0 :  1.1049104928970337\n",
      "Average loss at step:  0 :  nan\n",
      "Average loss at step:  0 :  1.0129151344299316\n",
      "Average loss at step:  0 :  0.9224826693534851\n",
      "Average loss at step:  0 :  1.0472941398620605\n",
      "Average loss at step:  0 :  0.9592974781990051\n",
      "Average loss at step:  0 :  1.0825215578079224\n",
      "Average loss at step:  0 :  0.9940515756607056\n",
      "Average loss at step:  0 :  1.1022578477859497\n",
      "Average loss at step:  0 :  0.8916249871253967\n",
      "Average loss at step:  0 :  1.028572678565979\n",
      "Average loss at step:  0 :  0.9959458708763123\n",
      "Average loss at step:  0 :  1.1201047897338867\n",
      "Average loss at step:  0 :  1.0041555166244507\n",
      "Average loss at step:  0 :  1.030945062637329\n",
      "Average loss at step:  0 :  1.0009984970092773\n",
      "Average loss at step:  0 :  0.9205565452575684\n",
      "Average loss at step:  0 :  0.9980557560920715\n",
      "Average loss at step:  0 :  0.9633084535598755\n",
      "Average loss at step:  0 :  0.9787662625312805\n",
      "Average loss at step:  0 :  0.976705014705658\n",
      "Average loss at step:  0 :  0.9471555948257446\n",
      "Average loss at step:  0 :  1.0700606107711792\n",
      "Average loss at step:  0 :  1.0046570301055908\n",
      "Average loss at step:  0 :  1.282098412513733\n",
      "Average loss at step:  0 :  0.9409448504447937\n",
      "Average loss at step:  0 :  1.096744179725647\n",
      "Average loss at step:  0 :  0.9812197685241699\n",
      "Average loss at step:  0 :  1.0597939491271973\n",
      "Average loss at step:  0 :  0.9397983551025391\n",
      "Average loss at step:  0 :  1.0231205224990845\n",
      "Average loss at step:  0 :  nan\n",
      "Average loss at step:  0 :  0.9715242385864258\n",
      "Average loss at step:  0 :  0.9697890281677246\n",
      "Average loss at step:  0 :  1.066117525100708\n",
      "Average loss at step:  0 :  0.9577668309211731\n",
      "Average loss at step:  0 :  0.9932932257652283\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # load pickled word embeddings\n",
    "    # because we want the number of users which we pickled here\n",
    "    embed_matrix, unigram_prob, wrd2idx, word_counter, n_users = pickle.load(open(parameters.output_pkl, 'rb'))\n",
    "    print('embed matrix shape 0: ', embed_matrix.shape[0])\n",
    "    print('embed matrix shape: ', embed_matrix.shape)\n",
    "    print('embed matrix shape 1: ', embed_matrix.shape[1])\n",
    "    print('embed matrix len: ', len(embed_matrix))\n",
    "    \n",
    "    user_train_data = pickle.load(open(parameters.output, 'rb'))\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "\n",
    "        model = build_model(sess, graph, embed_matrix.shape[0], n_users, embed_matrix)\n",
    "        \n",
    "        user_embeddings = train(sess, model, n_users)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
