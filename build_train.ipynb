{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "#from ipdb import set_trace\n",
    "#from negative_samples import negative_sampler\n",
    "import numpy as np\n",
    "import os\n",
    "#from sma_toolkit import embeddings as emb_utils \n",
    "#import streaming_pickle as stPickle\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import parameters\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import tokenize\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "MIN_DOC_LEN=4\n",
    "tokenizer = TweetTokenizer(preserve_case=True)\n",
    "regex = re.compile(r'[\\.\\]\\%\\[\\'\",\\?\\*!\\}\\{<>\\^-]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Negative Samples that are user specific and then using negative samples and the previous user word ids create user specific train data. create_user_train function is similar to the batch process of skigram model of word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = \"word2vec_nce.model\"\n",
    "train_file = \"cleaned_data.txt\"\n",
    "output_pkl = \"train_embeddings.pkl\"\n",
    "vocabulary_size = 100000\n",
    "min_word_freq = 0\n",
    "seed = 42\n",
    "neg_samples = 10\n",
    "output = \"user_train_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(filename, wordDict, embedding_size):\n",
    "    dictionary, steps, word_embeds = pickle.load(open(filename, 'rb'))        \n",
    "    \n",
    "    print(\"dictionary length\",len(dictionary))\n",
    "    print(\"dictionary keys\",len(dictionary.keys()))\n",
    "    \n",
    "    key = list(wordDict.keys())[0]\n",
    "    print('word dict key: ', key)\n",
    "    print('word dict val: ', wordDict[key])\n",
    "    \n",
    "    print(\"dictionary keys\",list(dictionary.keys())[0])\n",
    "    print(\"dictionary vals\",list(dictionary.values())[0])\n",
    "    \n",
    "    print(\"word embeds: \", len(word_embeds))\n",
    "    print(\"word embeds: \", len(word_embeds[0]))\n",
    "    \n",
    "    embedding_array = np.zeros((len(wordDict), embedding_size))\n",
    "    knownWords = list(wordDict.keys())\n",
    "    unknownWords = []\n",
    "    \n",
    "    #print(knownWords)\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        \n",
    "        w = knownWords[i]\n",
    "        if w in dictionary:\n",
    "            index = dictionary[w]\n",
    "        elif w.lower() in dictionary:\n",
    "            index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "        else:\n",
    "            unknownWords.append(w)\n",
    "#             embedding_array[i] = np.random.rand(embedding_size) * 0.02 - 0.01\n",
    "    #print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "    #print \"embedding_array shape\",embedding_array.shape\n",
    "\n",
    "    print('embed array len: ', len(embedding_array))\n",
    "    print('embed array len: ', len(embedding_array[0]))\n",
    "    return embedding_array, dictionary, unknownWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neg_samples(user_dict, wc):\n",
    "    neg_dict = defaultdict(list)\n",
    "    #sample = np.random.choice(vocabulary_size, num_sampled, p=unigram_prob, replace=False)\n",
    "   \n",
    "    for user, message in user_dict.items():\n",
    "        user_wrd = set(message)\n",
    "        word_corpus = set(wc.keys())\n",
    "        diff = word_corpus - user_wrd        \n",
    "        neg_dict[user] = random.sample(diff, parameters.sent_idx)\n",
    "    return neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from negative_samples.py of samiroid\n",
    "def multinomial_samples(unigram_distribution, exclude=[], n_samples=1):\n",
    "    samples = []        \n",
    "    while len(samples) != n_samples:            \n",
    "        wrd_idx = np.argmax(np.random.multinomial(1, unigram_distribution))\n",
    "        # from ipdb import set_trace; set_trace()\n",
    "        if wrd_idx not in exclude: \n",
    "            samples.append(wrd_idx)\n",
    "            \n",
    "    #print('samples: ', samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embeds():\n",
    "    t0 = time.time()\n",
    "    print(\"Started create_embeds: \", t0)\n",
    "    word_counter = Counter()\n",
    "    n_docs = 0\n",
    "    embedding_size = 128\n",
    "    user_dict = defaultdict(list)\n",
    "    c = 0\n",
    "    with open(train_file,\"r\") as fid:\n",
    "        for line in fid:\n",
    "#             c += 1\n",
    "#             if c >25:\n",
    "#                 break\n",
    "#             print(\"len\", len(line))\n",
    "            if len(line) > 1:\n",
    "                message = line.split(\" \")\n",
    "                #print(message)\n",
    "                content = tokenizer.tokenize(\" \".join(message[2:]))\n",
    "                content = [word for word in content if not regex.match(word)]\n",
    "           \n",
    "                user_dict[message[0]].extend(content)\n",
    "                word_counter.update(content)\n",
    "                n_docs += 1\n",
    "    #print(word_counter)\n",
    "    #keep only words that occur at least min_word_freq times\n",
    "    wc = {w:c for w,c in word_counter.items() if c > min_word_freq}        \n",
    "    tw = sorted(wc.items(), key=lambda x:x[1], reverse=True)\n",
    "    top_words = {w[0]:i for i,w in enumerate(tw[:vocabulary_size])}\n",
    "    \n",
    "    embed_matrix, dictionary, unknownWords = load_embeddings(embed, top_words, embedding_size)\n",
    "    \n",
    "#     print('unknown words len: ', unknownWords)\n",
    "#     print('top words len before: ', top_words)\n",
    "\n",
    "    for w in unknownWords:\n",
    "        del top_words[w]\n",
    "    \n",
    "    print('top words len after: ', len(top_words))\n",
    "\n",
    "    wrd2idx = {w:i for i,w in enumerate(top_words.keys())}\n",
    "    \n",
    "    #finding unigram probability\n",
    "    unigram_cnt = [c for w, c in top_words.items()]    \n",
    "    total = sum(unigram_cnt)\n",
    "    unigram_prob = [c*1.0/total for c in unigram_cnt]\n",
    "    \n",
    "    #generate the embedding matrix\n",
    "    print(\"embed_matrix shape\",embed_matrix.shape)\n",
    "    emb_size = embed_matrix.shape[1]\n",
    "    print(emb_size)\n",
    "    E = np.zeros((len(wrd2idx),int(emb_size)))\n",
    "    for wrd,idx in wrd2idx.items(): \n",
    "        E[:] = embed_matrix[top_words[wrd],:]\n",
    "    print(\"E shape\",E.shape)\n",
    "    pickle.dump([E, unigram_prob, wrd2idx, word_counter, len(user_dict.keys())], open(output_pkl, 'wb'))\n",
    "    #print(user_dict)\n",
    "    print(\"Finished create_embeds: \", time.time() - t0)\n",
    "    \n",
    "    return user_dict, wc, wrd2idx, n_docs, unigram_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob):\n",
    "    print(\"Started user train\")\n",
    "    start = time.time()\n",
    "    prev_user, prev_user_data, prev_ctxscores, prev_neg_samples  = None, [], [], []\n",
    "    full_train = []\n",
    "    rng = np.random.RandomState(seed)\n",
    "    c = 0\n",
    "    with open(train_file, \"r\") as fid:\n",
    "        for j, line in enumerate(fid):\n",
    "            c += 1\n",
    "            if c > 30:\n",
    "                break\n",
    "            if len(line) <= 1:\n",
    "                continue\n",
    "            ################## Changed this to following ###########\n",
    "            #message = line.lower().split()\n",
    "            #user_id = message[0]\n",
    "            #content = message[1:]\n",
    "            #######################################################\n",
    "            message = line.split(\" \")\n",
    "            #print(message)\n",
    "            user_id = message[0]\n",
    "            content = tokenizer.tokenize(\" \".join(message[2:]))\n",
    "            content = [word for word in content if not regex.match(word)]\n",
    "\n",
    "            negative_samples = multinomial_samples(unigram_prob, [], 10)\n",
    "            #print('negative samples: ', negative_samples)\n",
    "\n",
    "            #convert to indices\n",
    "            msg_idx = [wrd2idx[w] for w in content if w in wrd2idx]\n",
    "            \n",
    "            if prev_user == None:  # first user\n",
    "                prev_user = user_id\n",
    "            elif user_id != prev_user or j == n_docs - 1: # this user_id is seen for the first time\n",
    "\n",
    "                assert len(prev_user_data) == len(prev_neg_samples)\n",
    "                shuf_idx = np.arange(len(prev_user_data))\n",
    "                # shuffle numbers\n",
    "                rng.shuffle(shuf_idx)\n",
    "                \n",
    "                # fill these lists with the same data in a different order\n",
    "                prev_user_data = [prev_user_data[i] for i in shuf_idx]\n",
    "                prev_neg_samples = [prev_neg_samples[i] for i in shuf_idx]\n",
    "                \n",
    "                #uncomment the if-else if train is len 0\n",
    "                # 90-10 train-test split\n",
    "                split = int(len(prev_user_data)*.9)\n",
    "                train = prev_user_data[:split]\n",
    "                test  = prev_user_data[split:]\n",
    "                    \n",
    "                neg_samples = prev_neg_samples[:split]\n",
    "\n",
    "                # each training instance consists of:\n",
    "                # [user_name, train docs, test docs, negative samples]\n",
    "                full_train.append([prev_user, train, test, neg_samples])\n",
    "\n",
    "                prev_user_data = []\n",
    "                prev_neg_samples = []\n",
    "                \n",
    "            prev_user = user_id\n",
    "            prev_user_data.append(msg_idx)\n",
    "            prev_neg_samples.append(negative_samples)\n",
    "        \n",
    "        full_train.append([prev_user, train, test, neg_samples])\n",
    "        print('prev user: ', prev_user)\n",
    "        print('prev user data: ', train)\n",
    "        print('prev neg samples: ', prev_neg_samples)\n",
    "        \n",
    "    print(\"Finished user_train\", time.time() - start)\n",
    "    pickle.dump(full_train, open(output, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started create_embeds:  1525022119.662554\n",
      "dictionary length 100000\n",
      "dictionary keys 100000\n",
      "word dict key:  you\n",
      "word dict val:  0\n",
      "dictionary keys UNK\n",
      "dictionary vals 0\n",
      "word embeds:  100000\n",
      "word embeds:  128\n",
      "embed array len:  154\n",
      "embed array len:  128\n",
      "top words len after:  151\n",
      "embed_matrix shape (154, 128)\n",
      "128\n",
      "E shape (151, 128)\n",
      "Finished create_embeds:  0.16174864768981934\n",
      "Started user train\n",
      "prev user:  Khapsee\n",
      "prev user data:  [[28, 2, 76, 2, 139]]\n",
      "prev neg samples:  [[110, 145, 116, 93, 120, 81, 58, 56, 106, 149]]\n",
      "Finished user_train 0.01563239097595215\n"
     ]
    }
   ],
   "source": [
    "user_dict, wc, wrd2idx, n_docs, unigram_prob = create_embeds()\n",
    "\n",
    "create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
