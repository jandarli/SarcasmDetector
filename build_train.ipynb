{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "#from ipdb import set_trace\n",
    "#from negative_samples import negative_sampler\n",
    "import numpy as np\n",
    "import os\n",
    "#from sma_toolkit import embeddings as emb_utils \n",
    "#import streaming_pickle as stPickle\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import parameters\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "MIN_DOC_LEN=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Negative Samples that are user specific and then using negative samples and the previous user word ids create user specific train data. create_user_train function is similar to the batch process of skigram model of word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = \"word2vec_nce.model\"\n",
    "train_file = \"user_text_sample.txt\"\n",
    "output_pkl = \"train_embeddings.pkl\"\n",
    "vocabulary_size = 100000\n",
    "min_word_freq = 0\n",
    "seed = 42\n",
    "neg_samples = 10\n",
    "output = \"user_train_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(filename, wordDict, embedding_size):\n",
    "    dictionary, steps, word_embeds = pickle.load(open(filename, 'rb'))        \n",
    "    \n",
    "    print(\"dictionary length\",len(dictionary))\n",
    "    print(\"dictionary keys\",len(dictionary.keys()))\n",
    "    \n",
    "    key = list(wordDict.keys())[0]\n",
    "    print('word dict key: ', key)\n",
    "    print('word dict val: ', wordDict[key])\n",
    "    \n",
    "    print(\"dictionary keys\",list(dictionary.keys())[0])\n",
    "    print(\"dictionary vals\",list(dictionary.values())[0])\n",
    "    \n",
    "    print(\"word embeds: \", len(word_embeds))\n",
    "    print(\"word embeds: \", len(word_embeds[0]))\n",
    "    \n",
    "    embedding_array = np.zeros((len(wordDict), embedding_size))\n",
    "    knownWords = list(wordDict.keys())\n",
    "    unknownWords = []\n",
    "    \n",
    "    #print(knownWords)\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        \n",
    "        w = knownWords[i]\n",
    "        if w in dictionary:\n",
    "            index = dictionary[w]\n",
    "        elif w.lower() in dictionary:\n",
    "            index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "        else:\n",
    "            unknownWords.append(w)\n",
    "#             embedding_array[i] = np.random.rand(embedding_size) * 0.02 - 0.01\n",
    "    #print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "    #print \"embedding_array shape\",embedding_array.shape\n",
    "\n",
    "    print('embed array len: ', len(embedding_array))\n",
    "    print('embed array len: ', len(embedding_array[0]))\n",
    "    return embedding_array, dictionary, unknownWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neg_samples(user_dict, wc):\n",
    "    neg_dict = defaultdict(list)\n",
    "    #sample = np.random.choice(vocabulary_size, num_sampled, p=unigram_prob, replace=False)\n",
    "   \n",
    "    for user, message in user_dict.items():\n",
    "        user_wrd = set(message)\n",
    "        word_corpus = set(wc.keys())\n",
    "        diff = word_corpus - user_wrd        \n",
    "        neg_dict[user] = random.sample(diff, parameters.sent_idx)\n",
    "    return neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from negative_samples.py of samiroid\n",
    "def multinomial_samples(unigram_distribution, exclude=[], n_samples=1):\n",
    "    samples = []        \n",
    "    while len(samples) != n_samples:            \n",
    "        wrd_idx = np.argmax(np.random.multinomial(1, unigram_distribution))\n",
    "        # from ipdb import set_trace; set_trace()\n",
    "        if wrd_idx not in exclude: \n",
    "            samples.append(wrd_idx)                        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embeds():\n",
    "    t0 = time.time()\n",
    "    word_counter = Counter()\n",
    "    n_docs = 0\n",
    "    embedding_size = 128\n",
    "    user_dict = defaultdict(list)\n",
    "    c = 0\n",
    "    with open(train_file,\"r\") as fid:\n",
    "        for line in fid:\n",
    "#             print(\"len\", len(line))\n",
    "            if len(line) > 1:\n",
    "                message = line.split()\n",
    "                user_dict[message[0]].extend(message[1:])\n",
    "                word_counter.update(message[1:])\n",
    "                n_docs += 1\n",
    "    \n",
    "    #keep only words that occur at least min_word_freq times\n",
    "    wc = {w:c for w,c in word_counter.items() if c > min_word_freq} \n",
    "    #finding unigram probability\n",
    "    unigram_cnt = [c for w, c in wc.items()]\n",
    "    total = sum(unigram_cnt)\n",
    "    unigram_prob = [c*1.0/total for c in unigram_cnt]\n",
    "        \n",
    "    tw = sorted(wc.items(), key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    top_words = {w[0]:i for i,w in enumerate(tw[:vocabulary_size])}\n",
    "    print(type(top_words))    \n",
    "\n",
    "    embed_matrix, dictionary, unknownWords = load_embeddings(embed, top_words, embedding_size)\n",
    "    \n",
    "    print('unknown words len: ', unknownWords)\n",
    "    print('top words len before: ', top_words)\n",
    "\n",
    "    for w in unknownWords:\n",
    "        del top_words[w]\n",
    "    \n",
    "    print('top words len after: ', len(top_words))\n",
    "\n",
    "    wrd2idx = {w:i for i,w in enumerate(top_words.keys())}\n",
    "#     get_OOEVs(embed_matrix, wrd2idx)\n",
    "    \n",
    "    #full_E, full_wrd2idx = emb_utils.read_embeddings(args.emb,top_words)        \n",
    "#     embeddings = tf.Variable(embed_matrix, dtype=tf.float32)\n",
    "#     print(embeddings)\n",
    "    #generate the embedding matrix\n",
    "    print(\"embed_matrix shape\",embed_matrix.shape)\n",
    "    emb_size = embed_matrix.shape[1]\n",
    "    print(emb_size)\n",
    "    E = np.zeros((len(wrd2idx),int(emb_size)))\n",
    "    for wrd,idx in wrd2idx.items(): \n",
    "        E[:] = embed_matrix[top_words[wrd],:]\n",
    "    print(\"E shape\",E.shape)\n",
    "    pickle.dump([E,unigram_prob,wrd2idx,word_counter,len(user_dict.keys())], open(output_pkl, 'wb'))\n",
    "    \n",
    "    return user_dict, wc, wrd2idx, n_docs, unigram_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob):\n",
    "#     negative_samples = get_neg_samples(user_dict, wc)\n",
    "    #print(negative_samples)\n",
    "#     negative_samples = multinomial_samples(unigram_prob, [], 10)\n",
    "#     print('negative samples: ', negative_samples)\n",
    "\n",
    "    prev_user, prev_user_data, prev_ctxscores, prev_neg_samples  = None, [], [], []\n",
    "    full_train = []\n",
    "#     f_train = open(output,\"wb\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "    with open(train_file, \"r\") as fid:\n",
    "        for j, line in enumerate(fid):\n",
    "            if len(line) <= 1:\n",
    "                continue\n",
    "\n",
    "            message = line.lower().split()\n",
    "            user_id = message[0]\n",
    "            content = message[1:]\n",
    "\n",
    "            negative_samples = multinomial_samples(unigram_prob, [], 10)\n",
    "            #print('negative samples: ', negative_samples)\n",
    "\n",
    "            #convert to indices\n",
    "            msg_idx = [wrd2idx[w] for w in content if w in wrd2idx]\n",
    "#             neg_idx = [wrd2idx[w] for w in negative_samples[user_id] if w in wrd2idx]\n",
    "            \n",
    "            if prev_user == None:  # first user\n",
    "                prev_user = user_id\n",
    "            elif user_id != prev_user or j == n_docs - 1: # this user_id is seen for the first time\n",
    "\n",
    "#                 print('user data len: ', len(prev_user_data), 'prev neg len: ', len(prev_neg_samples))\n",
    "                assert len(prev_user_data) == len(prev_neg_samples)\n",
    "\n",
    "#                 if len(prev_user_data) > 0:\n",
    "                # get numbers in range [0, len(prev_user_data))\n",
    "                shuf_idx = np.arange(len(prev_user_data))\n",
    "                # shuffle numbers\n",
    "                rng.shuffle(shuf_idx)\n",
    "\n",
    "                # fill these lists with the same data in a different order\n",
    "                prev_user_data = [prev_user_data[i] for i in shuf_idx]\n",
    "                prev_neg_samples = [prev_neg_samples[i] for i in shuf_idx]\n",
    "                \n",
    "                #uncomment the if-else if train is len 0\n",
    "                # 90-10 train-test split\n",
    "                split = int(len(prev_user_data)*.9)\n",
    "#                 if len(prev_user_data) > 1:\n",
    "                train = prev_user_data[:split]\n",
    "                test  = prev_user_data[split:]\n",
    "#                 else:\n",
    "#                 train = prev_user_data\n",
    "#                 test = prev_user_data\n",
    "\n",
    "#                 print('train: ', train)\n",
    "                    \n",
    "                neg_samples = prev_neg_samples[:split]\n",
    "                #print(\"neg_samples\",neg_samples)\n",
    "                # each training instance consists of:\n",
    "                # [user_name, train docs, test docs, negative samples]\n",
    "                full_train.append([prev_user, train, test, neg_samples])\n",
    "                prev_user_data = []\n",
    "                prev_neg_samples = []\n",
    "\n",
    "            prev_user = user_id\n",
    "#             if len(msg_idx) > 0:\n",
    "            prev_user_data.append(msg_idx)\n",
    "#             if len(neg_idx) > 0:\n",
    "            prev_neg_samples.append(negative_samples)\n",
    "#             if j == 3:\n",
    "#                 break\n",
    "\n",
    "        print('prev user: ', prev_user)\n",
    "        print('prev user data: ', len(prev_user_data))\n",
    "        print('prev neg samples: ', len(prev_neg_samples))\n",
    "    pickle.dump(full_train, open(output, 'wb'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dictionary length 100000\n",
      "dictionary keys 100000\n",
      "word dict key:  the\n",
      "word dict val:  0\n",
      "dictionary keys UNK\n",
      "dictionary vals 0\n",
      "word embeds:  100000\n",
      "word embeds:  128\n",
      "embed array len:  1526\n",
      "embed array len:  128\n",
      "unknown words len:  [\"don't\", \"it's\", \"you're\", \"doesn't\", \"I'm\", \"he's\", '4', 'Yes,', \"can't\", 'Yeah,', 'right?', \"I'll\", \"I've\", '(I', \"there's\", \"aren't\", '3', \"they're\", 'Well,', \"Don't\", 'no,', 'ghostproducers', 'too.', 'now.', \"it'll\", \"It's\", 'rapist', '2', \"won't\", '100%', \"that's\", 'neocons', \"Dumbledore's\", 'people.', 'But...', 'it.', \"didn't\", '-', '.', 'obviously.', \"shouldn't\", 'out.', 'is,', \"week's\", \"they've\", 'wrong.', 'NH.', 'surprising!', 'self-loathing', 'thing,', 'eh?', 'gold,', 'jerry,', 'gold.', '\"This', \"isn't\", '\"\"new', 'nigga\"\"', 'are.\"', 'in?', 'brave!', 'OK..', 'guns.', '\"And', 'non-ironically', '\"\"SJW.\"\"\"', 'gamers.', 'see,', 'sheep...', 'Probably,', 'democracy,', \"You'd\", \"they'd\", 'truth!', 'and/or', 'dick?', 'Snarking', 'Reddit.', 'suck..', 'rite?', '2fort?', '1', 'loop,', ';)', 'rhymes.', '9', 'typoon', 'you,', 'Ayyyy', 'lmaokai', '*hint*', '*HINT*', 'forum?', 'Ohhhh,', 'president,', 'name.', \"'murica\", 'wut', 'urself', \"'merica\", 'commie', 'lib.', '2meta5me', 'bad,', 'gains,', 'brah.', 'sounds,', 'immense,', \"Marvel's\", 'Symbiote', 'Earth,', '(which', 'doing)', '2nd/3rd', 'Spider-Man', 'film.', 'Year.', 'expansion.', 'toenail', 'NSFW.', 'Injuratu', 'injuratu,', 'astia', 'ameninta', 'intimideaza.', 'sa-ti', 'faci', 'despre', 'arata,', 'imagineaza-ti', 'muchia', '200m', 'scade', '10m', '\"Poate', 'uitat', 'virgula', 'era:', '\"\"Acesta', 'fugit,', 'cumpere', 'mancare', 'incalzeasca', 'copiii', 'mainile!\"\"\"', '~~Half-Life', '3~~', '367', 'confirmed!', 'A-OK', 'horizon!', 'reasonable.', '...but', 'can...', '\"Yeah,', 'diminishes...', 'self-driving', '\"\"malfunction\"\"', '\"\"organ', 'quota\"\"', ':D\"', 'while.', '**SHUT', 'MISOGYNIST**', 'shithole.', '\"So', '\"\"the', 'foot\"\"', 'with)', 'less.\"', 'funny.', 'dislikes.', 'internships', 'fields.', 'words.', 'sex-obsessed', 'men.', '\"It', '\"\"shut', 'asshole\"\"\"', \"women's\", 'patriarchy.', 'stalked.', '\"My', '\"\"70\\'s', 'vans.\"\"\"', 'anymore;', 'manwich', 'crumbles!', 'twoX', 'article.', '*No', 'one*', 'spending.', 'confinement!', 'religion!', 'know,', 'points.', 'thats', \"'members.\", 'news,', 'great!', 'things.', \"Haven't\", 'before.', 'countries,', 'Wingblade', 'senpai', 'plz', 'DarkShanks', ':d', \"They're\", 'mana,', 'rogue/warlock', 'maybe.', 'Aggro', 'OTK', '30', 'minion', 'wrath.', 'entomb,', 'duh...', 'Word:', 'Pain,', 'unplayable.', '\"\"\"Industrial', 'turbines,', 'backup,', 'dioxide\"\"', 'Loooool', 'buddy,', 'suuuure,', 'change...\"', 'provinces,', 'yea,', \"we'll\", 'happens.', 'azr', '6', 'kinda', 'sexist.', 'opinion.', 'edit.', 'ends.', 'takbir', 'secular,', 'OMG,', 'lol!', 'knockoff', 'F-35,', 'duh.', 'message,', 'phrase.', 'Exactly.', 'citizen,', '~~our', 'governments~~', 'world....', 'wait,', 'government.', 'service.', 'Yep,', 'instakill', 'headshots', 'Courage.', 'design.', \"weren't\", 'long-range', 'missile?', 'Emacs!', 'dad?', 'jungler', 'intentions,', 'best.', 'worlds,', 'Tahm', 'Kench', 'does.', 'would,', 'Murican,', 'time,', 'internet.', 'though...', 'days..', 'skins.', 'word.', 'all?', 'it,', 'Man!', 'Wot', 'side,', 'Dumbledore;', 'man.', 'exist;', \"men's\", 'attention.', 'round?', 'sex,', 'relationship,', \"'everyone\", \"non-mono'\", 'as),', 'interesting...', 'accomplishment,', 'obviously!', '\"Heck,', \"I'd\", '\"\"two', 'Bookers\"\"\"', 'game.', 'Palestinians:', '2000', '!', 'wall!', 'Stuckmann', 'h2o', 'yet,', 'Conquest.', 'Junkrat', 'challenging.', 'innervate,', 'anti-consumer', '\"He', 'change,', '\"\"changes\"\"!\"', 'that.', 'cutscene', 'End...', 'And...and....', 'Blundell', 'sucks!', 'then....', 'Jesus.', 'nail.', 'Mics', 'canuck', 'great,', 'stat,', 'Players.', 'Yea,', 'top.', '\"I', '\"\"erection\"\"\"', 'Nonono', 'thay', 'messed', '@', 'crimestat', 'well.', 'criminal,', 'criminal.', 'game,', 'radical.', 'Lobo?', 'loves.', 'Greek,', 'few?', 'bigger,', 'dogs.', 'help?', 'believe.', 'bucks?', 'sooo', 'progressive.', 'EDL', 'victims!', 'amirite?', 'geopolitics,', 'you?', 'up,', 'law...', 'Amen,', 'AMERICAN!', 'Holocaust.', 'impeach.', 'purpose,', 'flopping', 'side.', 'relationship.', 'doing?', '24', '17.', 'everything...', 'use.......', 'work,', 'memo...', 'tantrum', 'out?', 'Sahne', 'Apfelmus!', 'Humor!', 'Sicherheit', 'wichtigste', 'Nachricht', 'Prozess!', 'ADAC', 'wohl', 'genug', 'frisiert', 'years?', 'definition,', '*I*', 'racists,', 'subgroup,', 'us.', '1991', 'dunno', 'man,', 'today.', 'rankings?', 'era?', 'trans?', \"you'll\", 'back!', '\"Michael', 'Swartz', '\"\"Holy', 'Truth\"\"', 'Assange', '--', 'Damn!\"', \"That's\", 'attitude.', 'pomegranites', '(for', '$6', '1)', 'Popegranate.', 'Shhhh.....', 'Usenet.', 'all)', 'weapon,', 'rk5', 'Reddit', '*well....', 'see......', 'book....', 'game....*', 'Awww,', 'cute.', '..............', '1280x960', '\"\"\"Like', 'tenis.\"\"\"', 'STEM,', 'count,', 'TLDR:', 'California,', 'lived.', 'more.', 'compatable', 'ops.', 'civilization,', 'EATALLMEN', 'malez.', 'true,', 'else.', 'think,', 'else,', 'gotten.', 'foot,', 'issues.', 'No,', 'course,', 'her.', 'Yep...', 'holes,', 'squarely...', 'floors.', 'engineers.', 'dates.', '90', 'though.', 'point.', 'didnt', 'ww2', 'rengar?', '9/11', \"wasn't\", 'either.', 'course.', '\"say', '\"\"only', 'verizon\"\"\"', 'shitty', 'anti-Semitic...', 'rallies?', 'bought.', '\"It\\'s', \"she's\", '\"\"natural\"\"', 'lolol\"']\n",
      "top words len before:  {'the': 0, 'a': 1, 'to': 2, 'and': 3, 'is': 4, 'of': 5, 'I': 6, 'you': 7, 'that': 8, 'it': 9, 'for': 10, 'on': 11, 'in': 12, 'have': 13, 'not': 14, 'they': 15, 'be': 16, 'this': 17, 'are': 18, 'so': 19, 'as': 20, 'all': 21, 'with': 22, 'just': 23, 'but': 24, 'like': 25, 'their': 26, 'if': 27, 'will': 28, 'because': 29, \"don't\": 30, 'about': 31, 'what': 32, 'know': 33, 'But': 34, 'no': 35, 'we': 36, 'your': 37, 'when': 38, 'The': 39, 'up': 40, 'other': 41, 'do': 42, 'more': 43, \"it's\": 44, 'now': 45, 'would': 46, 'at': 47, 'those': 48, 'people': 49, 'there': 50, 'was': 51, 'You': 52, 'than': 53, \"you're\": 54, 'one': 55, 'want': 56, 'think': 57, 'his': 58, 'can': 59, 'get': 60, 'some': 61, 'only': 62, 'sure': 63, 'my': 64, 'its': 65, 'This': 66, \"doesn't\": 67, 'them': 68, 'who': 69, 'even': 70, 'he': 71, 'same': 72, 'has': 73, 'game': 74, 'how': 75, 'much': 76, 'Yeah': 77, \"I'm\": 78, 'going': 79, 'me': 80, \"he's\": 81, '4': 82, 'work': 83, 'had': 84, 'No': 85, 'Yes,': 86, 'down': 87, 'from': 88, 'against': 89, 'really': 90, 'fact': 91, 'So': 92, 'still': 93, \"can't\": 94, 'probably': 95, 'As': 96, 'tell': 97, 'forgot': 98, 'fuck': 99, 'Yeah,': 100, 'being': 101, 'then': 102, 'si': 103, 'should': 104, 'men': 105, 'say': 106, 'getting': 107, 'women': 108, 'too': 109, 'great': 110, 'thing': 111, 'see': 112, 'by': 113, 'or': 114, 'totally': 115, 'They': 116, 'something': 117, 'said': 118, 'guys': 119, 'teams': 120, 'right?': 121, 'Wow': 122, 'use': 123, \"I'll\": 124, 'love': 125, 'make': 126, 'need': 127, 'better': 128, 'many': 129, 'were': 130, 'die': 131, 'such': 132, \"I've\": 133, 'right': 134, 'go': 135, 'feel': 136, 'everyone': 137, 'knows': 138, 'out': 139, 'while': 140, 'throw': 141, 'find': 142, 'an': 143, 'back': 144, '(I': 145, 'playing': 146, 'been': 147, \"there's\": 148, 'never': 149, 'If': 150, 'over': 151, 'new': 152, 'mean': 153, \"aren't\": 154, '3': 155, 'everything': 156, 'way': 157, \"they're\": 158, 'damn': 159, 'wait': 160, 'team': 161, 'saying': 162, 'full': 163, 'Still': 164, 'always': 165, 'Well,': 166, 'doing': 167, 'We': 168, 'most': 169, 'real': 170, 'Well': 171, 'socialism': 172, 'could': 173, 'west': 174, 'true': 175, 'actual': 176, \"Don't\": 177, 'us': 178, 'anyone': 179, 'uses': 180, 'am': 181, 'release': 182, 'no,': 183, 'bad': 184, 'ghostproducers': 185, 'wants': 186, 'lot': 187, 'too.': 188, 'kids': 189, 'months': 190, 'Just': 191, 'winning': 192, 'seen': 193, 'now.': 194, 'gotta': 195, \"it'll\": 196, 'opportunity': 197, 'may': 198, 'cost': 199, 'somehow': 200, 'YOU': 201, 'mai': 202, 'o': 203, 'sa': 204, 'le': 205, 'parents': 206, 'starting': 207, 'unknown': 208, 'Because': 209, 'patriarchy': 210, 'every': 211, 'paid': 212, 'years': 213, 'likes': 214, \"It's\": 215, 'using': 216, 'rapist': 217, 'solved': 218, 'issues': 219, 'first': 220, 'happen': 221, 'since': 222, 'why': 223, 'technology': 224, '2': 225, 'per': 226, 'mana': 227, 'dem': 228, 'yeah': 229, 'definitely': 230, 'rely': 231, 'next': 232, 'nothing': 233, \"won't\": 234, 'two': 235, 'reason': 236, 'doubt': 237, 'best': 238, '100%': 239, 'heard': 240, 'development': 241, 'made': 242, 'own': 243, \"that's\": 244, 'very': 245, 'US': 246, 'neocons': 247, 'child': 248, 'good': 249, 'Are': 250, 'A': 251, 'cozy': 252, 'another': 253, 'country': 254, 'fancy': 255, \"Dumbledore's\": 256, 'did': 257, 'Good': 258, 'anything': 259, 'hate': 260, 'time': 261, 'job': 262, 'people.': 263, 'muh': 264, 'music': 265, 'Tap': 266, 'mental': 267, 'bomber': 268, 'list': 269, 'needs': 270, 'But...': 271, 'Trump': 272, 'looks': 273, 'Damage': 274, 'it.': 275, 'off': 276, \"didn't\": 277, 'exist': 278, '-': 279, '.': 280, 'Once': 281, 'obviously.': 282, 'How': 283, 'clearly': 284, 'videos': 285, 'search': 286, 'Maybe': 287, \"shouldn't\": 288, 'dont': 289, 'during': 290, 'hard': 291, 'her': 292, 'out.': 293, 'immigration': 294, 'problem': 295, 'is,': 296, 'she': 297, \"week's\": 298, 'bet': 299, 'Edward': 300, 'Snowden': 301, 'require': 302, \"they've\": 303, 'wrong.': 304, 'Yea': 305, 'NC': 306, 'NH.': 307, 'Very': 308, 'surprising!': 309, 'play': 310, 'east': 311, 'Lakers': 312, 'embracing': 313, 'self-loathing': 314, 'thing,': 315, 'eh?': 316, 'video': 317, 'gold,': 318, 'jerry,': 319, 'gold.': 320, '\"This': 321, 'meme': 322, \"isn't\": 323, 'funny': 324, 'none': 325, '\"\"new': 326, 'york': 327, 'nigga\"\"': 328, 'ones': 329, 'are.\"': 330, 'Where': 331, 'suck': 332, 'in?': 333, 'brave!': 334, 'OK..': 335, 'negate': 336, 'target': 337, 'shooting': 338, 'gunpowder': 339, 'guns.': 340, '\"And': 341, 'humor': 342, 'argue': 343, 'important': 344, 'throughout': 345, 'thread': 346, 'non-ironically': 347, 'term': 348, '\"\"SJW.\"\"\"': 349, 'Real': 350, 'gamers.': 351, 'see,': 352, 'special': 353, 'Bethesda': 354, 'sheep...': 355, 'Probably,': 356, 'undermine': 357, 'democracy,': 358, \"You'd\": 359, \"they'd\": 360, 'helping': 361, 'spread': 362, 'truth!': 363, 'admitting': 364, 'troll': 365, 'and/or': 366, 'dick?': 367, 'Snarking': 368, 'Reddit.': 369, 'fallout': 370, 'graphics': 371, 'suck..': 372, 'rite?': 373, '2fort?': 374, 'Techno': 375, '1': 376, 'bar': 377, 'loop,': 378, 'repeat': 379, 'minutes': 380, 'EDM': 381, 'scene': 382, 'proper': 383, 'MLA': 384, 'format': 385, 'cited': 386, 'recognition': 387, 'john': 388, 'harbaugh': 389, 'texas': 390, 'beat': 391, 'Houston': 392, ';)': 393, 'sources': 394, 'chip': 395, 'brown': 396, 'Dre': 397, 'popped': 398, 'pills': 399, 'started': 400, 'sucking': 401, 'Infinitely': 402, 'lyrics': 403, 'rhymes.': 404, 'born': 405, '9': 406, 'later': 407, 'after': 408, 'typoon': 409, 'Christian': 410, 'folk': 411, 'rock': 412, 'band': 413, 'U': 414, 'gon': 415, 'logged': 416, 'you,': 417, 'Man': 418, 'heartbreaking': 419, 'high': 420, 'hopes': 421, 'worlds': 422, 'lose': 423, 'solo': 424, 'q': 425, 'Ayyyy': 426, 'lmaokai': 427, '*hint*': 428, 'patch': 429, 'notes': 430, '*HINT*': 431, 'What': 432, 'forum?': 433, 'Ohhhh,': 434, 'old': 435, 'Mexican': 436, 'president,': 437, 'name.': 438, \"'murica\": 439, 'gun': 440, 'matter': 441, 'wut': 442, 'urself': 443, 'u': 444, \"'merica\": 445, 'hating': 446, 'commie': 447, 'lib.': 448, '2meta5me': 449, 'mobility': 450, 'bad,': 451, 'kill': 452, 'ur': 453, 'gains,': 454, 'brah.': 455, 'nice': 456, 'sounds,': 457, 'scale': 458, 'Infinity': 459, 'War': 460, 'immense,': 461, \"Marvel's\": 462, 'introduce': 463, 'Symbiote': 464, 'Space': 465, 'rather': 466, 'Earth,': 467, 'unless': 468, 'Marvel': 469, 'Secret': 470, 'Wars': 471, 'route': 472, '(which': 473, 'doing)': 474, 'golden': 475, 'him': 476, 'Black': 477, 'Suit': 478, 'Venom': 479, 'justice': 480, '2nd/3rd': 481, 'Spider-Man': 482, 'film.': 483, 'Except': 484, 'February': 485, 'Leap': 486, 'Year.': 487, 'normally': 488, 'less': 489, 'expansion.': 490, 'Yet': 491, 'guy': 492, 'stabbing': 493, 'toenail': 494, 'NSFW.': 495, 'DAMN': 496, 'TRUMP': 497, 'Injuratu': 498, 'ca': 499, 'injuratu,': 500, 'dar': 501, 'astia': 502, 'ameninta': 503, 'sau': 504, 'intimideaza.': 505, 'Ca': 506, 'sa-ti': 507, 'faci': 508, 'imagine': 509, 'despre': 510, 'cum': 511, 'ar': 512, 'arata,': 513, 'imagineaza-ti': 514, 'un': 515, 'cub': 516, 'cu': 517, 'muchia': 518, 'de': 519, '200m': 520, 'scade': 521, '10m': 522, '\"Poate': 523, 'au': 524, 'uitat': 525, 'virgula': 526, 'era:': 527, '\"\"Acesta': 528, 'fugit,': 529, 'cumpere': 530, 'mancare': 531, 'incalzeasca': 532, 'copiii': 533, 'mainile!\"\"\"': 534, '~~Half-Life': 535, '3~~': 536, 'Bleach': 537, '367': 538, 'confirmed!': 539, 'A-OK': 540, 'dropping': 541, 'education': 542, 'available': 543, 'joke': 544, 'shop': 545, 'funds': 546, 'prospects': 547, 'war': 548, 'brewing': 549, 'horizon!': 550, 'Being': 551, 'reasonable.': 552, '...but': 553, 'obligated': 554, 'trash': 555, 'can...': 556, '\"Yeah,': 557, 'maybe': 558, 'promote': 559, 'healthier': 560, 'lifestyles': 561, 'demand': 562, 'organ': 563, 'donation': 564, 'diminishes...': 565, 'Or': 566, 'surreptitious': 567, 'programming': 568, 'self-driving': 569, 'cars': 570, '\"\"malfunction\"\"': 571, 'order': 572, 'meet': 573, '\"\"organ': 574, 'quota\"\"': 575, ':D\"': 576, 'conspiracy': 577, 'leaders': 578, 'cover': 579, 'under': 580, 'bus': 581, 'once': 582, 'while.': 583, '**SHUT': 584, 'UP': 585, 'MISOGYNIST**': 586, 'Go': 587, 'SRS': 588, 'shithole.': 589, '\"So': 590, '\"\"the': 591, 'shoe': 592, 'foot\"\"': 593, 'wage': 594, 'gap': 595, 'myth': 596, 'begin': 597, 'with)': 598, 'watch': 599, 'gives': 600, 'shit': 601, 'less.\"': 602, 'patriarchal': 603, 'oppression': 604, 'Sharon': 605, 'Osbourne': 606, 'funny.': 607, 'ridiculously': 608, 'disproportionate': 609, 'ratio': 610, 'dislikes.': 611, 'due': 612, 'internships': 613, 'programs': 614, 'aimed': 615, 'STEM': 616, 'fields.': 617, 'foreskin': 618, 'Thank': 619, 'kind': 620, 'words.': 621, 'sex': 622, 'objects': 623, 'satisfaction': 624, 'sex-obsessed': 625, 'men.': 626, '\"It': 627, '\"\"shut': 628, 'shallow': 629, 'asshole\"\"\"': 630, \"women's\": 631, 'rid': 632, 'patriarchy.': 633, 'appropriate': 634, 'consequence': 635, 'smoking': 636, 'stalked.': 637, '\"My': 638, 'teenager': 639, 'calls': 640, '\"\"70\\'s': 641, 'vans.\"\"\"': 642, 'drives': 643, 'coronation': 644, 'drive': 645, 'anymore;': 646, 'traffic': 647, 'Rest': 648, 'assured': 649, 'lockout': 650, 'laws': 651, 'again': 652, 'wanna': 653, 'easier': 654, 'pour': 655, 'manwich': 656, 'sauce': 657, 'veggie': 658, 'crumbles!': 659, 'Some': 660, 'truly': 661, 'discussion': 662, 'twoX': 663, 'article.': 664, '*No': 665, 'one*': 666, 'wasteful': 667, 'spending.': 668, 'Those': 669, 'chickens': 670, 'living': 671, 'luxurious': 672, 'printer': 673, 'paper': 674, 'sized': 675, 'confinement!': 676, 'stem': 677, 'cell': 678, 'research': 679, 'goes': 680, 'religion!': 681, 'cool': 682, 'bean': 683, 'know,': 684, 'points.': 685, 'OT': 686, 'greatest': 687, 'sliced': 688, 'bread': 689, 'thats': 690, 'lives': 691, 'fraught': 692, 'scandals': 693, 'Pepe': 694, 'farms': 695, \"'members.\": 696, 'news,': 697, 'market': 698, 'system': 699, 'forced': 700, 'increase': 701, 'wages': 702, 'teachers': 703, 'great!': 704, 'Great': 705, 'smart': 706, 'create': 707, 'things.': 708, \"Haven't\": 709, 'Internet': 710, 'before.': 711, 'Look': 712, 'flags': 713, 'across': 714, 'world': 715, 'regions': 716, 'countries,': 717, 'connection': 718, 'Virginia': 719, 'flag': 720, 'Wingblade': 721, 'senpai': 722, 'notice': 723, 'plz': 724, 'DarkShanks': 725, ':d': 726, \"They're\": 727, 'close': 728, 'worth': 729, 'mana,': 730, 'value': 731, 'crystal': 732, 'compared': 733, 'hero': 734, 'powers': 735, 'except': 736, 'rogue/warlock': 737, 'maybe.': 738, 'All': 739, 'goddamn': 740, 'Aggro': 741, 'Warrior': 742, 'decks': 743, 'ruining': 744, 'HS': 745, 'experience': 746, 'OTK': 747, '30': 748, 'minion': 749, 'draw': 750, 'into': 751, 'holy': 752, 'wrath.': 753, 'Second': 754, 'entomb,': 755, 'duh...': 756, 'Dies': 757, 'Shadow': 758, 'Word:': 759, 'Pain,': 760, 'unplayable.': 761, 'Oh': 762, 'iron': 763, 'beak': 764, 'owls': 765, 'seeing': 766, 'ladder': 767, 'Nah': 768, 'visual': 769, 'glitch': 770, '\"\"\"Industrial': 771, 'wind': 772, 'turbines,': 773, 'which': 774, 'fossil': 775, 'fuel': 776, 'backup,': 777, 'reduce': 778, 'carbon': 779, 'dioxide\"\"': 780, 'Loooool': 781, 'okay': 782, 'buddy,': 783, 'suuuure,': 784, 'dirty': 785, 'renewable': 786, 'energy': 787, 'slow': 788, 'climate': 789, 'change...\"': 790, 'Thankfully': 791, 'successfully': 792, 'already': 793, 'implemented': 794, 'cap': 795, 'trade': 796, 'provinces,': 797, 'widespread': 798, 'implementation': 799, 'immanent': 800, 'yea,': 801, \"we'll\": 802, 'happens.': 803, 'azr': 804, 'far': 805, 'player': 806, '6': 807, 'seems': 808, 'kinda': 809, 'sexist.': 810, 'stating': 811, 'opinion.': 812, 'Samoa': 813, 'edit.': 814, 'CIA': 815, 'trying': 816, 'tie': 817, 'loose': 818, 'ends.': 819, 'takbir': 820, 'secular,': 821, 'OMG,': 822, 'lol!': 823, 'Behind': 824, 'Chinese': 825, 'stole': 826, 'tons': 827, 'information': 828, 'successful': 829, 'knockoff': 830, 'F-35,': 831, 'duh.': 832, 'Misanthropic': 833, 'message,': 834, 'apt': 835, 'phrase.': 836, 'Exactly.': 837, 'citizen,': 838, 'wish': 839, '~~our': 840, 'governments~~': 841, 'worry': 842, 'fucking': 843, 'rest': 844, 'world....': 845, 'wait,': 846, 'mistake': 847, 'our': 848, 'government.': 849, 'free': 850, 'labour': 851, 'delivery': 852, 'service.': 853, 'Yep,': 854, 'instakill': 855, 'headshots': 856, 'auto': 857, 'weapon': 858, 'FPS': 859, 'Courage.': 860, 'wheels': 861, 'versatile': 862, 'bipedal': 863, 'design.': 864, \"weren't\": 865, 'long-range': 866, 'missile?': 867, 'Emacs!': 868, 'Hi': 869, 'dad?': 870, 'decent': 871, 'jungler': 872, 'states': 873, 'intentions,': 874, 'best.': 875, 'Looking': 876, 'performance': 877, 'worlds,': 878, 'any': 879, 'Tahm': 880, 'Kench': 881, 'does.': 882, 'would,': 883, 'Murican,': 884, 'time,': 885, 'moving': 886, 'complain': 887, 'internet.': 888, 'though...': 889, 'Their': 890, 'players': 891, 'alone': 892, 'able': 893, 'carry': 894, 'together': 895, 'days..': 896, 'allocate': 897, 'resources': 898, 'skins.': 899, 'Moist': 900, 'least': 901, 'sexy': 902, 'word.': 903, 'all?': 904, 'it,': 905, 'Man!': 906, 'Wot': 907, 'Phineus': 908, 'side,': 909, 'believed': 910, 'genuinely': 911, 'disagree': 912, 'Dumbledore;': 913, 'Kingsley': 914, 'man.': 915, 'lesbians': 916, 'exist;': 917, 'pretend': 918, \"men's\": 919, 'attention.': 920, 'round?': 921, 'article': 922, 'author': 923, 'adequately': 924, 'defends': 925, 'implicit': 926, 'assumption': 927, 'cheat': 928, 'sex,': 929, 'thrill': 930, 'breaking': 931, 'rules': 932, 'relationship,': 933, 'also': 934, \"'everyone\": 935, \"non-mono'\": 936, 'easy': 937, 'solution': 938, 'presented': 939, 'as),': 940, 'certainly': 941, 'interesting...': 942, 'wealthy': 943, 'having': 944, 'predeceased': 945, 'accomplishment,': 946, 'obviously!': 947, '\"Heck,': 948, \"I'd\": 949, 'accept': 950, '\"\"two': 951, 'batches': 952, 'Bookers\"\"\"': 953, 'actually': 954, 'shoot': 955, 'game.': 956, 'Clearly': 957, 'missed': 958, 'update': 959, 'That': 960, 'Physics': 961, 'literally': 962, 'apply': 963, 'these': 964, 'Palestinians:': 965, 'believe': 966, 'occupation': 967, '2000': 968, '!': 969, 'France': 970, 'gonna': 971, 'pay': 972, 'wall!': 973, 'Tell': 974, 'Chris': 975, 'Stuckmann': 976, 'h2o': 977, 'lean': 978, 'thang': 979, 'animations': 980, 'Then': 981, 'ask': 982, 'mom': 983, 'mow': 984, 'lawns': 985, 'superior': 986, 'taste': 987, 'Better': 988, 'yet,': 989, 'away': 990, 'shower': 991, 'curtain': 992, 'Sounds': 993, 'cheesy': 994, 'Revenge': 995, 'clone': 996, 'App': 997, 'Store': 998, 'Strictly': 999, 'Conquest.': 1000, 'lads': 1001, 'gone': 1002, 'Junkrat': 1003, 'intellectually': 1004, 'challenging.': 1005, 'innervate,': 1006, 'mad': 1007, 'madder': 1008, 'birthday': 1009, 'Profit': 1010, 'anti-consumer': 1011, '\"He': 1012, 'tupac': 1013, 'wanted': 1014, 'change,': 1015, 'obviously': 1016, 'listened': 1017, '\"\"changes\"\"!\"': 1018, 'acronyms': 1019, 'turn': 1020, 'settings': 1021, 'stop': 1022, 'that.': 1023, 'got': 1024, 'ending': 1025, 'cutscene': 1026, 'End...': 1027, 'And...and....': 1028, 'Blundell': 1029, 'sucks!': 1030, 'sideways': 1031, 'then....': 1032, 'loses': 1033, 'vote': 1034, 'thank': 1035, 'Jesus.': 1036, 'guess': 1037, 'hammer': 1038, 'nail.': 1039, 'Mics': 1040, 'puma': 1041, 'canuck': 1042, 'great,': 1043, 'Mark': 1044, 'Messier': 1045, 'Glorious': 1046, 'stat,': 1047, 'shields': 1048, 'Players.': 1049, 'Yea,': 1050, 'rallying': 1051, 'support': 1052, 'cause': 1053, 'helps': 1054, 'Looks': 1055, 'infected': 1056, 'top.': 1057, '\"I': 1058, 'called': 1059, '\"\"erection\"\"\"': 1060, 'Nonono': 1061, 'bring': 1062, 'sawed': 1063, 'Forgot': 1064, 'bug': 1065, 'beta': 1066, 'Idea': 1067, 'thay': 1068, 'messed': 1069, '@': 1070, 'final': 1071, 'tend': 1072, 'forget': 1073, 'things': 1074, 'owner': 1075, 'stolen': 1076, 'ship': 1077, 'gets': 1078, 'crimestat': 1079, 'raised': 1080, 'well.': 1081, 'criminal,': 1082, 'criminal.': 1083, 'game,': 1084, 'Citizen': 1085, 'Kane': 1086, 'radical.': 1087, 'keys': 1088, 'parallax': 1089, 'Lobo?': 1090, 'crap': 1091, 'wasting': 1092, 'talent': 1093, 'loves.': 1094, 'Greek,': 1095, 'democracy': 1096, 'trusted': 1097, 'Have': 1098, 'tried': 1099, 'taking': 1100, 'few?': 1101, 'alot': 1102, 'bigger,': 1103, 'cant': 1104, 'weird': 1105, 'kid': 1106, 'ancient': 1107, 'goin': 1108, 'school': 1109, 'works': 1110, 'smell': 1111, 'dead': 1112, 'dogs.': 1113, 'TJ': 1114, 'Collins': 1115, 'help?': 1116, 'Makes': 1117, 'sense': 1118, 'Marquez': 1119, 'upstanding': 1120, 'sportsman': 1121, 'believe.': 1122, 'Dollar': 1123, 'dollar': 1124, 'bucks?': 1125, 'dictating': 1126, 'wear': 1127, 'sooo': 1128, 'progressive.': 1129, 'EDL': 1130, 'victims!': 1131, 'Poor': 1132, 'multinational': 1133, 'corporation': 1134, 'amirite?': 1135, 'geopolitics,': 1136, 'you?': 1137, 'sign': 1138, 'up,': 1139, 'break': 1140, 'law...': 1141, 'Amen,': 1142, 'fellow': 1143, 'AMERICAN!': 1144, 'german': 1145, 'soldier': 1146, 'WWII': 1147, 'responsible': 1148, 'Holocaust.': 1149, 'clinton': 1150, 'impeach.': 1151, 'It': 1152, 'purpose,': 1153, 'dog': 1154, 'bowing': 1155, 'head': 1156, 'flopping': 1157, 'side.': 1158, 'absolutely': 1159, 'whatsoever': 1160, 'relationship.': 1161, 'Like': 1162, 'doing?': 1163, '24': 1164, '17.': 1165, 'dems': 1166, 'republicans': 1167, 'control': 1168, 'everything...': 1169, 'limited': 1170, 'use.......': 1171, 'added': 1172, 'work,': 1173, 'might': 1174, 'memo...': 1175, 'instead': 1176, 'throwing': 1177, 'temper': 1178, 'tantrum': 1179, 'walking': 1180, 'out?': 1181, 'George': 1182, 'W': 1183, 'Bush': 1184, 'VP': 1185, 'run': 1186, 'transition': 1187, 'saw': 1188, 'well': 1189, 'worked': 1190, 'Plus': 1191, 'Sahne': 1192, 'und': 1193, 'Apfelmus!': 1194, 'Dat': 1195, 'ma': 1196, 'schwarzer': 1197, 'Humor!': 1198, 'Das': 1199, 'ist': 1200, 'mit': 1201, 'Sicherheit': 1202, 'wichtigste': 1203, 'Nachricht': 1204, 'zu': 1205, 'Prozess!': 1206, 'Dem': 1207, 'ADAC': 1208, 'sind': 1209, 'Zahlen': 1210, 'wohl': 1211, 'noch': 1212, 'nicht': 1213, 'genug': 1214, 'frisiert': 1215, 'Canadian': 1216, 'par': 1217, 'illegal': 1218, 'Mexico': 1219, 'give': 1220, 'couple': 1221, 'hundred': 1222, 'years?': 1223, 'define': 1224, 'EVERY': 1225, 'MODERN': 1226, 'GOVERNMENT': 1227, 'IN': 1228, 'EXISTENCE': 1229, 'IS': 1230, 'SOCIALIST': 1231, 'according': 1232, 'definition,': 1233, '*I*': 1234, 'Im': 1235, 'plenty': 1236, 'genuine': 1237, 'racists,': 1238, 'identifying': 1239, 'crime': 1240, 'ethnic': 1241, 'subgroup,': 1242, 'racism': 1243, 'despite': 1244, 'try': 1245, 'us.': 1246, 'Of': 1247, 'course': 1248, 'LIBERAL': 1249, 'propaganda': 1250, 'herself': 1251, 'raise': 1252, 'finished': 1253, '1991': 1254, 'saddam': 1255, 'power': 1256, 'bound': 1257, 'trouble': 1258, 'line': 1259, 'Ya': 1260, 'Zionist': 1261, 'fabricated': 1262, 'massive': 1263, 'protests': 1264, 'pictures': 1265, 'dunno': 1266, 'man,': 1267, 'threw': 1268, 'pick': 1269, 'today.': 1270, 'rankings': 1271, 'based': 1272, 'rankings?': 1273, 'Is': 1274, 'situation': 1275, 'arise': 1276, 'causes': 1277, 'finish': 1278, 'exact': 1279, 'era?': 1280, 'trans?': 1281, 'cock': 1282, \"you'll\": 1283, 'back!': 1284, '\"Michael': 1285, 'Hastings': 1286, 'Aaron': 1287, 'Swartz': 1288, '\"\"Holy': 1289, 'Shit': 1290, 'assassinating': 1291, 'spreading': 1292, 'Truth\"\"': 1293, 'killed': 1294, 'Julian': 1295, 'Assange': 1296, 'gotten': 1297, 'hands': 1298, '--': 1299, 'God': 1300, 'Damn!\"': 1301, \"That's\": 1302, 'attitude.': 1303, 'Does': 1304, 'amused': 1305, 'inexpensive': 1306, 'pomegranites': 1307, 'live': 1308, '(for': 1309, 'reference': 1310, 'local': 1311, 'grocery': 1312, 'store': 1313, 'charges': 1314, '$6': 1315, '1)': 1316, 'spelling': 1317, 'Popegranate.': 1318, 'Shhhh.....': 1319, 'Usenet.': 1320, 'When': 1321, 'complete': 1322, 'easter': 1323, 'eggs': 1324, 'all)': 1325, 'tour': 1326, 'reward': 1327, 'second': 1328, 'weapon,': 1329, 'rk5': 1330, 'Too': 1331, 'single': 1332, 'payer': 1333, 'health': 1334, 'care': 1335, 'pie': 1336, 'sky': 1337, 'Welcome': 1338, 'Reddit': 1339, '*well....': 1340, 'see......': 1341, 'book....': 1342, 'game....*': 1343, 'Awww,': 1344, 'cute.': 1345, '..............': 1346, '1280x960': 1347, 'Overpowered': 1348, '\"\"\"Like': 1349, 'Far': 1350, 'Cry': 1351, 'tenis.\"\"\"': 1352, 'STEM,': 1353, 'count,': 1354, 'TLDR:': 1355, 'TRP': 1356, 'left': 1357, 'small': 1358, 'area': 1359, 'California,': 1360, 'whole': 1361, 'understanding': 1362, 'comes': 1363, 'few': 1364, 'places': 1365, 'where': 1366, 'lived.': 1367, 'social': 1368, 'security': 1369, 'number': 1370, 'fine': 1371, 'name': 1372, 'flair': 1373, 'more.': 1374, 'games': 1375, 'backwards': 1376, 'compatable': 1377, 'skate': 1378, 'black': 1379, 'ops.': 1380, 'Manage': 1381, 'show': 1382, 'ready': 1383, 'install': 1384, 'created': 1385, 'correct': 1386, 'civilization,': 1387, 'civilizations': 1388, 'terrible': 1389, 'spelled': 1390, 'christopher': 1391, 'nolan': 1392, 'EATALLMEN': 1393, 'hopefully': 1394, 'hillary': 1395, 'finally': 1396, 'fema': 1397, 'camps': 1398, 'round': 1399, 'malez.': 1400, 'chance': 1401, 'true,': 1402, 'else.': 1403, 'tablet': 1404, 'think,': 1405, 'review': 1406, 'stuff': 1407, 'amazon': 1408, 'else,': 1409, 'expensive': 1410, 'gotten.': 1411, 'More': 1412, 'airplane': 1413, 'races': 1414, 'yea': 1415, 'i': 1416, 'watching': 1417, 'flames': 1418, 'Be': 1419, 'six': 1420, 'foot,': 1421, 'solve': 1422, 'issues.': 1423, 'No,': 1424, 'CGI': 1425, 'course,': 1426, 'double': 1427, 'powder': 1428, 'cartridge': 1429, 'hand': 1430, 'loading': 1431, 'attacker': 1432, 'gave': 1433, 'consideration': 1434, 'victim': 1435, 'attacked': 1436, 'She': 1437, 'potential': 1438, 'lifetime': 1439, 'psychological': 1440, 'recovery': 1441, 'ahead': 1442, 'her.': 1443, 'Yep...': 1444, 'Walls': 1445, 'giant': 1446, 'gaps': 1447, 'holes,': 1448, 'boards': 1449, 'used': 1450, 'attached': 1451, 'squarely...': 1452, 'apparently': 1453, 'fashion': 1454, 'perfectly': 1455, 'matching': 1456, 'hardwood': 1457, 'floors.': 1458, 'engineers.': 1459, 'Getting': 1460, 'dates.': 1461, 'Prisma': 1462, 'Would': 1463, 'engine': 1464, 'door': 1465, 'opens': 1466, 'measurably': 1467, 'farther': 1468, '90': 1469, 'degrees': 1470, 'though.': 1471, 'Hypocrisy': 1472, 'finest': 1473, 'There': 1474, 'point.': 1475, 'Anyone': 1476, 'makes': 1477, 'blanket': 1478, 'statements': 1479, 'talking': 1480, 'Lol': 1481, 'Germany': 1482, 'didnt': 1483, 'ww2': 1484, 'rengar?': 1485, 'Mayor': 1486, 'press': 1487, 'briefing': 1488, '9/11': 1489, \"wasn't\": 1490, 'terrorism': 1491, 'either.': 1492, 'UK': 1493, 'course.': 1494, '\"say': 1495, 'commercials': 1496, '\"\"only': 1497, 'verizon\"\"\"': 1498, 'shitty': 1499, 'plot': 1500, 'Remember': 1501, 'leftists': 1502, 'guns': 1503, 'republican': 1504, 'TIL': 1505, 'South': 1506, 'Carolina': 1507, 'anti-Semitic...': 1508, 'Lets': 1509, 'lol': 1510, 'jk': 1511, 'takes': 1512, 'truth': 1513, 'says': 1514, 'rallies?': 1515, 'Totally': 1516, 'bought.': 1517, '\"It\\'s': 1518, 'ok': 1519, \"she's\": 1520, '\"\"natural\"\"': 1521, 'politician': 1522, 'lolol\"': 1523, 'begins': 1524, 'fellas': 1525}\n",
      "top words len after:  1034\n",
      "embed_matrix shape (1526, 128)\n",
      "128\n",
      "E shape (1034, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev user:  fragraptor\n",
      "prev user data:  7\n",
      "prev neg samples:  7\n"
     ]
    }
   ],
   "source": [
    "user_dict, wc, wrd2idx, n_docs, unigram_prob = create_embeds()\n",
    "create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
