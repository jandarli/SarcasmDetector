{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "#from ipdb import set_trace\n",
    "#from negative_samples import negative_sampler\n",
    "import numpy as np\n",
    "import os\n",
    "#from sma_toolkit import embeddings as emb_utils \n",
    "#import streaming_pickle as stPickle\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import parameters\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "MIN_DOC_LEN=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Negative Samples that are user specific and then using negative samples and the previous user word ids create user specific train data. create_user_train function is similar to the batch process of skigram model of word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = \"word2vec_nce.model\"\n",
    "train_file = \"user_text_sample.txt\"\n",
    "output_pkl = \"train_embeddings.pkl\"\n",
    "vocabulary_size = 100000\n",
    "min_word_freq = 5\n",
    "seed = 42\n",
    "neg_samples = 10\n",
    "output = \"user_train_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, wordDict, embedding_size):\n",
    "    dictionary, steps, word_embeds = pickle.load(open(filename, 'rb'))        \n",
    "    \n",
    "    print(\"dictionary length\",len(dictionary))\n",
    "    print(\"dictionary keys\",len(dictionary.keys()))\n",
    "    \n",
    "    key = list(wordDict.keys())[0]\n",
    "    print('word dict key: ', key)\n",
    "    print('word dict val: ', wordDict[key])\n",
    "    \n",
    "    print(\"dictionary keys\",list(dictionary.keys())[0])\n",
    "    print(\"dictionary vals\",list(dictionary.values())[0])\n",
    "    \n",
    "    print(\"word embeds: \", len(word_embeds))\n",
    "    print(\"word embeds: \", len(word_embeds[0]))\n",
    "    \n",
    "    embedding_array = np.zeros((len(wordDict), embedding_size))\n",
    "    knownWords = list(wordDict.keys())\n",
    "    unknownWords = []\n",
    "    \n",
    "    #print(knownWords)\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        \n",
    "        w = knownWords[i]\n",
    "        if w in dictionary:\n",
    "            index = dictionary[w]\n",
    "        elif w.lower() in dictionary:\n",
    "            index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "        else:\n",
    "            unknownWords.append(w)\n",
    "#             embedding_array[i] = np.random.rand(embedding_size) * 0.02 - 0.01\n",
    "    #print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "    #print \"embedding_array shape\",embedding_array.shape\n",
    "\n",
    "    print('embed array len: ', len(embedding_array))\n",
    "    print('embed array len: ', len(embedding_array[0]))\n",
    "    return embedding_array, dictionary, unknownWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_samples(user_dict, wc):\n",
    "    neg_dict = defaultdict(list)\n",
    "    #sample = np.random.choice(vocabulary_size, num_sampled, p=unigram_prob, replace=False)\n",
    "   \n",
    "    for user, message in user_dict.items():\n",
    "        user_wrd = set(message)\n",
    "        word_corpus = set(wc.keys())\n",
    "        diff = word_corpus - user_wrd        \n",
    "        neg_dict[user] = random.sample(diff, parameters.sent_idx)\n",
    "    return neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from negative_samples.py of samiroid\n",
    "def multinomial_samples(unigram_distribution, exclude=[], n_samples=1):\n",
    "    samples = []        \n",
    "    while len(samples) != n_samples:            \n",
    "        wrd_idx = np.argmax(np.random.multinomial(1, unigram_distribution))\n",
    "        # from ipdb import set_trace; set_trace()\n",
    "        if wrd_idx not in exclude: \n",
    "            samples.append(wrd_idx)                        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeds():\n",
    "    t0 = time.time()\n",
    "    word_counter = Counter()\n",
    "    n_docs = 0\n",
    "    embedding_size = 128\n",
    "    user_dict = defaultdict(list)\n",
    "    c = 0\n",
    "    with open(train_file,\"r\") as fid:\n",
    "        for line in fid:\n",
    "#             print(\"len\", len(line))\n",
    "            if len(line) > 1:\n",
    "                message = line.split()\n",
    "                user_dict[message[0]].extend(message[1:])\n",
    "                word_counter.update(message[1:])\n",
    "                n_docs += 1\n",
    "    \n",
    "    #keep only words that occur at least min_word_freq times\n",
    "    wc = {w:c for w,c in word_counter.items() if c > min_word_freq} \n",
    "    #finding unigram probability\n",
    "    unigram_cnt = [c for w, c in wc.items()]\n",
    "    total = sum(unigram_cnt)\n",
    "    unigram_prob = [c*1.0/total for c in unigram_cnt]\n",
    "        \n",
    "    tw = sorted(wc.items(), key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    top_words = {w[0]:i for i,w in enumerate(tw[:vocabulary_size])}\n",
    "    print(type(top_words))    \n",
    "\n",
    "    embed_matrix, dictionary, unknownWords = load_embeddings(embed, top_words, embedding_size)\n",
    "    \n",
    "    print('unknown words len: ', unknownWords)\n",
    "    print('top words len before: ', top_words)\n",
    "\n",
    "    for w in unknownWords:\n",
    "        del top_words[w]\n",
    "    \n",
    "    print('top words len after: ', len(top_words))\n",
    "\n",
    "    wrd2idx = {w:i for i,w in enumerate(top_words.keys())}\n",
    "#     get_OOEVs(embed_matrix, wrd2idx)\n",
    "    \n",
    "    #full_E, full_wrd2idx = emb_utils.read_embeddings(args.emb,top_words)        \n",
    "#     embeddings = tf.Variable(embed_matrix, dtype=tf.float32)\n",
    "#     print(embeddings)\n",
    "    #generate the embedding matrix\n",
    "    print(\"embed_matrix shape\",embed_matrix.shape)\n",
    "    emb_size = embed_matrix.shape[0]\n",
    "    E = np.zeros((int(emb_size), len(wrd2idx)))\n",
    "    for wrd,idx in wrd2idx.items(): \n",
    "        E[:, idx] = embed_matrix[:,top_words[wrd]]\n",
    "    print(\"E shape\",E.shape)\n",
    "    pickle.dump([E,unigram_prob,wrd2idx,word_counter,len(user_dict.keys())], open(output_pkl, 'wb'))\n",
    "    \n",
    "    return user_dict, wc, wrd2idx, n_docs, unigram_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob):\n",
    "#     negative_samples = get_neg_samples(user_dict, wc)\n",
    "    #print(negative_samples)\n",
    "#     negative_samples = multinomial_samples(unigram_prob, [], 10)\n",
    "#     print('negative samples: ', negative_samples)\n",
    "\n",
    "    prev_user, prev_user_data, prev_ctxscores, prev_neg_samples  = None, [], [], []\n",
    "    full_train = []\n",
    "#     f_train = open(output,\"wb\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "    with open(train_file, \"r\") as fid:\n",
    "        for j, line in enumerate(fid):\n",
    "            if len(line) <= 1:\n",
    "                continue\n",
    "\n",
    "            message = line.lower().split()\n",
    "            user_id = message[0]\n",
    "            content = message[1:]\n",
    "\n",
    "            negative_samples = multinomial_samples(unigram_prob, [], 10)\n",
    "            #print('negative samples: ', negative_samples)\n",
    "\n",
    "            #convert to indices\n",
    "            msg_idx = [wrd2idx[w] for w in content if w in wrd2idx]\n",
    "#             neg_idx = [wrd2idx[w] for w in negative_samples[user_id] if w in wrd2idx]\n",
    "            \n",
    "            if prev_user == None:  # first user\n",
    "                prev_user = user_id\n",
    "            elif user_id != prev_user or j == n_docs - 1: # this user_id is seen for the first time\n",
    "\n",
    "#                 print('user data len: ', len(prev_user_data), 'prev neg len: ', len(prev_neg_samples))\n",
    "                assert len(prev_user_data) == len(prev_neg_samples)\n",
    "\n",
    "#                 if len(prev_user_data) > 0:\n",
    "                # get numbers in range [0, len(prev_user_data))\n",
    "                shuf_idx = np.arange(len(prev_user_data))\n",
    "                # shuffle numbers\n",
    "                rng.shuffle(shuf_idx)\n",
    "\n",
    "                # fill these lists with the same data in a different order\n",
    "                prev_user_data = [prev_user_data[i] for i in shuf_idx]\n",
    "                prev_neg_samples = [prev_neg_samples[i] for i in shuf_idx]\n",
    "                \n",
    "                #uncomment the if-else if train is len 0\n",
    "                # 90-10 train-test split\n",
    "                split = int(len(prev_user_data)*.9)\n",
    "#                 if len(prev_user_data) > 1:\n",
    "                train = prev_user_data[:split]\n",
    "                test  = prev_user_data[split:]\n",
    "#                 else:\n",
    "#                 train = prev_user_data\n",
    "#                 test = prev_user_data\n",
    "\n",
    "#                 print('train: ', train)\n",
    "                    \n",
    "                neg_samples = prev_neg_samples[:split]\n",
    "                #print(\"neg_samples\",neg_samples)\n",
    "                # each training instance consists of:\n",
    "                # [user_name, train docs, test docs, negative samples]\n",
    "                full_train.append([prev_user, train, test, neg_samples])\n",
    "                prev_user_data = []\n",
    "                prev_neg_samples = []\n",
    "\n",
    "            prev_user = user_id\n",
    "#             if len(msg_idx) > 0:\n",
    "            prev_user_data.append(msg_idx)\n",
    "#             if len(neg_idx) > 0:\n",
    "            prev_neg_samples.append(negative_samples)\n",
    "#             if j == 3:\n",
    "#                 break\n",
    "\n",
    "        print('prev user: ', prev_user)\n",
    "        print('prev user data: ', len(prev_user_data))\n",
    "        print('prev neg samples: ', len(prev_neg_samples))\n",
    "    pickle.dump(full_train, open(output, 'wb'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dictionary length 100000\n",
      "dictionary keys 100000\n",
      "word dict key:  about\n",
      "word dict val:  30\n",
      "dictionary keys obedient\n",
      "dictionary vals 41768\n",
      "word embeds:  100000\n",
      "word embeds:  128\n",
      "embed array len:  78\n",
      "embed array len:  128\n",
      "unknown words len:  [\"it's\", \"you're\", \"don't\", \"doesn't\"]\n",
      "top words len before:  {'about': 30, 'but': 25, 'same': 66, 'more': 43, 'was': 44, 'than': 53, 'game': 75, 'at': 42, 'of': 5, 'how': 67, 'all': 21, 'who': 73, 'be': 16, 'because': 28, 'they': 15, 'The': 38, 'the': 0, 'will': 29, 'people': 47, 'But': 33, 'as': 19, 'now': 46, 'my': 52, 'is': 4, 'do': 45, 'has': 70, 'Yeah': 71, \"it's\": 48, 'for': 10, 'know': 34, 'he': 68, 'some': 55, 'them': 72, 'like': 24, 'on': 11, 'only': 56, 'in': 12, 'are': 17, 'so': 20, 'there': 49, 'You': 57, 'want': 58, 'can': 59, \"you're\": 61, 'up': 39, 'a': 1, 'no': 35, 'what': 31, 'not': 14, 'have': 13, 'his': 60, 'your': 37, 'other': 40, 'just': 23, 'when': 41, 'even': 69, 'it': 9, 'its': 62, 'that': 7, 'this': 18, 'their': 26, 'to': 2, 'get': 63, 'much': 74, 'with': 22, 'those': 50, 'sure': 54, 'would': 51, 'you': 8, \"don't\": 32, 'This': 76, 'if': 27, \"doesn't\": 77, 'and': 3, 'one': 64, 'I': 6, 'think': 65, 'we': 36}\n",
      "top words len after:  74\n",
      "embed_matrix shape (78, 128)\n",
      "E shape (78, 74)\n",
      "prev user:  fragraptor\n",
      "prev user data:  7\n",
      "prev neg samples:  7\n"
     ]
    }
   ],
   "source": [
    "user_dict, wc, wrd2idx, n_docs, unigram_prob = create_embeds()\n",
    "create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
