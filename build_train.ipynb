{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "#from ipdb import set_trace\n",
    "#from negative_samples import negative_sampler\n",
    "import numpy as np\n",
    "import os\n",
    "#from sma_toolkit import embeddings as emb_utils \n",
    "#import streaming_pickle as stPickle\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import parameters\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import tokenize\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "MIN_DOC_LEN=4\n",
    "tokenizer = TweetTokenizer(preserve_case=False)\n",
    "regex = re.compile(r'[\\.\\]\\%\\[\\'\",\\?\\*!\\}\\{<>\\^-]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Negative Samples that are user specific and then using negative samples and the previous user word ids create user specific train data. create_user_train function is similar to the batch process of skigram model of word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = \"word2vec_nce.model\"\n",
    "train_file = \"user_text_sample.txt\"\n",
    "output_pkl = \"train_embeddings.pkl\"\n",
    "vocabulary_size = 100000\n",
    "min_word_freq = 0\n",
    "seed = 42\n",
    "neg_samples = 10\n",
    "output = \"user_train_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(filename, wordDict, embedding_size):\n",
    "    dictionary, steps, word_embeds = pickle.load(open(filename, 'rb'))        \n",
    "    \n",
    "    print(\"dictionary length\",len(dictionary))\n",
    "    print(\"dictionary keys\",len(dictionary.keys()))\n",
    "    \n",
    "    key = list(wordDict.keys())[0]\n",
    "    print('word dict key: ', key)\n",
    "    print('word dict val: ', wordDict[key])\n",
    "    \n",
    "    print(\"dictionary keys\",list(dictionary.keys())[0])\n",
    "    print(\"dictionary vals\",list(dictionary.values())[0])\n",
    "    \n",
    "    print(\"word embeds: \", len(word_embeds))\n",
    "    print(\"word embeds: \", len(word_embeds[0]))\n",
    "    \n",
    "    embedding_array = np.zeros((len(wordDict), embedding_size))\n",
    "    knownWords = list(wordDict.keys())\n",
    "    unknownWords = []\n",
    "    \n",
    "    #print(knownWords)\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        \n",
    "        w = knownWords[i]\n",
    "        if w in dictionary:\n",
    "            index = dictionary[w]\n",
    "        elif w.lower() in dictionary:\n",
    "            index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "        else:\n",
    "            unknownWords.append(w)\n",
    "#             embedding_array[i] = np.random.rand(embedding_size) * 0.02 - 0.01\n",
    "    #print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "    #print \"embedding_array shape\",embedding_array.shape\n",
    "\n",
    "    print('embed array len: ', len(embedding_array))\n",
    "    print('embed array len: ', len(embedding_array[0]))\n",
    "    return embedding_array, dictionary, unknownWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neg_samples(user_dict, wc):\n",
    "    neg_dict = defaultdict(list)\n",
    "    #sample = np.random.choice(vocabulary_size, num_sampled, p=unigram_prob, replace=False)\n",
    "   \n",
    "    for user, message in user_dict.items():\n",
    "        user_wrd = set(message)\n",
    "        word_corpus = set(wc.keys())\n",
    "        diff = word_corpus - user_wrd        \n",
    "        neg_dict[user] = random.sample(diff, parameters.sent_idx)\n",
    "    return neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from negative_samples.py of samiroid\n",
    "def multinomial_samples(unigram_distribution, exclude=[], n_samples=1):\n",
    "    samples = []        \n",
    "    while len(samples) != n_samples:            \n",
    "        wrd_idx = np.argmax(np.random.multinomial(1, unigram_distribution))\n",
    "        # from ipdb import set_trace; set_trace()\n",
    "        if wrd_idx not in exclude: \n",
    "            samples.append(wrd_idx)\n",
    "            \n",
    "    #print('samples: ', samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embeds():\n",
    "    t0 = time.time()\n",
    "    word_counter = Counter()\n",
    "    n_docs = 0\n",
    "    embedding_size = 128\n",
    "    user_dict = defaultdict(list)\n",
    "    c = 0\n",
    "    with open(train_file,\"r\") as fid:\n",
    "        for line in fid:\n",
    "#             print(\"len\", len(line))\n",
    "            if len(line) > 1:\n",
    "                message = line.split()\n",
    "                content = tokenizer.tokenize(\" \".join(message[1:]))\n",
    "                content = [word for word in content if not regex.match(word)]\n",
    "                user_dict[message[0]].extend(content)\n",
    "                word_counter.update(message[1:])\n",
    "                n_docs += 1\n",
    "    \n",
    "    #keep only words that occur at least min_word_freq times\n",
    "    wc = {w:c for w,c in word_counter.items() if c > min_word_freq}        \n",
    "    tw = sorted(wc.items(), key=lambda x:x[1], reverse=True)\n",
    "    top_words = {w[0]:i for i,w in enumerate(tw[:vocabulary_size])}\n",
    "    \n",
    "    embed_matrix, dictionary, unknownWords = load_embeddings(embed, top_words, embedding_size)\n",
    "    \n",
    "#     print('unknown words len: ', unknownWords)\n",
    "#     print('top words len before: ', top_words)\n",
    "\n",
    "    for w in unknownWords:\n",
    "        del top_words[w]\n",
    "    \n",
    "    print('top words len after: ', len(top_words))\n",
    "\n",
    "    wrd2idx = {w:i for i,w in enumerate(top_words.keys())}\n",
    "    \n",
    "    #finding unigram probability\n",
    "    unigram_cnt = [c for w, c in top_words.items()]    \n",
    "    total = sum(unigram_cnt)\n",
    "    unigram_prob = [c*1.0/total for c in unigram_cnt]\n",
    "    \n",
    "    #generate the embedding matrix\n",
    "    print(\"embed_matrix shape\",embed_matrix.shape)\n",
    "    emb_size = embed_matrix.shape[1]\n",
    "    print(emb_size)\n",
    "    E = np.zeros((len(wrd2idx),int(emb_size)))\n",
    "    for wrd,idx in wrd2idx.items(): \n",
    "        E[:] = embed_matrix[top_words[wrd],:]\n",
    "    print(\"E shape\",E.shape)\n",
    "    pickle.dump([E, unigram_prob, wrd2idx, word_counter, len(user_dict.keys())], open(output_pkl, 'wb'))\n",
    "    #print(user_dict)\n",
    "    return user_dict, wc, wrd2idx, n_docs, unigram_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob):\n",
    "#     negative_samples = get_neg_samples(user_dict, wc)\n",
    "    #print(negative_samples)\n",
    "#     negative_samples = multinomial_samples(unigram_prob, [], 10)\n",
    "#     print('negative samples: ', negative_samples)\n",
    "\n",
    "    prev_user, prev_user_data, prev_ctxscores, prev_neg_samples  = None, [], [], []\n",
    "    full_train = []\n",
    "#     f_train = open(output,\"wb\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "    with open(train_file, \"r\") as fid:\n",
    "        for j, line in enumerate(fid):\n",
    "            if len(line) <= 1:\n",
    "                continue\n",
    "\n",
    "            message = line.lower().split()\n",
    "            user_id = message[0]\n",
    "            content = message[1:]\n",
    "\n",
    "            negative_samples = multinomial_samples(unigram_prob, [], 10)\n",
    "            #print('negative samples: ', negative_samples)\n",
    "\n",
    "            #convert to indices\n",
    "            msg_idx = [wrd2idx[w] for w in content if w in wrd2idx]\n",
    "#             neg_idx = [wrd2idx[w] for w in negative_samples[user_id] if w in wrd2idx]\n",
    "            \n",
    "            if prev_user == None:  # first user\n",
    "                prev_user = user_id\n",
    "            elif user_id != prev_user or j == n_docs - 1: # this user_id is seen for the first time\n",
    "\n",
    "#                 print('user data len: ', len(prev_user_data), 'prev neg len: ', len(prev_neg_samples))\n",
    "                assert len(prev_user_data) == len(prev_neg_samples)\n",
    "\n",
    "#                 if len(prev_user_data) > 0:\n",
    "                # get numbers in range [0, len(prev_user_data))\n",
    "                shuf_idx = np.arange(len(prev_user_data))\n",
    "                # shuffle numbers\n",
    "                rng.shuffle(shuf_idx)\n",
    "\n",
    "                # fill these lists with the same data in a different order\n",
    "                prev_user_data = [prev_user_data[i] for i in shuf_idx]\n",
    "                prev_neg_samples = [prev_neg_samples[i] for i in shuf_idx]\n",
    "                \n",
    "                #uncomment the if-else if train is len 0\n",
    "                # 90-10 train-test split\n",
    "                split = int(len(prev_user_data)*.9)\n",
    "#                 if len(prev_user_data) > 1:\n",
    "                train = prev_user_data[:split]\n",
    "                test  = prev_user_data[split:]\n",
    "#                 else:\n",
    "#                 train = prev_user_data\n",
    "#                 test = prev_user_data\n",
    "\n",
    "#                 print('train: ', train)\n",
    "                    \n",
    "                neg_samples = prev_neg_samples[:split]\n",
    "                #print(\"neg_samples\",neg_samples)\n",
    "                # each training instance consists of:\n",
    "                # [user_name, train docs, test docs, negative samples]\n",
    "                full_train.append([prev_user, train, test, neg_samples])\n",
    "                \n",
    "#                 print('formed data: ', [prev_user, train, test, neg_samples])\n",
    "                prev_user_data = []\n",
    "                prev_neg_samples = []\n",
    "\n",
    "            prev_user = user_id\n",
    "#             if len(msg_idx) > 0:\n",
    "            prev_user_data.append(msg_idx)\n",
    "#             if len(neg_idx) > 0:\n",
    "            prev_neg_samples.append(negative_samples)\n",
    "#             if j == 3:\n",
    "#                 break\n",
    "\n",
    "        print('prev user: ', prev_user)\n",
    "        print('prev user data: ', len(prev_user_data))\n",
    "        print('prev neg samples: ', len(prev_neg_samples))        \n",
    "    pickle.dump(full_train, open(output, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary length 100000\n",
      "dictionary keys 100000\n",
      "word dict key:  the\n",
      "word dict val:  0\n",
      "dictionary keys UNK\n",
      "dictionary vals 0\n",
      "word embeds:  100000\n",
      "word embeds:  128\n",
      "embed array len:  100000\n",
      "embed array len:  128\n",
      "top words len after:  46474\n",
      "embed_matrix shape (100000, 128)\n",
      "128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-919e4c4d5418>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muser_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrd2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munigram_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_embeds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcreate_user_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrd2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munigram_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-bb5a394a7ccb>\u001b[0m in \u001b[0;36mcreate_embeds\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrd2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mwrd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwrd2idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtop_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrd\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E shape\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munigram_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrd2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_pkl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_dict, wc, wrd2idx, n_docs, unigram_prob = create_embeds()\n",
    "\n",
    "create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
