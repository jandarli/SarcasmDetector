{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "#from ipdb import set_trace\n",
    "#from negative_samples import negative_sampler\n",
    "import numpy as np\n",
    "import os\n",
    "#from sma_toolkit import embeddings as emb_utils \n",
    "#import streaming_pickle as stPickle\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import parameters\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "MIN_DOC_LEN=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Negative Samples that are user specific and then using negative samples and the previous user word ids create user specific train data. create_user_train function is similar to the batch process of skigram model of word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = \"word2vec_nce.model\"\n",
    "train_file = \"user_text_sample.txt\"\n",
    "output_pkl = \"train_embeddings.pkl\"\n",
    "vocabulary_size = 100000\n",
    "min_word_freq = 0\n",
    "seed = 42\n",
    "neg_samples = 10\n",
    "output = \"user_train_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, wordDict, embedding_size):\n",
    "    dictionary, steps, word_embeds = pickle.load(open(filename, 'rb'))        \n",
    "    \n",
    "    print(\"dictionary length\",len(dictionary))\n",
    "    print(\"dictionary keys\",len(dictionary.keys()))\n",
    "    \n",
    "    key = list(wordDict.keys())[0]\n",
    "    print('word dict key: ', key)\n",
    "    print('word dict val: ', wordDict[key])\n",
    "    \n",
    "    print(\"dictionary keys\",list(dictionary.keys())[0])\n",
    "    print(\"dictionary vals\",list(dictionary.values())[0])\n",
    "    \n",
    "    print(\"word embeds: \", len(word_embeds))\n",
    "    print(\"word embeds: \", len(word_embeds[0]))\n",
    "    \n",
    "    embedding_array = np.zeros((len(wordDict), embedding_size))\n",
    "    knownWords = list(wordDict.keys())\n",
    "    unknownWords = []\n",
    "    \n",
    "    #print(knownWords)\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        \n",
    "        w = knownWords[i]\n",
    "        if w in dictionary:\n",
    "            index = dictionary[w]\n",
    "        elif w.lower() in dictionary:\n",
    "            index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "        else:\n",
    "            unknownWords.append(w)\n",
    "#             embedding_array[i] = np.random.rand(embedding_size) * 0.02 - 0.01\n",
    "    #print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "    #print \"embedding_array shape\",embedding_array.shape\n",
    "\n",
    "    print('embed array len: ', len(embedding_array))\n",
    "    print('embed array len: ', len(embedding_array[0]))\n",
    "    return embedding_array, dictionary, unknownWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_samples(user_dict, wc):\n",
    "    neg_dict = defaultdict(list)\n",
    "    #sample = np.random.choice(vocabulary_size, num_sampled, p=unigram_prob, replace=False)\n",
    "   \n",
    "    for user, message in user_dict.items():\n",
    "        user_wrd = set(message)\n",
    "        word_corpus = set(wc.keys())\n",
    "        diff = word_corpus - user_wrd        \n",
    "        neg_dict[user] = random.sample(diff, parameters.sent_idx)\n",
    "    return neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from negative_samples.py of samiroid\n",
    "def multinomial_samples(unigram_distribution, exclude=[], n_samples=1):\n",
    "    samples = []        \n",
    "    while len(samples) != n_samples:            \n",
    "        wrd_idx = np.argmax(np.random.multinomial(1, unigram_distribution))\n",
    "        # from ipdb import set_trace; set_trace()\n",
    "        if wrd_idx not in exclude: \n",
    "            samples.append(wrd_idx)\n",
    "            \n",
    "    print('samples: ', samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeds():\n",
    "    t0 = time.time()\n",
    "    word_counter = Counter()\n",
    "    n_docs = 0\n",
    "    embedding_size = 128\n",
    "    user_dict = defaultdict(list)\n",
    "    c = 0\n",
    "    with open(train_file,\"r\") as fid:\n",
    "        for line in fid:\n",
    "#             print(\"len\", len(line))\n",
    "            if len(line) > 1:\n",
    "                message = line.split()\n",
    "                user_dict[message[0]].extend(message[1:])\n",
    "                word_counter.update(message[1:])\n",
    "                n_docs += 1\n",
    "    \n",
    "    #keep only words that occur at least min_word_freq times\n",
    "    wc = {w:c for w,c in word_counter.items() if c > min_word_freq}        \n",
    "    tw = sorted(wc.items(), key=lambda x:x[1], reverse=True)\n",
    "    top_words = {w[0]:i for i,w in enumerate(tw[:vocabulary_size])}\n",
    "    \n",
    "    embed_matrix, dictionary, unknownWords = load_embeddings(embed, top_words, embedding_size)\n",
    "    \n",
    "#     print('unknown words len: ', unknownWords)\n",
    "#     print('top words len before: ', top_words)\n",
    "\n",
    "    for w in unknownWords:\n",
    "        del top_words[w]\n",
    "    \n",
    "    print('top words len after: ', len(top_words))\n",
    "\n",
    "    wrd2idx = {w:i for i,w in enumerate(top_words.keys())}\n",
    "    \n",
    "    #finding unigram probability\n",
    "    unigram_cnt = [c for w, c in top_words.items()]    \n",
    "    total = sum(unigram_cnt)\n",
    "    unigram_prob = [c*1.0/total for c in unigram_cnt]\n",
    "    \n",
    "    #generate the embedding matrix\n",
    "    print(\"embed_matrix shape\",embed_matrix.shape)\n",
    "    emb_size = embed_matrix.shape[1]\n",
    "    print(emb_size)\n",
    "    E = np.zeros((len(wrd2idx),int(emb_size)))\n",
    "    for wrd,idx in wrd2idx.items(): \n",
    "        E[:] = embed_matrix[top_words[wrd],:]\n",
    "    print(\"E shape\",E.shape)\n",
    "    pickle.dump([E, unigram_prob, wrd2idx, word_counter, len(user_dict.keys())], open(output_pkl, 'wb'))\n",
    "    \n",
    "    return user_dict, wc, wrd2idx, n_docs, unigram_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob):\n",
    "#     negative_samples = get_neg_samples(user_dict, wc)\n",
    "    #print(negative_samples)\n",
    "#     negative_samples = multinomial_samples(unigram_prob, [], 10)\n",
    "#     print('negative samples: ', negative_samples)\n",
    "\n",
    "    prev_user, prev_user_data, prev_ctxscores, prev_neg_samples  = None, [], [], []\n",
    "    full_train = []\n",
    "#     f_train = open(output,\"wb\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "    with open(train_file, \"r\") as fid:\n",
    "        for j, line in enumerate(fid):\n",
    "            if len(line) <= 1:\n",
    "                continue\n",
    "\n",
    "            message = line.lower().split()\n",
    "            user_id = message[0]\n",
    "            content = message[1:]\n",
    "\n",
    "            negative_samples = multinomial_samples(unigram_prob, [], 10)\n",
    "            #print('negative samples: ', negative_samples)\n",
    "\n",
    "            #convert to indices\n",
    "            msg_idx = [wrd2idx[w] for w in content if w in wrd2idx]\n",
    "#             neg_idx = [wrd2idx[w] for w in negative_samples[user_id] if w in wrd2idx]\n",
    "            \n",
    "            if prev_user == None:  # first user\n",
    "                prev_user = user_id\n",
    "            elif user_id != prev_user or j == n_docs - 1: # this user_id is seen for the first time\n",
    "\n",
    "#                 print('user data len: ', len(prev_user_data), 'prev neg len: ', len(prev_neg_samples))\n",
    "                assert len(prev_user_data) == len(prev_neg_samples)\n",
    "\n",
    "#                 if len(prev_user_data) > 0:\n",
    "                # get numbers in range [0, len(prev_user_data))\n",
    "                shuf_idx = np.arange(len(prev_user_data))\n",
    "                # shuffle numbers\n",
    "                rng.shuffle(shuf_idx)\n",
    "\n",
    "                # fill these lists with the same data in a different order\n",
    "                prev_user_data = [prev_user_data[i] for i in shuf_idx]\n",
    "                prev_neg_samples = [prev_neg_samples[i] for i in shuf_idx]\n",
    "                \n",
    "                #uncomment the if-else if train is len 0\n",
    "                # 90-10 train-test split\n",
    "                split = int(len(prev_user_data)*.9)\n",
    "#                 if len(prev_user_data) > 1:\n",
    "                train = prev_user_data[:split]\n",
    "                test  = prev_user_data[split:]\n",
    "#                 else:\n",
    "#                 train = prev_user_data\n",
    "#                 test = prev_user_data\n",
    "\n",
    "#                 print('train: ', train)\n",
    "                    \n",
    "                neg_samples = prev_neg_samples[:split]\n",
    "                #print(\"neg_samples\",neg_samples)\n",
    "                # each training instance consists of:\n",
    "                # [user_name, train docs, test docs, negative samples]\n",
    "                full_train.append([prev_user, train, test, neg_samples])\n",
    "                \n",
    "#                 print('formed data: ', [prev_user, train, test, neg_samples])\n",
    "                prev_user_data = []\n",
    "                prev_neg_samples = []\n",
    "\n",
    "            prev_user = user_id\n",
    "#             if len(msg_idx) > 0:\n",
    "            prev_user_data.append(msg_idx)\n",
    "#             if len(neg_idx) > 0:\n",
    "            prev_neg_samples.append(negative_samples)\n",
    "#             if j == 3:\n",
    "#                 break\n",
    "\n",
    "        print('prev user: ', prev_user)\n",
    "        print('prev user data: ', len(prev_user_data))\n",
    "        print('prev neg samples: ', len(prev_neg_samples))        \n",
    "    pickle.dump(full_train, open(output, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary length 100000\n",
      "dictionary keys 100000\n",
      "word dict key:  apply\n",
      "word dict val:  306\n",
      "dictionary keys waiter\n",
      "dictionary vals 22753\n",
      "word embeds:  100000\n",
      "word embeds:  128\n",
      "embed array len:  1526\n",
      "embed array len:  128\n",
      "top words len after:  1086\n",
      "embed_matrix shape (1526, 128)\n",
      "128\n",
      "E shape (1086, 128)\n",
      "samples:  [172, 978, 43, 892, 766, 724, 696, 320, 944, 755]\n",
      "samples:  [779, 1011, 800, 813, 816, 1017, 89, 745, 856, 624]\n",
      "samples:  [1039, 1043, 291, 648, 53, 554, 840, 983, 711, 811]\n",
      "samples:  [817, 291, 605, 28, 316, 194, 450, 961, 1008, 309]\n",
      "samples:  [524, 216, 927, 562, 937, 545, 541, 847, 475, 516]\n",
      "samples:  [221, 480, 588, 723, 884, 1078, 844, 810, 908, 250]\n",
      "samples:  [1066, 610, 907, 334, 890, 219, 312, 777, 417, 574]\n",
      "samples:  [204, 156, 469, 716, 1017, 202, 989, 341, 948, 142]\n",
      "samples:  [707, 140, 997, 867, 370, 968, 749, 890, 71, 768]\n",
      "samples:  [173, 1045, 409, 964, 757, 651, 647, 433, 927, 856]\n",
      "samples:  [569, 384, 326, 814, 991, 883, 781, 753, 764, 549]\n",
      "samples:  [1047, 158, 720, 325, 412, 333, 501, 396, 265, 921]\n",
      "samples:  [1067, 755, 895, 220, 835, 987, 825, 736, 325, 343]\n",
      "samples:  [301, 580, 335, 824, 100, 572, 383, 930, 909, 511]\n",
      "samples:  [531, 709, 1022, 832, 1017, 145, 721, 916, 767, 382]\n",
      "samples:  [690, 428, 758, 756, 934, 308, 838, 993, 724, 705]\n",
      "samples:  [182, 916, 447, 25, 720, 689, 887, 511, 441, 1065]\n",
      "samples:  [923, 551, 845, 713, 510, 342, 325, 715, 493, 557]\n",
      "samples:  [368, 653, 124, 313, 999, 297, 1057, 913, 5, 905]\n",
      "samples:  [293, 700, 1055, 723, 1038, 357, 691, 718, 931, 943]\n",
      "samples:  [1082, 996, 1048, 697, 808, 971, 500, 1055, 557, 1003]\n",
      "samples:  [1007, 1030, 663, 783, 139, 441, 623, 740, 408, 575]\n",
      "samples:  [785, 924, 340, 408, 440, 473, 237, 1012, 1075, 1024]\n",
      "samples:  [816, 192, 498, 666, 368, 639, 771, 220, 918, 218]\n",
      "samples:  [495, 568, 1077, 896, 512, 198, 337, 456, 987, 604]\n",
      "samples:  [813, 423, 1021, 994, 458, 125, 879, 511, 1046, 758]\n",
      "samples:  [420, 937, 406, 225, 16, 626, 861, 129, 860, 969]\n",
      "samples:  [907, 990, 746, 991, 925, 639, 440, 779, 909, 653]\n",
      "samples:  [335, 108, 589, 973, 977, 522, 553, 584, 300, 568]\n",
      "samples:  [849, 716, 938, 928, 690, 760, 641, 879, 539, 434]\n",
      "samples:  [938, 558, 263, 511, 707, 813, 667, 799, 313, 624]\n",
      "samples:  [841, 170, 466, 296, 1002, 445, 515, 637, 325, 1046]\n",
      "samples:  [717, 682, 100, 945, 494, 321, 983, 1047, 1024, 871]\n",
      "samples:  [692, 890, 888, 415, 914, 1019, 95, 1069, 857, 649]\n",
      "samples:  [1047, 720, 1069, 1055, 227, 669, 569, 888, 1044, 588]\n",
      "samples:  [789, 970, 867, 409, 645, 296, 1070, 953, 818, 121]\n",
      "samples:  [813, 946, 170, 706, 188, 6, 977, 696, 604, 572]\n",
      "samples:  [1019, 615, 949, 105, 307, 254, 893, 1063, 278, 423]\n",
      "samples:  [466, 780, 303, 860, 812, 87, 446, 918, 867, 215]\n",
      "samples:  [790, 953, 586, 476, 1039, 121, 1032, 440, 300, 462]\n",
      "samples:  [624, 926, 777, 168, 310, 489, 974, 907, 733, 1041]\n",
      "samples:  [944, 543, 275, 399, 284, 13, 343, 187, 1022, 878]\n",
      "samples:  [895, 627, 144, 950, 488, 940, 949, 702, 985, 120]\n",
      "samples:  [353, 366, 660, 809, 819, 474, 1045, 823, 861, 532]\n",
      "samples:  [436, 918, 691, 256, 1016, 479, 650, 717, 302, 228]\n",
      "samples:  [489, 496, 135, 407, 473, 1041, 413, 469, 962, 418]\n",
      "samples:  [983, 1023, 83, 249, 305, 378, 480, 923, 260, 641]\n",
      "samples:  [912, 843, 808, 448, 857, 91, 312, 934, 1043, 674]\n",
      "samples:  [432, 364, 636, 494, 544, 809, 588, 145, 432, 541]\n",
      "samples:  [172, 357, 473, 1033, 821, 269, 319, 464, 437, 1031]\n",
      "samples:  [706, 289, 109, 483, 777, 49, 878, 616, 929, 437]\n",
      "samples:  [918, 813, 962, 122, 843, 812, 98, 930, 251, 562]\n",
      "samples:  [733, 902, 264, 1023, 10, 911, 690, 936, 143, 931]\n",
      "samples:  [716, 978, 433, 1033, 1071, 873, 950, 951, 492, 549]\n",
      "samples:  [729, 27, 289, 558, 1027, 873, 393, 1018, 610, 440]\n",
      "samples:  [1058, 907, 797, 319, 66, 651, 340, 1016, 411, 1032]\n",
      "samples:  [455, 1071, 572, 252, 669, 447, 207, 871, 847, 684]\n",
      "samples:  [61, 769, 977, 978, 381, 802, 43, 187, 113, 959]\n",
      "samples:  [961, 502, 248, 352, 359, 288, 841, 1063, 505, 885]\n",
      "samples:  [783, 768, 862, 902, 876, 396, 666, 912, 430, 1020]\n",
      "samples:  [311, 703, 856, 970, 443, 724, 888, 83, 1020, 248]\n",
      "samples:  [220, 900, 409, 176, 980, 840, 1073, 542, 883, 979]\n",
      "samples:  [336, 802, 1032, 733, 921, 255, 340, 898, 799, 953]\n",
      "samples:  [333, 574, 580, 682, 627, 900, 979, 878, 71, 305]\n",
      "samples:  [1069, 132, 931, 584, 735, 24, 379, 870, 1032, 620]\n",
      "samples:  [260, 1012, 903, 13, 540, 471, 595, 849, 296, 243]\n",
      "samples:  [109, 441, 895, 753, 602, 1045, 375, 930, 863, 917]\n",
      "samples:  [935, 917, 767, 639, 762, 421, 962, 605, 938, 915]\n",
      "samples:  [1058, 988, 733, 480, 1076, 170, 127, 100, 1015, 174]\n",
      "samples:  [885, 429, 477, 443, 1045, 2, 928, 902, 897, 703]\n",
      "samples:  [915, 483, 916, 1084, 303, 338, 476, 267, 984, 340]\n",
      "samples:  [893, 1061, 777, 972, 398, 512, 883, 823, 1062, 256]\n",
      "samples:  [989, 464, 752, 85, 755, 598, 1065, 348, 451, 685]\n",
      "samples:  [838, 871, 522, 962, 755, 605, 949, 795, 1051, 452]\n",
      "samples:  [588, 690, 876, 975, 955, 459, 699, 39, 536, 874]\n",
      "samples:  [859, 879, 1074, 905, 614, 312, 54, 1061, 313, 1028]\n",
      "samples:  [220, 556, 1026, 271, 980, 675, 1012, 268, 482, 723]\n",
      "samples:  [497, 748, 724, 1074, 1033, 1077, 333, 905, 951, 724]\n",
      "samples:  [87, 407, 847, 749, 689, 1017, 1012, 781, 328, 513]\n",
      "samples:  [1082, 735, 317, 595, 506, 888, 345, 505, 930, 1053]\n",
      "samples:  [1013, 1063, 540, 818, 591, 708, 69, 508, 647, 299]\n",
      "samples:  [584, 538, 1039, 493, 703, 552, 709, 387, 737, 482]\n",
      "samples:  [583, 173, 109, 981, 911, 624, 651, 1042, 68, 904]\n",
      "samples:  [305, 1064, 611, 988, 500, 861, 1044, 1032, 539, 951]\n",
      "samples:  [911, 61, 1033, 552, 320, 884, 394, 575, 588, 306]\n",
      "samples:  [950, 782, 381, 913, 776, 516, 600, 625, 96, 445]\n",
      "samples:  [754, 1014, 925, 539, 984, 699, 285, 786, 266, 344]\n",
      "samples:  [837, 13, 530, 863, 1029, 779, 1003, 477, 859, 533]\n",
      "samples:  [735, 670, 732, 895, 163, 557, 571, 700, 658, 788]\n",
      "samples:  [880, 577, 553, 243, 817, 26, 113, 490, 903, 80]\n",
      "samples:  [530, 945, 605, 943, 737, 619, 487, 1025, 694, 940]\n",
      "samples:  [221, 309, 569, 938, 708, 707, 861, 688, 630, 314]\n",
      "samples:  [833, 181, 700, 54, 1019, 1002, 853, 108, 943, 315]\n",
      "samples:  [60, 783, 558, 970, 723, 902, 679, 529, 1062, 166]\n",
      "samples:  [989, 814, 1069, 948, 984, 969, 1071, 370, 504, 771]\n",
      "samples:  [489, 997, 974, 975, 473, 801, 811, 71, 885, 559]\n",
      "samples:  [219, 1073, 733, 396, 911, 591, 820, 26, 701, 1067]\n",
      "samples:  [174, 1067, 809, 1076, 82, 690, 810, 702, 595, 923]\n",
      "samples:  [438, 572, 861, 1070, 682, 1008, 702, 777, 667, 613]\n",
      "samples:  [912, 847, 678, 937, 455, 507, 1058, 572, 239, 274]\n",
      "samples:  [927, 281, 509, 1004, 931, 980, 247, 963, 988, 1029]\n",
      "samples:  [950, 368, 850, 1068, 828, 590, 840, 1042, 517, 1047]\n",
      "samples:  [752, 1083, 975, 830, 673, 933, 733, 313, 484, 483]\n",
      "samples:  [863, 918, 334, 720, 788, 790, 874, 521, 917, 440]\n",
      "samples:  [1029, 670, 107, 594, 1001, 656, 701, 138, 773, 940]\n",
      "samples:  [790, 846, 974, 256, 21, 876, 761, 670, 360, 547]\n",
      "samples:  [235, 420, 261, 796, 790, 870, 942, 272, 853, 611]\n",
      "samples:  [670, 955, 692, 302, 843, 771, 888, 814, 798, 934]\n",
      "samples:  [507, 1035, 897, 491, 856, 366, 431, 859, 130, 827]\n",
      "samples:  [907, 1070, 810, 492, 908, 812, 718, 561, 399, 970]\n",
      "samples:  [781, 817, 561, 890, 967, 448, 979, 768, 410, 858]\n",
      "samples:  [633, 583, 502, 648, 413, 485, 1055, 342, 405, 943]\n",
      "samples:  [187, 867, 343, 736, 979, 625, 61, 717, 719, 276]\n",
      "samples:  [700, 926, 1030, 963, 673, 623, 926, 502, 77, 217]\n",
      "samples:  [588, 149, 736, 710, 971, 973, 1064, 768, 1025, 557]\n",
      "samples:  [592, 812, 422, 511, 64, 1014, 730, 699, 296, 674]\n",
      "samples:  [428, 257, 1039, 823, 493, 390, 436, 1020, 765, 542]\n",
      "samples:  [717, 16, 19, 492, 574, 1019, 784, 868, 21, 824]\n",
      "samples:  [616, 953, 139, 1027, 499, 769, 256, 1079, 1028, 800]\n",
      "samples:  [748, 760, 903, 780, 921, 494, 306, 808, 930, 602]\n",
      "samples:  [870, 902, 440, 785, 1025, 476, 919, 599, 973, 898]\n",
      "samples:  [96, 1012, 852, 911, 262, 969, 631, 268, 358, 1019]\n",
      "samples:  [1048, 988, 323, 1073, 44, 920, 639, 893, 903, 68]\n",
      "samples:  [485, 1063, 1055, 45, 203, 375, 695, 909, 717, 922]\n",
      "samples:  [381, 357, 973, 653, 873, 267, 946, 500, 612, 226]\n",
      "samples:  [579, 718, 811, 196, 857, 188, 415, 281, 565, 239]\n",
      "samples:  [1058, 903, 947, 634, 85, 925, 299, 931, 1080, 462]\n",
      "samples:  [1065, 67, 44, 715, 531, 963, 842, 353, 387, 949]\n",
      "samples:  [172, 984, 45, 878, 870, 981, 684, 696, 760, 947]\n",
      "samples:  [335, 1062, 103, 36, 894, 1058, 682, 360, 496, 831]\n",
      "samples:  [438, 1061, 691, 916, 901, 127, 275, 233, 745, 934]\n",
      "samples:  [813, 665, 103, 1079, 1030, 729, 149, 1059, 230, 543]\n",
      "samples:  [542, 464, 333, 631, 144, 41, 701, 383, 643, 925]\n",
      "samples:  [439, 969, 626, 251, 1050, 947, 142, 1013, 505, 310]\n",
      "samples:  [211, 834, 1071, 785, 921, 162, 858, 911, 923, 493]\n",
      "samples:  [279, 599, 654, 680, 1004, 718, 522, 769, 1045, 986]\n",
      "samples:  [878, 796, 974, 666, 1027, 1068, 111, 859, 1002, 167]\n",
      "samples:  [812, 465, 723, 1039, 114, 275, 911, 641, 1063, 820]\n",
      "samples:  [900, 551, 930, 716, 84, 740, 1063, 786, 265, 926]\n",
      "samples:  [772, 978, 579, 1004, 761, 764, 1035, 595, 722, 1077]\n",
      "samples:  [318, 575, 748, 1063, 1067, 987, 988, 859, 683, 942]\n",
      "samples:  [650, 549, 120, 554, 926, 730, 923, 800, 841, 901]\n",
      "samples:  [1039, 930, 977, 1073, 206, 621, 647, 294, 984, 1055]\n",
      "samples:  [429, 982, 256, 687, 369, 561, 431, 802, 1042, 1005]\n",
      "samples:  [179, 970, 47, 882, 711, 920, 1061, 730, 384, 427]\n",
      "samples:  [492, 465, 151, 369, 532, 575, 229, 907, 848, 767]\n",
      "samples:  [939, 335, 588, 492, 710, 1012, 856, 691, 1071, 1001]\n",
      "samples:  [114, 166, 821, 907, 818, 917, 132, 635, 37, 540]\n",
      "samples:  [187, 1023, 805, 293, 444, 970, 125, 844, 862, 742]\n",
      "samples:  [1035, 792, 1067, 486, 427, 292, 888, 895, 551, 168]\n",
      "samples:  [326, 228, 738, 436, 701, 516, 963, 278, 1030, 635]\n",
      "samples:  [926, 383, 344, 8, 243, 398, 837, 910, 767, 932]\n",
      "samples:  [435, 697, 167, 292, 686, 1011, 256, 1008, 1068, 904]\n",
      "samples:  [1071, 583, 950, 13, 987, 314, 873, 847, 558, 859]\n",
      "samples:  [913, 513, 849, 1066, 351, 1062, 379, 1006, 1007, 802]\n",
      "samples:  [592, 750, 781, 971, 971, 693, 101, 653, 912, 241]\n",
      "samples:  [558, 339, 591, 82, 1021, 985, 1038, 1027, 887, 584]\n",
      "samples:  [294, 709, 139, 1066, 641, 440, 776, 513, 962, 930]\n",
      "samples:  [767, 509, 128, 1080, 704, 316, 885, 441, 902, 71]\n",
      "samples:  [1024, 567, 805, 906, 687, 559, 939, 905, 695, 297]\n",
      "samples:  [526, 292, 816, 271, 846, 704, 692, 612, 653, 630]\n",
      "samples:  [835, 130, 921, 1007, 1084, 13, 870, 248, 361, 131]\n",
      "samples:  [692, 893, 893, 490, 729, 632, 970, 939, 435, 448]\n",
      "samples:  [497, 68, 577, 436, 553, 1069, 1063, 384, 1002, 95]\n",
      "samples:  [39, 710, 243, 871, 963, 110, 887, 872, 737, 545]\n",
      "samples:  [543, 157, 616, 546, 132, 746, 725, 300, 643, 254]\n",
      "samples:  [838, 614, 299, 682, 813, 184, 648, 764, 512, 60]\n",
      "samples:  [947, 729, 551, 517, 771, 1007, 257, 718, 307, 648]\n",
      "samples:  [716, 1015, 719, 51, 233, 868, 860, 919, 707, 694]\n",
      "samples:  [530, 125, 1039, 860, 338, 950, 696, 565, 517, 390]\n",
      "samples:  [225, 746, 994, 709, 944, 670, 539, 391, 629, 951]\n",
      "samples:  [22, 859, 758, 612, 868, 889, 985, 732, 906, 862]\n",
      "samples:  [1027, 758, 101, 477, 492, 978, 692, 989, 653, 326]\n",
      "samples:  [843, 205, 484, 553, 899, 767, 1043, 669, 399, 898]\n",
      "samples:  [540, 878, 724, 687, 559, 1020, 278, 121, 626, 314]\n",
      "samples:  [406, 648, 779, 1004, 781, 268, 772, 420, 680, 375]\n",
      "samples:  [1016, 959, 766, 408, 801, 683, 586, 442, 797, 931]\n",
      "samples:  [393, 1033, 749, 1071, 254, 975, 2, 868, 290, 848]\n",
      "samples:  [591, 1033, 454, 522, 862, 440, 753, 909, 325, 870]\n",
      "samples:  [1067, 811, 873, 1012, 139, 73, 1065, 129, 1012, 593]\n",
      "samples:  [820, 781, 95, 268, 703, 798, 834, 454, 483, 690]\n",
      "samples:  [1013, 196, 712, 289, 441, 485, 916, 187, 977, 200]\n",
      "samples:  [731, 371, 828, 870, 879, 994, 532, 517, 878, 441]\n",
      "samples:  [980, 725, 912, 810, 480, 412, 108, 613, 595, 17]\n",
      "samples:  [311, 438, 790, 90, 502, 355, 774, 378, 997, 905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples:  [1031, 146, 525, 1, 934, 477, 471, 411, 265, 129]\n",
      "samples:  [225, 1006, 1018, 237, 722, 352, 571, 1021, 718, 652]\n",
      "samples:  [320, 488, 814, 928, 1047, 1055, 139, 1023, 613, 727]\n",
      "samples:  [423, 693, 798, 633, 231, 684, 312, 326, 990, 65]\n",
      "samples:  [1041, 897, 988, 409, 1006, 229, 538, 17, 298, 885]\n",
      "samples:  [191, 1012, 323, 551, 690, 477, 868, 937, 735, 927]\n",
      "samples:  [1061, 763, 474, 517, 324, 8, 1065, 1017, 322, 820]\n",
      "samples:  [378, 1062, 1004, 201, 848, 225, 513, 804, 360, 455]\n",
      "samples:  [653, 1011, 848, 799, 663, 455, 834, 458, 862, 685]\n",
      "samples:  [843, 390, 476, 780, 701, 1026, 855, 1055, 700, 673]\n",
      "samples:  [610, 409, 1006, 777, 267, 867, 611, 884, 471, 259]\n",
      "samples:  [738, 1041, 432, 110, 315, 813, 473, 638, 458, 1071]\n",
      "samples:  [543, 737, 647, 853, 549, 616, 923, 396, 1076, 17]\n",
      "samples:  [307, 239, 87, 524, 772, 910, 301, 320, 692, 851]\n",
      "samples:  [500, 247, 285, 592, 767, 803, 738, 406, 252, 787]\n",
      "samples:  [51, 178, 698, 784, 928, 745, 953, 745, 64, 212]\n",
      "samples:  [397, 907, 824, 694, 348, 446, 767, 471, 979, 532]\n",
      "samples:  [802, 702, 824, 1019, 895, 204, 291, 543, 848, 783]\n",
      "samples:  [155, 246, 739, 997, 512, 497, 673, 749, 785, 313]\n",
      "samples:  [348, 456, 622, 855, 535, 780, 765, 629, 653, 584]\n",
      "samples:  [180, 898, 335, 108, 846, 505, 303, 948, 890, 469]\n",
      "samples:  [164, 1063, 684, 114, 502, 37, 864, 942, 887, 940]\n",
      "samples:  [1018, 360, 863, 187, 146, 180, 978, 228, 848, 997]\n",
      "samples:  [612, 483, 544, 757, 894, 536, 705, 547, 38, 985]\n",
      "samples:  [553, 839, 766, 871, 908, 451, 312, 829, 691, 750]\n",
      "samples:  [948, 182, 730, 978, 549, 220, 505, 264, 710, 907]\n",
      "samples:  [673, 500, 963, 544, 196, 234, 925, 66, 308, 1084]\n",
      "samples:  [724, 848, 1044, 415, 439, 530, 885, 968, 487, 903]\n",
      "samples:  [820, 849, 1028, 213, 862, 897, 736, 257, 1002, 247]\n",
      "samples:  [797, 397, 911, 989, 1039, 1062, 1077, 1079, 912, 947]\n",
      "samples:  [750, 475, 987, 640, 593, 692, 827, 45, 341, 1007]\n",
      "samples:  [756, 748, 621, 312, 987, 747, 1010, 21, 1066, 915]\n",
      "samples:  [1063, 579, 423, 494, 895, 372, 446, 418, 970, 108]\n",
      "samples:  [929, 289, 856, 1070, 678, 162, 961, 762, 856, 613]\n",
      "samples:  [515, 370, 792, 586, 401, 837, 243, 67, 488, 15]\n",
      "samples:  [225, 1047, 849, 808, 611, 561, 464, 561, 174, 1084]\n",
      "samples:  [143, 779, 649, 849, 175, 914, 807, 888, 541, 264]\n",
      "samples:  [579, 943, 1027, 1072, 1006, 633, 138, 593, 539, 433]\n",
      "samples:  [643, 85, 297, 1010, 955, 524, 880, 296, 394, 61]\n",
      "samples:  [812, 551, 909, 698, 1077, 665, 94, 13, 445, 1064]\n",
      "samples:  [337, 49, 1072, 751, 516, 469, 337, 80, 768, 1026]\n",
      "samples:  [979, 243, 817, 510, 831, 553, 981, 911, 13, 517]\n",
      "samples:  [771, 820, 930, 980, 990, 843, 1004, 989, 1080, 1052]\n",
      "samples:  [19, 773, 721, 415, 915, 427, 893, 554, 226, 653]\n",
      "samples:  [972, 1084, 655, 638, 1064, 743, 715, 901, 969, 1069]\n",
      "samples:  [617, 202, 368, 616, 818, 665, 420, 213, 962, 780]\n",
      "samples:  [555, 1073, 1040, 779, 685, 934, 743, 330, 260, 838]\n",
      "samples:  [919, 897, 1079, 256, 1079, 1032, 814, 509, 542, 158]\n",
      "samples:  [350, 1080, 908, 577, 349, 980, 951, 379, 696, 955]\n",
      "samples:  [906, 236, 949, 588, 701, 834, 831, 894, 792, 549]\n",
      "samples:  [448, 942, 693, 487, 69, 325, 131, 860, 918, 872]\n",
      "samples:  [853, 275, 638, 511, 328, 766, 819, 300, 707, 490]\n",
      "samples:  [653, 853, 887, 853, 737, 940, 834, 917, 412, 1016]\n",
      "samples:  [480, 725, 1062, 818, 373, 13, 673, 874, 561, 464]\n",
      "samples:  [138, 1070, 981, 711, 705, 781, 972, 471, 874, 897]\n",
      "samples:  [972, 384, 540, 93, 241, 858, 614, 544, 188, 719]\n",
      "samples:  [848, 596, 456, 442, 1022, 438, 458, 813, 394, 308]\n",
      "samples:  [962, 108, 988, 45, 1077, 116, 197, 938, 1065, 923]\n",
      "samples:  [779, 1035, 990, 255, 731, 950, 844, 660, 187, 660]\n",
      "samples:  [630, 82, 625, 899, 575, 498, 678, 142, 484, 949]\n",
      "samples:  [183, 729, 945, 769, 812, 455, 125, 1010, 64, 893]\n",
      "samples:  [251, 1006, 359, 474, 1071, 800, 223, 766, 265, 219]\n",
      "samples:  [960, 851, 543, 624, 850, 1048, 788, 1010, 807, 745]\n",
      "samples:  [484, 313, 95, 574, 472, 511, 897, 577, 436, 484]\n",
      "samples:  [308, 641, 436, 506, 1024, 784, 409, 987, 627, 268]\n",
      "samples:  [1048, 887, 294, 403, 1028, 943, 873, 660, 447, 68]\n",
      "samples:  [702, 821, 708, 819, 1035, 49, 431, 325, 561, 369]\n",
      "samples:  [635, 590, 471, 545, 1033, 321, 767, 698, 904, 1042]\n",
      "samples:  [485, 685, 954, 421, 924, 1030, 647, 85, 769, 851]\n",
      "samples:  [597, 1031, 790, 361, 340, 243, 722, 281, 506, 219]\n",
      "samples:  [47, 906, 905, 1068, 868, 118, 754, 473, 545, 780]\n",
      "samples:  [326, 717, 326, 751, 785, 873, 268, 665, 1027, 512]\n",
      "samples:  [525, 510, 808, 1042, 522, 561, 849, 1014, 807, 620]\n",
      "samples:  [760, 1053, 494, 1084, 107, 903, 1026, 579, 484, 546]\n",
      "samples:  [721, 709, 323, 751, 592, 620, 843, 539, 105, 704]\n",
      "samples:  [584, 953, 697, 867, 665, 888, 979, 410, 434, 989]\n",
      "samples:  [911, 285, 988, 431, 620, 1069, 738, 764, 630, 859]\n",
      "samples:  [306, 13, 612, 823, 960, 1042, 684, 1004, 411, 1012]\n",
      "samples:  [946, 636, 1085, 1025, 987, 927, 447, 261, 927, 283]\n",
      "samples:  [575, 229, 414, 719, 1014, 996, 833, 569, 263, 1024]\n",
      "samples:  [592, 375, 369, 1006, 725, 643, 542, 268, 656, 21]\n",
      "samples:  [1042, 483, 217, 755, 309, 778, 715, 818, 343, 256]\n",
      "samples:  [1011, 446, 923, 897, 575, 690, 516, 89, 397, 879]\n",
      "samples:  [727, 1008, 774, 998, 341, 583, 1079, 337, 589, 493]\n",
      "samples:  [693, 1033, 82, 267, 512, 1071, 927, 256, 47, 81]\n",
      "samples:  [1039, 758, 887, 175, 717, 528, 176, 937, 648, 801]\n",
      "prev user:  fragraptor\n",
      "prev user data:  7\n",
      "prev neg samples:  7\n"
     ]
    }
   ],
   "source": [
    "user_dict, wc, wrd2idx, n_docs, unigram_prob = create_embeds()\n",
    "create_user_train(user_dict, wc, wrd2idx, n_docs, unigram_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
