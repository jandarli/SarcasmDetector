{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import Counter\n",
    "#from ipdb import set_trace\n",
    "#from negative_samples import negative_sampler\n",
    "import numpy as np\n",
    "import os\n",
    "#from sma_toolkit import embeddings as emb_utils \n",
    "#import streaming_pickle as stPickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "MIN_DOC_LEN=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename,wordDict,embedding_size):\n",
    "    dictionary, steps, word_embeds = pickle.load(open(filename, 'rb'))\n",
    "    embedding_array = np.zeros((len(wordDict), embedding_size))\n",
    "    knownWords = list(wordDict.keys())\n",
    "    #print(knownWords)\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        \n",
    "        w = knownWords[i]\n",
    "        if w in dictionary:\n",
    "            index = dictionary[w]\n",
    "        elif w.lower() in dictionary:\n",
    "            index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "        else:\n",
    "            embedding_array[i] = np.random.rand(embedding_size) * 0.02 - 0.01\n",
    "    #print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "    #print \"embedding_array shape\",embedding_array.shape\n",
    "\n",
    "    return embedding_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<tf.Variable 'Variable_6:0' shape=(66596, 128) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    embed = \"word2vec_nce.model\"\n",
    "    train_file = \"user_text.txt\"\n",
    "    output = \"train_embeddings.pkl\"\n",
    "    vocabulary_size = 100000\n",
    "    min_word_freq = 5\n",
    "    seed = 42\n",
    "    neg_samples = 10\n",
    "    t0 = time.time()\n",
    "    word_counter = Counter()\n",
    "    n_docs=0\n",
    "    embedding_size = 128\n",
    "    \n",
    "    with open(train_file,\"r\") as fid:\n",
    "        for line in fid:\n",
    "            message = line.split()[1:]\n",
    "            word_counter.update(message)\n",
    "            n_docs+=1\n",
    "    \n",
    "    #keep only words that occur at least min_word_freq times\n",
    "    wc = {w:c for w,c in word_counter.items() if c>min_word_freq} \n",
    "    \n",
    "    #keep only the args.vocab_size most frequent words\n",
    "    tw = sorted(wc.items(), key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    \n",
    "    top_words = {w[0]:i for i,w in enumerate(tw[:vocabulary_size])}\n",
    "    print(type(top_words))\n",
    "    \n",
    "    \n",
    "    #full_E, full_wrd2idx = emb_utils.read_embeddings(args.emb,top_words)\n",
    "    \n",
    "    embed_matrix = load_embeddings(embed,top_words,embedding_size)\n",
    "    embeddings = tf.Variable(embed_matrix, dtype=tf.float32)\n",
    "    print(embeddings)\n",
    "   \n",
    "    \n",
    "    # Start Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
