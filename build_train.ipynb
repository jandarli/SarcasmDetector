{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "#from ipdb import set_trace\n",
    "#from negative_samples import negative_sampler\n",
    "import numpy as np\n",
    "import os\n",
    "#from sma_toolkit import embeddings as emb_utils \n",
    "#import streaming_pickle as stPickle\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "MIN_DOC_LEN=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Negative Samples that are user specific and then using negative samples and the previous user word ids create user specific train data. create_user_train function is similar to the batch process of skigram model of word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = \"word2vec_nce.model\"\n",
    "train_file = \"user_text_sample.txt\"\n",
    "output_pkl = \"train_embeddings.pkl\"\n",
    "vocabulary_size = 100000\n",
    "min_word_freq = 5\n",
    "seed = 42\n",
    "neg_samples = 10\n",
    "output = \"user_train_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename,wordDict,embedding_size):\n",
    "    dictionary, steps, word_embeds = pickle.load(open(filename, 'rb'))\n",
    "    embedding_array = np.zeros((len(wordDict), embedding_size))\n",
    "    knownWords = list(wordDict.keys())\n",
    "    #print(knownWords)\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        \n",
    "        w = knownWords[i]\n",
    "        if w in dictionary:\n",
    "            index = dictionary[w]\n",
    "        elif w.lower() in dictionary:\n",
    "            index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "        else:\n",
    "            embedding_array[i] = np.random.rand(embedding_size) * 0.02 - 0.01\n",
    "    #print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "    #print \"embedding_array shape\",embedding_array.shape\n",
    "\n",
    "    return embedding_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neg_samples(user_dict, wc):\n",
    "    neg_dict = defaultdict(list)\n",
    "    #sample = np.random.choice(vocabulary_size, num_sampled, p=unigram_prob, replace=False)\n",
    "   \n",
    "    for user, message in user_dict.items():\n",
    "        user_wrd = set(message)\n",
    "        word_corpus = set(wc.keys())\n",
    "        diff = word_corpus - user_wrd        \n",
    "        neg_dict[user] = random.sample(diff, 15)\n",
    "    return neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeds():\n",
    "    \n",
    "    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    word_counter = Counter()\n",
    "    n_docs=0\n",
    "    embedding_size = 128\n",
    "    user_dict = defaultdict(list)\n",
    "    c = 0\n",
    "    with open(train_file,\"r\") as fid:\n",
    "        for line in fid:\n",
    "#             print(\"len\", len(line))\n",
    "            if len(line) > 1:\n",
    "                message = line.split()\n",
    "                user_dict[message[0]].extend(message[1:])\n",
    "                word_counter.update(message[1:])\n",
    "                n_docs += 1\n",
    "    \n",
    "    #keep only words that occur at least min_word_freq times\n",
    "    wc = {w:c for w,c in word_counter.items() if c > min_word_freq} \n",
    "    #finding unigram probability\n",
    "    unigram_cnt = [c for w, c in wc.items()]\n",
    "    total = sum(unigram_cnt)\n",
    "    unigram_prob = [c*1.0/total for c in unigram_cnt]\n",
    "        \n",
    "    tw = sorted(wc.items(), key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    top_words = {w[0]:i for i,w in enumerate(tw[:vocabulary_size])}\n",
    "    print(type(top_words))\n",
    "    wrd2idx = {w:i for i,w in enumerate(top_words.keys())}\n",
    "    \n",
    "    #full_E, full_wrd2idx = emb_utils.read_embeddings(args.emb,top_words)\n",
    "    \n",
    "    embed_matrix = load_embeddings(embed,top_words,embedding_size)\n",
    "    embeddings = tf.Variable(embed_matrix, dtype=tf.float32)\n",
    "    print(embeddings)\n",
    "    pickle.dump([embed_matrix,unigram_prob,wrd2idx,word_counter,len(user_dict.keys())], open(output_pkl, 'wb'))\n",
    "    \n",
    "    return user_dict,wc,wrd2idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_user_train(user_dict, wc, wrd2idx):\n",
    "    negative_samples = get_neg_samples(user_dict, wc)\n",
    "    #print(negative_samples)\n",
    "\n",
    "    prev_user, prev_user_data, prev_ctxscores, prev_neg_samples  = None, [], [], []\n",
    "    full_train = []\n",
    "#     f_train = open(output,\"wb\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "    with open(train_file, \"r\") as fid:\n",
    "        for j, line in enumerate(fid):\n",
    "            if len(line) <= 1:\n",
    "                continue\n",
    "\n",
    "            message = line.split()\n",
    "            user_id = message[0]\n",
    "\n",
    "            #convert to indices\n",
    "            msg_idx = [wrd2idx[w] for w in message if w in wrd2idx]\n",
    "            neg_idx = [wrd2idx[w] for w in negative_samples[user_id] if w in wrd2idx]\n",
    "            \n",
    "            if prev_user == None:  # first user\n",
    "                prev_user = user_id\n",
    "            elif user_id != prev_user or j == n_docs - 1: # this user_id is seen for the first time\n",
    "                \n",
    "                # get numbers in range [0, len(prev_user_data))\n",
    "                shuf_idx = np.arange(len(prev_user_data))\n",
    "                # shuffle numbers\n",
    "                rng.shuffle(shuf_idx)\n",
    "\n",
    "                # fill these lists with the same data in a different order\n",
    "                prev_user_data = [prev_user_data[i] for i in shuf_idx]\n",
    "                prev_neg_samples = [prev_neg_samples[i] for i in shuf_idx]\n",
    "                \n",
    "                # 90-10 train-test split\n",
    "                split = int(len(prev_user_data)*.9)\n",
    "                train = prev_user_data[:split]\n",
    "                test  = prev_user_data[split:]\n",
    "\n",
    "                neg_samples = prev_neg_samples[:split]\n",
    "                # each training instance consists of:\n",
    "                # [user_name, train docs, test docs, negative samples]\n",
    "                full_train.append([prev_user, train, test, neg_samples])\n",
    "                prev_user_data = []\n",
    "                prev_neg_samples = []\n",
    "\n",
    "            prev_user = user_id\n",
    "            prev_user_data.append(msg_idx)\n",
    "            prev_neg_samples.append(neg_idx)\n",
    "\n",
    "    pickle.dump(full_train, open(output, 'wb'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<tf.Variable 'Variable_55:0' shape=(78, 128) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "user_dict, wc, wrd2idx = create_embeds()\n",
    "create_user_train(user_dict,wc,wrd2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
