{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import build_train \n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import parameters\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import tokenize\n",
    "import re\n",
    "import math\n",
    "max_len = 10\n",
    "batch_size = 10\n",
    "hidden_size = 10\n",
    "embedding_size = 128\n",
    "#pooled_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sent_corpus():\n",
    "    line_split = []\n",
    "    message_list = defaultdict(list)\n",
    "    with open(\"train_labels_sample.txt\",\"r\") as fid:\n",
    "            c = 0\n",
    "            for line in fid:\n",
    "                c+=1\n",
    "                #if c > 100:\n",
    "                #    break\n",
    "    #             print(\"len\", len(line))\n",
    "                if len(line) > 1:\n",
    "                    line_split = line.split(\" \")\n",
    "                    message_line = \" \".join(line_split[2:])\n",
    "                    #message_list.append(\" \".join(line.split()[2:]))\n",
    "                    message_list[line_split[0]].append((message_line,line_split[1]))\n",
    "            #max_len = len(max(message_list, key=len))\n",
    "            #print(message_list)\n",
    "            print(\" total \",c,\" lines read fron train_labels\")\n",
    "            return  message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genTrainExamples(message_list, max_len, wrd2idx):\n",
    "    #feature_list = defaultdict(list)\n",
    "    #found max_len greater that embedding size don't understand a thing about it. Keeping it 10 temporarily.\n",
    "    feature_list = []\n",
    "    c = 0\n",
    "    sent_list = []\n",
    "    tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    regex = re.compile(r'[\\.\\]\\%\\[\\'\",\\?\\*!\\}\\{<>\\^-]')\n",
    "    for key, message_line_list in message_list.items():\n",
    "        #c += 1\n",
    "        #if c > 70:\n",
    "        #    break\n",
    "        for sent,label in message_line_list:\n",
    "            content = tokenizer.tokenize(sent)\n",
    "            content = [word for word in content if not regex.match(word)]\n",
    "            for w in content:                 \n",
    "                if w in wrd2idx: #don't know if we have to take words comming only in wrd2idx(topwords)\n",
    "                    sent_list.append(wrd2idx[w])\n",
    "            while len(sent_list) < max_len:\n",
    "                sent_list.append(0)\n",
    "            while len(sent_list) > max_len:\n",
    "                sent_list.pop()\n",
    "            assert len(sent_list) == 10\n",
    "            feature_list.append((key,sent_list,label))\n",
    "            sent_list = []  \n",
    "    #print(feature_list)\n",
    "    return feature_list\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try dense layer, try to figure out why reshape has to use 3*3, try to figure out drop out and also try to understand what happens if we give same heights to all filters, try to figure out the strides, also for varying heights try different height combinations. \n",
    "\n",
    "Used this remember to put this in refference if the structure eremains the same for cnn in future.\n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNModel(object):\n",
    "     \n",
    "    \n",
    "    def __init__(self, graph, E, U):\n",
    "\n",
    "        \n",
    "        self.build_graph(graph, E, U)\n",
    "\n",
    "    def build_graph(self, graph, E, U):\n",
    "        \"\"\"\n",
    "\n",
    "        :param graph:\n",
    "        :param embedding_array:\n",
    "        :param Config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        with graph.as_default():\n",
    "            # for CNN #####\n",
    "            #pooled_outputs = []\n",
    "            self.embeddings = tf.Variable(E, dtype=tf.float32)\n",
    "            self.train_inputs = tf.placeholder(tf.int32, shape=[None,max_len])\n",
    "            print(\"train_inputs\",self.train_inputs )\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=[None,1])\n",
    "            print(\"train_labels\",self.train_labels )\n",
    "            embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs)\n",
    "            print(\"embed after lookup\",embed)\n",
    "            embed = tf.expand_dims(embed, -1)\n",
    "            total_h_grams = 3 * 3\n",
    "            \n",
    "            \n",
    "            # for RNN ###\n",
    "            self.user_embeddings = tf.Variable(U, dtype=tf.float32)\n",
    "            self.user_id = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "            print(\"user_id\",self.user_id )\n",
    "            user_embed = tf.nn.embedding_lookup(self.user_embeddings, self.user_id)\n",
    "            print(\"user_embed after lookup\",user_embed)\n",
    "            \n",
    "            hidden_layer_weights = tf.Variable(tf.truncated_normal([hidden_size, total_h_grams + embedding_size],\n",
    "                                                                      stddev=1.0 / math.sqrt(hidden_size)))\n",
    "            print(\"hidden layer weights\",hidden_layer_weights)\n",
    "            hidden_layer_bias = tf.Variable(tf.zeros([hidden_size,1]))\n",
    "            \n",
    "            print(\"hidden_layer_bias\",hidden_layer_bias )\n",
    "            ####################### was 2 in paper but have taken 1######################\n",
    "            output_layer_weights = tf.Variable(tf.random_normal([1 ,hidden_size ],\n",
    "            \n",
    "                                                                   stddev=1.0 / math.sqrt(hidden_size)))\n",
    "            print(\"output_layer_weights\",output_layer_weights)  \n",
    "            output_layer_bias = tf.Variable(tf.zeros([1,1]))\n",
    "            print(\"output_layer_bias\",output_layer_bias)         \n",
    "            \n",
    "            # CNN #####\n",
    "            for f_size in [1,3,5]:                \n",
    "                filter_weights = tf.Variable(tf.truncated_normal([f_size, 128, 1, 3], stddev=0.1))\n",
    "                bias_conv = tf.Variable(tf.zeros(3))\n",
    "                conv = tf.nn.conv2d(embed,filter_weights,strides=[1, 1, 1, 1],padding=\"VALID\")\n",
    "                additive_bias = tf.nn.bias_add(conv, bias_conv)\n",
    "                Relu_layer = tf.nn.relu(additive_bias)\n",
    "                pooled = tf.nn.max_pool(Relu_layer,ksize=[1, max_len - f_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID')\n",
    "                pooled_outputs.append(pooled)\n",
    "            \n",
    "            print(len(pooled_outputs))\n",
    "            \n",
    "            concatenate_map_filters = tf.concat(pooled_outputs,3)\n",
    "            pooled_outputs = []\n",
    "            print(\"Concatenated map filters\", concatenate_map_filters)\n",
    "            Cs_matrix = tf.reshape(concatenate_map_filters,[total_h_grams, -1])\n",
    "            print(\"Cs_matrx\",Cs_matrix)\n",
    "            \n",
    "            # RNN ###\n",
    "            reshaped_user_embed = tf.reshape(user_embed,[embedding_size,-1])\n",
    "            print(\"reshaped_user_embed\", reshaped_user_embed)\n",
    "            rnn_input = tf.concat([reshaped_user_embed,Cs_matrix],0)\n",
    "            print(\"rnn_input\",rnn_input)\n",
    "            H_Cs_U = tf.matmul(hidden_layer_weights,rnn_input)\n",
    "            print(\"H_Cs_U\",H_Cs_U)\n",
    "            hidden_layer_output = tf.add(H_Cs_U,hidden_layer_bias)\n",
    "            print(\"hidden_layer_output\",hidden_layer_output)\n",
    "            activation_layer_output = tf.nn.relu(hidden_layer_output)\n",
    "            \n",
    "            print(\"activation_layer_output\",activation_layer_output)\n",
    "            self.prediction = tf.add(tf.matmul(output_layer_weights,activation_layer_output),output_layer_bias)\n",
    "            print(\"prediction\",self.prediction)\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.prediction),\n",
    "                                                                                         labels=self.train_labels))\n",
    "            \"\"\"\n",
    "            cue_cnn_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.prediction),\n",
    "                                                                                          labels=self.train_labels)\n",
    "            \n",
    "            \n",
    "            filter_weight_regularizer = tf.nn.l2_loss(filter_weights)\n",
    "            \n",
    "            h_weight_regularizer = tf.nn.l2_loss(hidden_layer_weights)\n",
    "            h_bias_regularizer = tf.nn.l2_loss(hidden_layer_bias)\n",
    "            \n",
    "            output_layer_weights_regularizer = tf.nn.l2_loss(output_layer_weights)\n",
    "            output_layer_bias_regularizer = tf.nn.l2_loss(output_layer_bias)\n",
    "            \n",
    "\n",
    "            self.loss = tf.reduce_mean(cue_cnn_loss + (1e-6/2) *  filter_weight_regularizer \n",
    "                                                    + (1e-6/2) *  h_weight_regularizer\n",
    "                                                    + (1e-6/2) *  h_bias_regularizer\n",
    "                                                    + (1e-6/2) *  output_layer_weights_regularizer\n",
    "                                                    + (1e-6/2) *  output_layer_bias_regularizer\n",
    "                                       \n",
    "                                                    )\n",
    "            \n",
    "            \"\"\"\n",
    "            optimizer = tf.train.GradientDescentOptimizer(1e-6)\n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "            clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "            self.app = optimizer.apply_gradients(clipped_grads)\n",
    "            \n",
    "            \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "  \n",
    "            \n",
    "    def train(self, sess, trainFeats, max_len ):\n",
    "        self.init.run()\n",
    "        print(\"Initailized\")\n",
    "        c = 0\n",
    "        max_num_steps = 1000\n",
    "        user_idx = {}  \n",
    "        user  = []\n",
    "        sent  = []\n",
    "        label = []\n",
    "        for tuple_i in trainFeats:\n",
    "            try:\n",
    "                u_id = user_idx[tuple_i[0]]\n",
    "            except KeyError:\n",
    "                user_idx[tuple_i[0]] = len(user_idx)\n",
    "            u_id = user_idx[tuple_i[0]]\n",
    "            if u_id > 84: ################################### ADDED because of the error in training of user2vec\n",
    "                break\n",
    "            user.append(u_id)\n",
    "            \n",
    "            sent_i , label_i = tuple_i[1], tuple_i[2]\n",
    "            sent.append(sent_i)\n",
    "            label.append([float(label_i)])\n",
    "        average_loss = 0\n",
    "        print(\"label\",len(label))\n",
    "        print(\"sent\",len(sent))\n",
    "        print(\"user\",len(user))\n",
    "        print(\"trainFeats\",len(trainFeats))\n",
    "        for step in range(max_num_steps):\n",
    "            start = (step * batch_size) % len(user)\n",
    "            end = ((step + 1) * batch_size) % len(user)\n",
    "            if end < start:\n",
    "                start -= end\n",
    "                end = len(user)\n",
    "            batch_user, batch_inputs, batch_labels = user[start:end], sent[start:end], label[start:end]\n",
    "            len(batch_inputs)\n",
    "            #feed_dict = {self.train_inputs: batch_inputs, self.train_labels: batch_labels, self.prob: 0.5}\n",
    "            feed_dict = {self.user_id.name:batch_user, self.train_inputs.name: batch_inputs,self.train_labels.name: batch_labels}\n",
    "            _, loss_val = sess.run([self.app, self.loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "            #print(\"loss_val\",loss_val)\n",
    "            if step % 1 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= 1\n",
    "                    print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                    average_loss = 0\n",
    "\n",
    "        print(\"Train Finished.\")\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "            for step in range(max_num_steps):\n",
    "                \n",
    "                \n",
    "                \n",
    "                #k=np.asarray(sent)\n",
    "                #sent_t = np.transpose(k)\n",
    "                print(\"sent\",sent)\n",
    "                print(\"label\",label)\n",
    "                #for i in range(len(label)):\n",
    "                    #print(\"label\",label[i])\n",
    "                    #print(\"sent\",sent[i])\n",
    "                feed_dict = {self.user_id.name:[u_id], self.train_inputs.name: sent,self.train_labels.name: label}\n",
    "                #pooled_outputs = []\n",
    "                _, loss_val = sess.run([self.app, self.cue_cnn_loss], feed_dict=feed_dict)\n",
    "                #print(\"type of loss val\",loss_val)\n",
    "                average_loss += loss_val[0]\n",
    "\n",
    "                if step % 1 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= 1\n",
    "                    print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                    average_loss = 0\n",
    "            #print(\"next\")\n",
    "            c +=1\n",
    "        print(\"train finished\")\n",
    "        #print(c)\n",
    "        #print(len(self.pooled_outputs))\n",
    "        #return self.pooled_outputs\n",
    "           \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_cnn():\n",
    "    E,unigram_prob,wrd2idx,word_counter,n_users = pickle.load(open('train_embeddings.pkl', 'rb'))\n",
    "    U = pickle.load(open(\"user_embeddings.pkl\",\"rb\"))\n",
    "    print(E.shape)\n",
    "    message_list = create_sent_corpus()\n",
    "    print(\"Generating Traning Examples\")\n",
    "    trainFeats= genTrainExamples(message_list, max_len, wrd2idx)\n",
    "    #print(trainFeats)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Build the graph model\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    model = CNNModel(graph, E, U)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        Cs_matrix = model.train(sess, trainFeats, max_len)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56777, 128)\n",
      " total  121  lines read fron train_labels\n",
      "Generating Traning Examples\n",
      "Done.\n",
      "train_inputs Tensor(\"Placeholder:0\", shape=(?, 10), dtype=int32)\n",
      "train_labels Tensor(\"Placeholder_1:0\", shape=(?, 1), dtype=int32)\n",
      "embed after lookup Tensor(\"embedding_lookup:0\", shape=(?, 10, 128), dtype=float32)\n",
      "user_id Tensor(\"Placeholder_2:0\", shape=(10,), dtype=int32)\n",
      "user_embed after lookup Tensor(\"embedding_lookup_1:0\", shape=(10, 128), dtype=float32)\n",
      "hidden layer weights <tf.Variable 'Variable_2:0' shape=(10, 137) dtype=float32_ref>\n",
      "hidden_layer_bias <tf.Variable 'Variable_3:0' shape=(10, 1) dtype=float32_ref>\n",
      "output_layer_weights <tf.Variable 'Variable_4:0' shape=(1, 10) dtype=float32_ref>\n",
      "output_layer_bias <tf.Variable 'Variable_5:0' shape=(1, 1) dtype=float32_ref>\n",
      "3\n",
      "Concatenated map filters Tensor(\"concat:0\", shape=(?, 1, 1, 9), dtype=float32)\n",
      "Cs_matrx Tensor(\"Reshape:0\", shape=(9, ?), dtype=float32)\n",
      "reshaped_user_embed Tensor(\"Reshape_1:0\", shape=(128, 10), dtype=float32)\n",
      "rnn_input Tensor(\"concat_1:0\", shape=(137, 10), dtype=float32)\n",
      "H_Cs_U Tensor(\"MatMul:0\", shape=(10, 10), dtype=float32)\n",
      "hidden_layer_output Tensor(\"Add:0\", shape=(10, 10), dtype=float32)\n",
      "activation_layer_output Tensor(\"Relu_3:0\", shape=(10, 10), dtype=float32)\n",
      "prediction Tensor(\"Add_1:0\", shape=(1, 10), dtype=float32)\n",
      "Initailized\n",
      "label 91\n",
      "sent 91\n",
      "user 91\n",
      "trainFeats 121\n",
      "Average loss at step  0 :  0.0\n",
      "Average loss at step  1 :  0.0\n",
      "Average loss at step  2 :  0.0\n",
      "Average loss at step  3 :  0.0\n",
      "Average loss at step  4 :  0.0\n",
      "Average loss at step  5 :  0.0\n",
      "Average loss at step  6 :  0.0\n",
      "Average loss at step  7 :  0.0\n",
      "Average loss at step  8 :  0.0\n",
      "Average loss at step  9 :  0.0\n",
      "Average loss at step  10 :  0.0\n",
      "Average loss at step  11 :  0.0\n",
      "Average loss at step  12 :  0.0\n",
      "Average loss at step  13 :  0.0\n",
      "Average loss at step  14 :  0.0\n",
      "Average loss at step  15 :  0.0\n",
      "Average loss at step  16 :  0.0\n",
      "Average loss at step  17 :  0.0\n",
      "Average loss at step  18 :  0.0\n",
      "Average loss at step  19 :  0.0\n",
      "Average loss at step  20 :  0.0\n",
      "Average loss at step  21 :  0.0\n",
      "Average loss at step  22 :  0.0\n",
      "Average loss at step  23 :  0.0\n",
      "Average loss at step  24 :  0.0\n",
      "Average loss at step  25 :  0.0\n",
      "Average loss at step  26 :  0.0\n",
      "Average loss at step  27 :  0.0\n",
      "Average loss at step  28 :  0.0\n",
      "Average loss at step  29 :  0.0\n",
      "Average loss at step  30 :  0.0\n",
      "Average loss at step  31 :  0.0\n",
      "Average loss at step  32 :  0.0\n",
      "Average loss at step  33 :  0.0\n",
      "Average loss at step  34 :  0.0\n",
      "Average loss at step  35 :  0.0\n",
      "Average loss at step  36 :  0.0\n",
      "Average loss at step  37 :  0.0\n",
      "Average loss at step  38 :  0.0\n",
      "Average loss at step  39 :  0.0\n",
      "Average loss at step  40 :  0.0\n",
      "Average loss at step  41 :  0.0\n",
      "Average loss at step  42 :  0.0\n",
      "Average loss at step  43 :  0.0\n",
      "Average loss at step  44 :  0.0\n",
      "Average loss at step  45 :  0.0\n",
      "Average loss at step  46 :  0.0\n",
      "Average loss at step  47 :  0.0\n",
      "Average loss at step  48 :  0.0\n",
      "Average loss at step  49 :  0.0\n",
      "Average loss at step  50 :  0.0\n",
      "Average loss at step  51 :  0.0\n",
      "Average loss at step  52 :  0.0\n",
      "Average loss at step  53 :  0.0\n",
      "Average loss at step  54 :  0.0\n",
      "Average loss at step  55 :  0.0\n",
      "Average loss at step  56 :  0.0\n",
      "Average loss at step  57 :  0.0\n",
      "Average loss at step  58 :  0.0\n",
      "Average loss at step  59 :  0.0\n",
      "Average loss at step  60 :  0.0\n",
      "Average loss at step  61 :  0.0\n",
      "Average loss at step  62 :  0.0\n",
      "Average loss at step  63 :  0.0\n",
      "Average loss at step  64 :  0.0\n",
      "Average loss at step  65 :  0.0\n",
      "Average loss at step  66 :  0.0\n",
      "Average loss at step  67 :  0.0\n",
      "Average loss at step  68 :  0.0\n",
      "Average loss at step  69 :  0.0\n",
      "Average loss at step  70 :  0.0\n",
      "Average loss at step  71 :  0.0\n",
      "Average loss at step  72 :  0.0\n",
      "Average loss at step  73 :  0.0\n",
      "Average loss at step  74 :  0.0\n",
      "Average loss at step  75 :  0.0\n",
      "Average loss at step  76 :  0.0\n",
      "Average loss at step  77 :  0.0\n",
      "Average loss at step  78 :  0.0\n",
      "Average loss at step  79 :  0.0\n",
      "Average loss at step  80 :  0.0\n",
      "Average loss at step  81 :  0.0\n",
      "Average loss at step  82 :  0.0\n",
      "Average loss at step  83 :  0.0\n",
      "Average loss at step  84 :  0.0\n",
      "Average loss at step  85 :  0.0\n",
      "Average loss at step  86 :  0.0\n",
      "Average loss at step  87 :  0.0\n",
      "Average loss at step  88 :  0.0\n",
      "Average loss at step  89 :  0.0\n",
      "Average loss at step  90 :  0.0\n",
      "Average loss at step  91 :  0.0\n",
      "Average loss at step  92 :  0.0\n",
      "Average loss at step  93 :  0.0\n",
      "Average loss at step  94 :  0.0\n",
      "Average loss at step  95 :  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-621-99ce17f48c9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minit_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-620-3e07b4151e1d>\u001b[0m in \u001b[0;36minit_cnn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mCs_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainFeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-619-5d5d4eb0e995>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sess, trainFeats, max_len)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;31m#feed_dict = {self.train_inputs: batch_inputs, self.train_labels: batch_labels, self.prob: 0.5}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_user\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0maverage_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m#print(\"loss_val\",loss_val)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
