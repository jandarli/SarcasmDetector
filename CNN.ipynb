{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import build_train \n",
    "import pickle\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import parameters\n",
    "max_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sent_corpus():\n",
    "    message_list = []\n",
    "    with open(build_train.train_file,\"r\") as fid:\n",
    "            c = 0\n",
    "            for line in fid:\n",
    "                c+=1\n",
    "    #             print(\"len\", len(line))\n",
    "                if len(line) > 1:\n",
    "                    message_list.append(\" \".join(line.split()[1:]))\n",
    "            max_len = len(max(message_list, key=len))\n",
    "            \n",
    "            return  message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genTrainExamples(message_list, max_len, wrd2idx):\n",
    "    feature_list = []\n",
    "    #found max_len greater that embedding size don't understand a thing about it. Keeping it 10 temporarily.\n",
    "    \n",
    "    sent_list = []\n",
    "    for sent in message_list:       \n",
    "        for w in sent.split(\" \"):                      \n",
    "            if w in wrd2idx: #don't know if we have to take words comming only in wrd2idx\n",
    "                sent_list.append(wrd2idx[w])\n",
    "        while len(sent_list) < max_len:\n",
    "            sent_list.append(-1)\n",
    "        feature_list.append(sent_list)\n",
    "        sent_list = []            \n",
    "    return feature_list\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNModel(object):\n",
    "     \n",
    "\n",
    "    def __init__(self, graph, E):\n",
    "\n",
    "        \n",
    "        self.build_graph(graph, E)\n",
    "\n",
    "    def build_graph(self, graph, E):\n",
    "        \"\"\"\n",
    "\n",
    "        :param graph:\n",
    "        :param embedding_array:\n",
    "        :param Config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        with graph.as_default():\n",
    "            self.embeddings = tf.Variable(E, dtype=tf.float32)\n",
    "            self.train_inputs = tf.placeholder(tf.int32, shape=[1,max_len])\n",
    "            embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs)\n",
    "            print(\"embed after lookup\",embed)\n",
    "            embed = tf.expand_dims(embed, -1)\n",
    "            for f_size in [1,3,5]:\n",
    "                kernel_size = [f_size, 128, 1, 3]\n",
    "                filter_weights = tf.Variable(tf.truncated_normal(kernel_size, stddev=0.1))\n",
    "                bias_conv = tf.Variable(tf.zeros(3))\n",
    "                conv = tf.nn.conv2d(embed,filter_weights,strides=[1, 1, 1, 1],padding=\"VALID\")\n",
    "                # Apply nonlinearity\n",
    "                Relu_layer = tf.nn.relu(tf.nn.bias_add(conv, bias_conv))\n",
    "                # Max-pooling over the outputs\n",
    "                pooled = tf.nn.max_pool(Relu_layer,ksize=[1, max_len - f_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID')\n",
    "                pooled_outputs.append(pooled)\n",
    "            total_h_grams = 3 * 3\n",
    "            concatenate_map_filters = tf.concat(pooled_outputs,3)\n",
    "            print(\"Concatenated map filters\", concatenate_map_filters)\n",
    "            Cs_matrix = tf.reshape(concatenate_map_filters,[-1])\n",
    "            print(\"Cs_matrx\",Cs_matrix)\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "  \n",
    "            \n",
    "    def train(self, sess, trainFeats, max_len ):\n",
    "        self.init.run()\n",
    "        print(\"Initailized\")\n",
    "        c = 0\n",
    "        for sent in trainFeats:\n",
    "            feed_dict = {self.train_inputs.name: sent}\n",
    "            #print(\"next\")\n",
    "            c +=1\n",
    "        print(\"train finished\")\n",
    "        #print(c)\n",
    "        #print(len(self.pooled_outputs))\n",
    "        #return self.pooled_outputs\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_cnn():\n",
    "    E,unigram_prob,wrd2idx,word_counter,n_users = pickle.load(open('train_embeddings.pkl', 'rb'))\n",
    "    print(E.shape)\n",
    "    message_list = create_sent_corpus()\n",
    "    print(\"Generating Traning Examples\")\n",
    "    trainFeats= genTrainExamples(message_list, max_len, wrd2idx)\n",
    "    #print(trainFeats)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Build the graph model\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    model = CNNModel(graph, E)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        Cs_matrix = model.train(sess, trainFeats, max_len)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1034, 128)\n",
      "Generating Traning Examples\n",
      "Done.\n",
      "embed after lookup Tensor(\"embedding_lookup:0\", shape=(1, 10, 128), dtype=float32)\n",
      "Concatenated map filters Tensor(\"concat:0\", shape=(1, 1, 1, 9), dtype=float32)\n",
      "Cs_matrx Tensor(\"Reshape:0\", shape=(9,), dtype=float32)\n",
      "Initailized\n",
      "train finished\n"
     ]
    }
   ],
   "source": [
    "init_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
