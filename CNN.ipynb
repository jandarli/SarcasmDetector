{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dictionary length 100000\n",
      "dictionary keys 100000\n",
      "word dict key:  the\n",
      "word dict val:  0\n",
      "dictionary keys UNK\n",
      "dictionary vals 0\n",
      "word embeds:  100000\n",
      "word embeds:  128\n",
      "embed array len:  174\n",
      "embed array len:  128\n",
      "unknown words len:  [\"don't\", \"it's\", \"you're\", \"doesn't\", \"I'm\", \"he's\", '4', 'Yes,', \"can't\", 'Yeah,', 'right?', \"I'll\", \"I've\", '(I', \"there's\", \"aren't\", '3', \"they're\", 'Well,']\n",
      "top words len before:  {'the': 0, 'a': 1, 'to': 2, 'and': 3, 'is': 4, 'of': 5, 'I': 6, 'you': 7, 'that': 8, 'it': 9, 'for': 10, 'on': 11, 'in': 12, 'have': 13, 'not': 14, 'they': 15, 'be': 16, 'this': 17, 'are': 18, 'so': 19, 'as': 20, 'all': 21, 'with': 22, 'just': 23, 'but': 24, 'like': 25, 'their': 26, 'if': 27, 'will': 28, 'because': 29, \"don't\": 30, 'about': 31, 'what': 32, 'know': 33, 'But': 34, 'no': 35, 'we': 36, 'your': 37, 'when': 38, 'The': 39, 'up': 40, 'other': 41, 'do': 42, 'more': 43, \"it's\": 44, 'now': 45, 'would': 46, 'at': 47, 'those': 48, 'people': 49, 'there': 50, 'was': 51, 'You': 52, 'than': 53, \"you're\": 54, 'one': 55, 'want': 56, 'think': 57, 'his': 58, 'can': 59, 'get': 60, 'some': 61, 'only': 62, 'sure': 63, 'my': 64, 'its': 65, 'This': 66, \"doesn't\": 67, 'them': 68, 'who': 69, 'even': 70, 'he': 71, 'same': 72, 'has': 73, 'game': 74, 'how': 75, 'much': 76, 'Yeah': 77, \"I'm\": 78, 'going': 79, 'me': 80, \"he's\": 81, '4': 82, 'work': 83, 'had': 84, 'No': 85, 'Yes,': 86, 'down': 87, 'from': 88, 'against': 89, 'really': 90, 'fact': 91, 'So': 92, 'still': 93, \"can't\": 94, 'probably': 95, 'As': 96, 'tell': 97, 'forgot': 98, 'fuck': 99, 'Yeah,': 100, 'being': 101, 'then': 102, 'si': 103, 'should': 104, 'men': 105, 'say': 106, 'getting': 107, 'women': 108, 'too': 109, 'great': 110, 'thing': 111, 'see': 112, 'by': 113, 'or': 114, 'totally': 115, 'They': 116, 'something': 117, 'said': 118, 'guys': 119, 'teams': 120, 'right?': 121, 'Wow': 122, 'use': 123, \"I'll\": 124, 'love': 125, 'make': 126, 'need': 127, 'better': 128, 'many': 129, 'were': 130, 'die': 131, 'such': 132, \"I've\": 133, 'right': 134, 'go': 135, 'feel': 136, 'everyone': 137, 'knows': 138, 'out': 139, 'while': 140, 'throw': 141, 'find': 142, 'an': 143, 'back': 144, '(I': 145, 'playing': 146, 'been': 147, \"there's\": 148, 'never': 149, 'If': 150, 'over': 151, 'new': 152, 'mean': 153, \"aren't\": 154, '3': 155, 'everything': 156, 'way': 157, \"they're\": 158, 'damn': 159, 'wait': 160, 'team': 161, 'saying': 162, 'full': 163, 'Still': 164, 'always': 165, 'Well,': 166, 'doing': 167, 'We': 168, 'most': 169, 'real': 170, 'Well': 171, 'socialism': 172, 'could': 173}\n",
      "top words len after:  155\n",
      "embed_matrix shape (174, 128)\n",
      "128\n",
      "E shape (155, 128)\n",
      "prev user:  fragraptor\n",
      "prev user data:  7\n",
      "prev neg samples:  7\n"
     ]
    }
   ],
   "source": [
    "import build_train \n",
    "import pickle\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import parameters\n",
    "max_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sent_corpus():\n",
    "    message_list = []\n",
    "    with open(build_train.train_file,\"r\") as fid:\n",
    "            c = 0\n",
    "            for line in fid:\n",
    "                c+=1\n",
    "    #             print(\"len\", len(line))\n",
    "                if len(line) > 1:\n",
    "                    message_list.append(\" \".join(line.split()[1:]))\n",
    "            max_len = len(max(message_list, key=len))\n",
    "            \n",
    "            return  message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genTrainExamples(message_list, max_len, wrd2idx):\n",
    "    feature_list = []\n",
    "    #found max_len greater that embedding size don't understand a thing about it. Keeping it 10 temporarily.\n",
    "    \n",
    "    sent_list = []\n",
    "    for sent in message_list:       \n",
    "        for w in sent.split(\" \"):                      \n",
    "            if w in wrd2idx: #don't know if we have to take words comming only in wrd2idx\n",
    "                sent_list.append(wrd2idx[w])\n",
    "        while len(sent_list) < max_len:\n",
    "            sent_list.append(-1)\n",
    "        feature_list.append(sent_list)\n",
    "        sent_list = []            \n",
    "    return feature_list\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try dense layer, try to figure out why reshape has to use 3*3, try to figure out drop out and also try to understand what happens if we give same heights to all filters, try to figure out the strides, also for varying heights try different height combinations. \n",
    "\n",
    "Used this remember to put this in refference if the structure eremains the same for cnn in future.\n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNModel(object):\n",
    "     \n",
    "\n",
    "    def __init__(self, graph, E):\n",
    "\n",
    "        \n",
    "        self.build_graph(graph, E)\n",
    "\n",
    "    def build_graph(self, graph, E):\n",
    "        \"\"\"\n",
    "\n",
    "        :param graph:\n",
    "        :param embedding_array:\n",
    "        :param Config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        with graph.as_default():\n",
    "            self.embeddings = tf.Variable(E, dtype=tf.float32)\n",
    "            self.train_inputs = tf.placeholder(tf.int32, shape=[1,max_len])\n",
    "            embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs)\n",
    "            print(\"embed after lookup\",embed)\n",
    "            embed = tf.expand_dims(embed, -1)\n",
    "            for f_size in [1,3,5]:                \n",
    "                filter_weights = tf.Variable(tf.truncated_normal([f_size, 128, 1, 3], stddev=0.1))\n",
    "                bias_conv = tf.Variable(tf.zeros(3))\n",
    "                conv = tf.nn.conv2d(embed,filter_weights,strides=[1, 1, 1, 1],padding=\"VALID\")\n",
    "                additive_bias = tf.nn.bias_add(conv, bias_conv)\n",
    "                Relu_layer = tf.nn.relu(additive_bias)\n",
    "                pooled = tf.nn.max_pool(Relu_layer,ksize=[1, max_len - f_size + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID')\n",
    "                pooled_outputs.append(pooled)\n",
    "            #total_h_grams = 3 * 3\n",
    "            concatenate_map_filters = tf.concat(pooled_outputs,3)\n",
    "            print(\"Concatenated map filters\", concatenate_map_filters)\n",
    "            Cs_matrix = tf.reshape(concatenate_map_filters,[-1])\n",
    "            print(\"Cs_matrx\",Cs_matrix)\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "  \n",
    "            \n",
    "    def train(self, sess, trainFeats, max_len ):\n",
    "        self.init.run()\n",
    "        print(\"Initailized\")\n",
    "        c = 0\n",
    "        for sent in trainFeats:\n",
    "            feed_dict = {self.train_inputs.name: sent}\n",
    "            #print(\"next\")\n",
    "            c +=1\n",
    "        print(\"train finished\")\n",
    "        #print(c)\n",
    "        #print(len(self.pooled_outputs))\n",
    "        #return self.pooled_outputs\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_cnn():\n",
    "    E,unigram_prob,wrd2idx,word_counter,n_users = pickle.load(open('train_embeddings.pkl', 'rb'))\n",
    "    print(E.shape)\n",
    "    message_list = create_sent_corpus()\n",
    "    print(\"Generating Traning Examples\")\n",
    "    trainFeats= genTrainExamples(message_list, max_len, wrd2idx)\n",
    "    #print(trainFeats)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Build the graph model\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    model = CNNModel(graph, E)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        Cs_matrix = model.train(sess, trainFeats, max_len)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 128)\n",
      "Generating Traning Examples\n",
      "Done.\n",
      "embed after lookup Tensor(\"embedding_lookup:0\", shape=(1, 10, 128), dtype=float32)\n",
      "Concatenated map filters Tensor(\"concat:0\", shape=(1, 1, 1, 9), dtype=float32)\n",
      "Cs_matrx Tensor(\"Reshape:0\", shape=(9,), dtype=float32)\n",
      "Initailized\n",
      "train finished\n"
     ]
    }
   ],
   "source": [
    "init_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
